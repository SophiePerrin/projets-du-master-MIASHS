{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 90248,
          "databundleVersionId": 10480827,
          "sourceType": "competition"
        },
        {
          "sourceId": 214728,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 183064,
          "modelId": 205264
        },
        {
          "sourceId": 215510,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 183739,
          "modelId": 205927
        }
      ],
      "dockerImageVersionId": 30823,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Confection de résumés automatiques d'articles scientifiques par un modèle de langue pré-entraîné : étude et comparaison des performances de plusieurs modèles dans l'environnement informatique de kaggle.\n",
        "\n",
        "**Auteur.e.s :** Sophie Perrin, Emmanuelle Kouadio, Belbaron Nzuego, Jean Marius Kombou, Sylvano Kpedotossi, tous et toutes étudiant.e.s du master 2 MIASHS, pour le cours *Representation learning for NLP* @ Master 2 MALIA et MIASHS.\n",
        "\n",
        "**Equipe :** les léopards"
      ],
      "metadata": {
        "id": "Jm3aCEp_w2w2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Préparation de l'environnement\n",
        "\n",
        "1. Se connecter ou se créer un compte sur https://huggingface.co/\n",
        "2. Se créer un nouveau token d'accès : https://huggingface.co/settings/tokens\n",
        "3. Exécuter le code ci-dessous\n",
        "\n",
        "Note importante pour la vitesse de calcul : pour activer l'accélération GPU dans votre notebook Kaggle :\n",
        "\n",
        "    Ouvrez le menu « Settings » : Dans votre notebook Kaggle, cliquez sur le menu « Accelerator » en haut de l'écran, et choisissez un GPU.\n",
        "\n",
        "    Dans ce même menu, vérifiez qu'il n'est pas écrit \"turn off internet\" - sinon cliquer dessus pour rétablir l'accès au reste d'internet depuis kaggle. Si vous n'avez ni \"turn off internet\" ni \"turn on internet\" d'écrit, alors il faut ajouter votre téléphone et votre photo pour certifier votre identité pour le compte, avant de pouvoir accéder au reste d'internet depuis kaggle (nécessaire pour charger les modèles de huggingface et même les packages de python...!).\n",
        "\n"
      ],
      "metadata": {
        "id": "oiCaaZ8YC8hZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()"
      ],
      "metadata": {
        "id": "3pp0itMoj78l",
        "outputId": "6fc6780e-9cd4-447f-c06d-f7f325e10549",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-12T01:26:23.494678Z",
          "iopub.execute_input": "2025-01-12T01:26:23.494996Z",
          "iopub.status.idle": "2025-01-12T01:26:23.512380Z",
          "shell.execute_reply.started": "2025-01-12T01:26:23.494971Z",
          "shell.execute_reply": "2025-01-12T01:26:23.511407Z"
        },
        "colab": {
          "referenced_widgets": [
            "3a1246ae4116445a99ebf37524c7ff26"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(HTML(value='<center> <img\\nsrc=https://www.kaggle.com/static/images/site-logo.png\\nalt=\\'Kaggle…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3a1246ae4116445a99ebf37524c7ff26"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "m_2_maliash_resume_darticles_scientifiques_path = kagglehub.competition_download('m-2-maliash-resume-darticles-scientifiques')\n",
        "\n",
        "print('Data source import complete.')"
      ],
      "metadata": {
        "id": "z-S3zGw4BfUX",
        "outputId": "b91c7e68-5d81-41a5-e693-63e126680a97",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-12T01:26:41.275755Z",
          "iopub.execute_input": "2025-01-12T01:26:41.276042Z",
          "iopub.status.idle": "2025-01-12T01:26:42.382260Z",
          "shell.execute_reply.started": "2025-01-12T01:26:41.276020Z",
          "shell.execute_reply": "2025-01-12T01:26:42.381559Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Data source import complete.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#Pour créer le \"HF_TOKEN\" dans Kaggle, aller dans \"Add-ons\" (dans les menus de ce notebook, juste à gauche de \"Help\").\n",
        "#Une fois là, aller dans \"secrets\", puis ça marche à peu près comme dans colab : il faut nommer la variable (HF_TOKEN) et\n",
        "#y copier sa valeur.\n",
        "\n",
        "\n",
        "from kaggle_secrets import UserSecretsClient\n",
        "user_secrets = UserSecretsClient()\n",
        "secret_value_0 = user_secrets.get_secret(\"HF_TOKEN\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-12T01:27:33.133435Z",
          "iopub.execute_input": "2025-01-12T01:27:33.133769Z",
          "iopub.status.idle": "2025-01-12T01:27:33.234930Z",
          "shell.execute_reply.started": "2025-01-12T01:27:33.133741Z",
          "shell.execute_reply": "2025-01-12T01:27:33.233876Z"
        },
        "id": "M6QOo-fOMA0F"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Préparation des données d'entraînement pour leur utilisation par le modèle"
      ],
      "metadata": {
        "id": "hoSW39T9djGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On a à faire les résumés automatisés de deux types d'articles différents :\n",
        "\n",
        "- les articles de type \"OBS\"\n",
        "\n",
        "- et les articles de type \"RCT\"\n",
        "\n",
        "Nous allons donc fine-tuner le modèle spécifiquement sur chaque catégorie d'article, afin que sa spécialisation soit, pour chaque catégorie, au plus proche pour elle."
      ],
      "metadata": {
        "id": "82_hwUHBMA0I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Données du dossier OBS"
      ],
      "metadata": {
        "id": "HBD-iCT_MA0J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On crée un dictionnaire qui apparie les articles de l'ensemble d'entraînement (\"fine tuning\" puisque le modèle est déjà pré-entraîné) et leurs résumés par leur identifiant commun.\n",
        "\n",
        "A noter que quelques résumés n'ont pas d'article qui leur correspond !\n",
        "Ils ne sont pas inclus dans le dictionnaire qui nous servira d'ensemble d'entrainement."
      ],
      "metadata": {
        "id": "iSOUl2N1dtAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Chemins dans Kaggle des dossiers contenant les fichiers\n",
        "dossier_abstracts = \"/kaggle/input/m-2-maliash-resume-darticles-scientifiques/train/OBS/abstracts_OBS/\"\n",
        "dossier_articles = \"/kaggle/input/m-2-maliash-resume-darticles-scientifiques/train/OBS/articles_OBS/\"\n",
        "\n",
        "\n",
        "# Liste des fichiers dans chaque dossier\n",
        "fichiers_abstracts = [f for f in os.listdir(dossier_abstracts) if f.startswith(\"abstract-\")]\n",
        "fichiers_articles = [f for f in os.listdir(dossier_articles) if f.startswith(\"article-\")]\n",
        "\n",
        "# Dictionnaires pour stocker les fichiers par identifiant\n",
        "abstracts = {}\n",
        "articles = {}\n",
        "\n",
        "# Remplir les dictionnaires avec les fichiers en fonction des identifiants\n",
        "for fichier in fichiers_abstracts:\n",
        "    # Extraire l'identifiant du fichier abstract\n",
        "    identifiant = fichier.split(\"-\")[1].split(\".\")[0]\n",
        "     #fichier.split(\"-\") : La méthode split(\"-\") divise le nom du fichier\n",
        "    #en une liste de sous-chaînes en utilisant le caractère \"-\" comme séparateur.\n",
        "    #Par exemple, si le fichier est \"abstract-123.txt\", fichier.split(\"-\") renverra la liste [\"abstract\", \"123.txt\"].\n",
        "    #fichier.split(\"-\")[1] : En prenant l'élément d'indice 1 de cette liste (c'est-à-dire \"123.txt\"), on obtient la partie du nom du fichier après le préfixe \"abstract-\".\n",
        "    #split(\".\")[0] : Ensuite, on divise cette chaîne \"123.txt\" avec split(\".\"), ce qui donne [\"123\", \"txt\"].\n",
        "    #On prend le premier élément de la liste (c'est-à-dire \"123\") qui est l'identifiant unique de l'abstract.\n",
        "    abstracts[identifiant] = fichier\n",
        "    #Cette ligne ajoute une entrée dans le dictionnaire abstracts, où la clé est : identifiant (par exemple \"123\")\n",
        "    #et la valeur est le nom du fichier : fichier (par exemple \"abstract-123.txt\").\n",
        "\n",
        "for fichier in fichiers_articles:\n",
        "    # Extraire l'identifiant du fichier article\n",
        "    identifiant = fichier.split(\"-\")[1].split(\".\")[0]\n",
        "    articles[identifiant] = fichier\n",
        "\n",
        "# Dictionnaire pour stocker les appariements\n",
        "appariements = {}\n",
        "\n",
        "# Liste des abstracts et articles non-appariés\n",
        "non_apparies_abstracts = []\n",
        "non_apparies_articles = []\n",
        "\n",
        "# Appariement des fichiers abstracts et articles par identifiant\n",
        "for identifiant in abstracts:\n",
        "    if identifiant in articles:\n",
        "        # Ajouter l'appariement au dictionnaire\n",
        "        appariements[identifiant] = {\n",
        "            \"abstract\": abstracts[identifiant],\n",
        "            \"article\": articles[identifiant]\n",
        "        }\n",
        "    else:\n",
        "        # Ajouter à la liste des abstracts non appariés\n",
        "        non_apparies_abstracts.append(abstracts[identifiant])\n",
        "\n",
        "# Vérifier les articles non-appariés\n",
        "for identifiant in articles:\n",
        "    if identifiant not in abstracts:\n",
        "        # Ajouter à la liste des articles non appariés\n",
        "        non_apparies_articles.append(articles[identifiant])\n",
        "\n",
        "# Affichage des appariements\n",
        "\"\"\"\n",
        "print(\"Appariements :\")\n",
        "for identifiant, fichiers in appariements.items():\n",
        "    print(f\"Identifiant {identifiant}:\")\n",
        "    print(f\"  Abstract: {fichiers['abstract']}\")\n",
        "    print(f\"  Article: {fichiers['article']}\")\n",
        "\"\"\"\n",
        "# Affichage des abstracts non-appariés\n",
        "print(\"\\nAbstracts non-appariés :\")\n",
        "for abstract in non_apparies_abstracts:\n",
        "    print(abstract)\n",
        "\n",
        "# Affichage des articles non-appariés\n",
        "print(\"\\nArticles non-appariés :\")\n",
        "for article in non_apparies_articles:\n",
        "    print(article)"
      ],
      "metadata": {
        "id": "_5wciJ57jnhk",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-11T16:00:23.434403Z",
          "iopub.execute_input": "2025-01-11T16:00:23.434767Z",
          "iopub.status.idle": "2025-01-11T16:00:23.524244Z",
          "shell.execute_reply.started": "2025-01-11T16:00:23.434739Z",
          "shell.execute_reply": "2025-01-11T16:00:23.523588Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "On retravaille un peu la forme de ce dictionnaire pour pouvoir le convertir au format \"dataset\" de Hugging face, nécessaire pour pouvoir entraîner le modèle choisi dessus.\n"
      ],
      "metadata": {
        "id": "7axwPxrAd7Di"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets #le \"-U\" demande l'installation de la toute dernière version du package\n",
        "from datasets import Dataset #importe la classe Dataset de la bibliothèque datasets de Hugging Face.\n",
        "#Cette classe est utilisée pour manipuler des ensembles de données dans un format qui peut être utilisé pour l'entraînement de modèles issus d'Hugging Face.\n",
        "\n",
        "# Limiter le nombre de threads de JAX à 1 (pour éviter les problèmes dans kaggle)\n",
        "os.environ[\"JAX_NUM_THREADS\"] = \"1\"\n",
        "\n",
        "#Notre dictionnaire appariements n'est pas tout à fait sous la bonne forme pour être transformé en dataset de Hugging Face :\n",
        "#on le remanie donc pour qu'il ait cette bonne forme.\n",
        "\n",
        "# Initialisation des listes vides\n",
        "identifiants = []\n",
        "articles = []\n",
        "abstracts = []\n",
        "\n",
        "# Remplir les listes avec les données extraites du dictionnaire appariements\n",
        "for identifiant, values in appariements.items():\n",
        "    identifiants.append(identifiant)         # Ajout de l'identifiant\n",
        "    articles.append(values[\"article\"])      # Ajout de l'article\n",
        "    abstracts.append(values[\"abstract\"])    # Ajout de l'abstract\n",
        "\n",
        "# Créer un dictionnaire avec les listes\n",
        "data = {\n",
        "    \"identifiant\": identifiants,\n",
        "    \"article\": articles,\n",
        "    \"abstract\": abstracts\n",
        "}\n",
        "\n",
        "# Créer le Dataset en convertissant notre nouveau dictionnaire via l'instruction Dataset.from_dict() de Dataset\n",
        "dataset = Dataset.from_dict(data)\n",
        "\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "wJ-fvqMz7ahj",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-11T16:00:28.291025Z",
          "iopub.execute_input": "2025-01-11T16:00:28.291331Z",
          "iopub.status.idle": "2025-01-11T16:00:34.104733Z",
          "shell.execute_reply.started": "2025-01-11T16:00:28.291307Z",
          "shell.execute_reply": "2025-01-11T16:00:34.103917Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## le fichier à utiliser pour l'entraînement du modèle sur les articles de type OBS sera donc \"dataset\""
      ],
      "metadata": {
        "id": "HyshRHJMMA0Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Données du dossier RCT"
      ],
      "metadata": {
        "id": "wXWeR5B5MA0R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On procède de même pour les données des articles de type \"RCT\""
      ],
      "metadata": {
        "id": "0lm5hKb-MA0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Chemins dans Kaggle des dossiers contenant les fichiers\n",
        "dossier_abstracts = \"/kaggle/input/m-2-maliash-resume-darticles-scientifiques/train/RCT/abstracts_RCT/\"\n",
        "dossier_articles = \"/kaggle/input/m-2-maliash-resume-darticles-scientifiques/train/RCT/articles_RCT/\"\n",
        "\n",
        "\n",
        "# Liste des fichiers dans chaque dossier\n",
        "fichiers_abstracts = [f for f in os.listdir(dossier_abstracts) if f.startswith(\"abstract-\")]\n",
        "fichiers_articles = [f for f in os.listdir(dossier_articles) if f.startswith(\"article-\")]\n",
        "\n",
        "# Dictionnaires pour stocker les fichiers par identifiant\n",
        "abstracts = {}\n",
        "articles = {}\n",
        "\n",
        "# Remplir les dictionnaires avec les fichiers en fonction des identifiants\n",
        "for fichier in fichiers_abstracts:\n",
        "    # Extraire l'identifiant du fichier abstract\n",
        "    identifiant = fichier.split(\"-\")[1].split(\".\")[0]\n",
        "     #fichier.split(\"-\") : La méthode split(\"-\") divise le nom du fichier\n",
        "    #en une liste de sous-chaînes en utilisant le caractère \"-\" comme séparateur.\n",
        "    #Par exemple, si le fichier est \"abstract-123.txt\", fichier.split(\"-\") renverra la liste [\"abstract\", \"123.txt\"].\n",
        "    #fichier.split(\"-\")[1] : En prenant l'élément d'indice 1 de cette liste (c'est-à-dire \"123.txt\"), on obtient la partie du nom du fichier après le préfixe \"abstract-\".\n",
        "    #split(\".\")[0] : Ensuite, on divise cette chaîne \"123.txt\" avec split(\".\"), ce qui donne [\"123\", \"txt\"].\n",
        "    #On prend le premier élément de la liste (c'est-à-dire \"123\") qui est l'identifiant unique de l'abstract.\n",
        "    abstracts[identifiant] = fichier\n",
        "    #Cette ligne ajoute une entrée dans le dictionnaire abstracts, où la clé est : identifiant (par exemple \"123\")\n",
        "    #et la valeur est le nom du fichier : fichier (par exemple \"abstract-123.txt\").\n",
        "\n",
        "for fichier in fichiers_articles:\n",
        "    # Extraire l'identifiant du fichier article\n",
        "    identifiant = fichier.split(\"-\")[1].split(\".\")[0]\n",
        "    articles[identifiant] = fichier\n",
        "\n",
        "# Dictionnaire pour stocker les appariements\n",
        "appariements_RCT = {}\n",
        "\n",
        "# Liste des abstracts et articles non-appariés\n",
        "non_apparies_abstracts_RCT = []\n",
        "non_apparies_articles_RCT = []\n",
        "\n",
        "# Appariement des fichiers abstracts et articles par identifiant\n",
        "for identifiant in abstracts:\n",
        "    if identifiant in articles:\n",
        "        # Ajouter l'appariement au dictionnaire\n",
        "        appariements_RCT[identifiant] = {\n",
        "            \"abstract\": abstracts[identifiant],\n",
        "            \"article\": articles[identifiant]\n",
        "        }\n",
        "    else:\n",
        "        # Ajouter à la liste des abstracts non appariés\n",
        "        non_apparies_abstracts_RCT.append(abstracts[identifiant])\n",
        "\n",
        "# Vérifier les articles non-appariés\n",
        "for identifiant in articles:\n",
        "    if identifiant not in abstracts:\n",
        "        # Ajouter à la liste des articles non appariés\n",
        "        non_apparies_articles_RCT.append(articles[identifiant])\n",
        "\n",
        "# Affichage des appariements\n",
        "\"\"\"\n",
        "print(\"Appariements_RCT :\")\n",
        "for identifiant, fichiers in appariements_RCT.items():\n",
        "    print(f\"Identifiant {identifiant}:\")\n",
        "    print(f\"  Abstract: {fichiers['abstract']}\")\n",
        "    print(f\"  Article: {fichiers['article']}\")\n",
        "\"\"\"\n",
        "# Affichage des abstracts non-appariés\n",
        "print(\"\\nAbstracts non-appariés :\")\n",
        "for abstract in non_apparies_abstracts_RCT:\n",
        "    print(abstract)\n",
        "\n",
        "# Affichage des articles non-appariés\n",
        "print(\"\\nArticles non-appariés :\")\n",
        "for article in non_apparies_articles_RCT:\n",
        "    print(article)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-30T22:31:52.628490Z",
          "iopub.execute_input": "2024-12-30T22:31:52.628827Z",
          "iopub.status.idle": "2024-12-30T22:31:52.691396Z",
          "shell.execute_reply.started": "2024-12-30T22:31:52.628802Z",
          "shell.execute_reply": "2024-12-30T22:31:52.690382Z"
        },
        "id": "VXznw-QzMA0R"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets #le \"-U\" demande l'installation de la toute dernière version du package\n",
        "from datasets import Dataset #importe la classe Dataset de la bibliothèque datasets de Hugging Face.\n",
        "#Cette classe est utilisée pour manipuler des ensembles de données dans un format qui peut être utilisé pour l'entraînement de modèles issus d'Hugging Face.\n",
        "\n",
        "# Limiter le nombre de threads de JAX à 1 (pour éviter les problèmes dans kaggle)\n",
        "os.environ[\"JAX_NUM_THREADS\"] = \"1\"\n",
        "\n",
        "#Notre dictionnaire appariements_RCT n'est pas tout à fait sous la bonne forme pour être transformé en dataset de Hugging Face :\n",
        "#on le remanie donc pour qu'il ait cette bonne forme.\n",
        "\n",
        "# Initialisation des listes vides\n",
        "identifiants = []\n",
        "articles = []\n",
        "abstracts = []\n",
        "\n",
        "# Remplir les listes avec les données extraites du dictionnaire appariements\n",
        "for identifiant, values in appariements_RCT.items():\n",
        "    identifiants.append(identifiant)         # Ajout de l'identifiant\n",
        "    articles.append(values[\"article\"])      # Ajout de l'article\n",
        "    abstracts.append(values[\"abstract\"])    # Ajout de l'abstract\n",
        "\n",
        "# Créer un dictionnaire avec les listes\n",
        "data = {\n",
        "    \"identifiant\": identifiants,\n",
        "    \"article\": articles,\n",
        "    \"abstract\": abstracts\n",
        "}\n",
        "\n",
        "# Créer le Dataset en convertissant notre nouveau dictionnaire via l'instruction Dataset.from_dict() de Dataset\n",
        "dataset_RCT = Dataset.from_dict(data)\n",
        "\n",
        "#print(dataset_RCT)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-30T22:31:52.692252Z",
          "iopub.execute_input": "2024-12-30T22:31:52.692542Z",
          "iopub.status.idle": "2024-12-30T22:31:56.234047Z",
          "shell.execute_reply.started": "2024-12-30T22:31:52.692507Z",
          "shell.execute_reply": "2024-12-30T22:31:56.232991Z"
        },
        "id": "Nt2unGfoMA0S"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Le fichier à utiliser pour l'entrainement sur les données RCT sera donc \"dataset_RCT\""
      ],
      "metadata": {
        "id": "2PV0EkhyMA0T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Essai du modèle bigbird-pegasus-large-arxiv"
      ],
      "metadata": {
        "id": "avC5197SMA0U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notre corpus d'articles scientifiques à résumer est constitué de textes longs.\n",
        "C'est un facteur déterminant dans le choix du modèle - et dans ses difficultés...\n",
        "\n",
        "Le modèle bigbird-pegasus-large-arxiv a pour singularité, par rapport à des modèles de type BERT, d'utiliser une version modifiée de l'architecture Transformer : il a un mécanisme d'attention sparse (attention réduite) par blocs. Concrètement, cela signifie qu'au lieu de calculer l'attention entre toutes les positions du texte, il divise la séquence (le texte) en blocs et applique l'attention uniquement au sein de ces blocs, ainsi qu'entre certains blocs (par exemple les blocs voisins ou un échantillonnage aléatoire de blocs).\n",
        "\n",
        "Cela permet d'être économe en temps de calcul : avec un modèle de type BERT, ce dernier croissait en $O(N^2)$, où $N$ représente la taille du texte (ou document). Avec le mécanisme d'attention sparse par blocs, on tombe à des temps de calcul en $O(N.log(N))$ ou en $O(N)$.\n",
        "\n",
        "Pour nos articles scientifiques, qui sont des documents longs, ce type de modèles, avec attention sparse par blocs, peut donc être particulièrement pertinent."
      ],
      "metadata": {
        "id": "fUiZk0D9q-c1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On convertit le texte des articles et des résumés (ici ceux de type \"OBS\") en séquences de tokens pour le modèle bigbird-pegasus-large-arxiv."
      ],
      "metadata": {
        "id": "eDcpedpneMHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Fonction pour préparer les données (tokenisation) pour un modèle chargé depuis huggingface\n",
        "def tokenize_function(examples):\n",
        "    # Tokeniser les articles et les résumés\n",
        "    inputs = tokenizer(examples['article'], truncation=True, padding=\"max_length\", max_length=1024) #données en entrée\n",
        "    #examples['article'] : C'est la donnée brute (l'article) fournie à la fonction sous forme de texte.\n",
        "    #Cette donnée vient d'un Dataset ou d'une DataLoader et est passée sous forme de liste de textes (articles).\n",
        "    #tokenizer(examples['article']) : Cette ligne utilise le tokenizer pour convertir le texte brut des articles en une séquence de tokens.\n",
        "    #Le tokenizer transforme le texte en un format compréhensible par le modèle (i.e., des IDs de tokens).\n",
        "\n",
        "    #truncation=True : Si le texte est plus long que la longueur maximale définie, il sera tronqué pour correspondre à cette longueur maximale.\n",
        "\n",
        "    #padding=\"max_length\" : Cela permet de remplir (padd) le texte pour qu'il atteigne la longueur maximale spécifiée.\n",
        "    #Si un article est plus court que la longueur maximale, des tokens de remplissage seront ajoutés.\n",
        "\n",
        "    #max_length=1024 : La longueur maximale des séquences d'entrée est fixée à 1024 tokens. Si un article est plus long que cela, il sera tronqué à 1024 tokens.\n",
        "\n",
        "    targets = tokenizer(examples['abstract'], truncation=True, padding=\"max_length\", max_length=256)#données cible de l'entrainement du modèle\n",
        "\n",
        "    # Retourner les inputs et targets sous la forme de dictionnaires\n",
        "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
        "    #inputs[\"labels\"] : Dans un modèle de génération de texte comme BigBirdPegasus, les labels sont les séquences que le modèle doit prédire\n",
        "    #(les résumés dans ce cas). En d'autres termes, le modèle apprend à prédire les tokens du résumé à partir des tokens de l'article.\n",
        "    #targets[\"input_ids\"] : input_ids est la représentation des tokens du résumé (les IDs numériques des tokens). Ces tokens sont utilisés comme labels lors de l'entraînement,\n",
        "    #c'est-à-dire que le modèle essaiera de prédire ces input_ids lorsqu'il verra les tokens des articles.\n",
        "    #Cette ligne ajoute donc les input_ids des résumés dans la clé \"labels\" des données d'entrée, qui est utilisée pendant l'entraînement pour calculer la perte\n",
        "    #entre les prédictions du modèle et les résumés réels.\n",
        "    return inputs\n"
      ],
      "metadata": {
        "id": "41fMtp6-pHgH",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-30T22:31:56.235227Z",
          "iopub.execute_input": "2024-12-30T22:31:56.235573Z",
          "iopub.status.idle": "2024-12-30T22:31:56.241697Z",
          "shell.execute_reply.started": "2024-12-30T22:31:56.235542Z",
          "shell.execute_reply": "2024-12-30T22:31:56.240793Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chargement du modèle bigbird-pegasus-large-arxiv\n",
        "\n"
      ],
      "metadata": {
        "id": "TxbTu-oNDOC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import BigBirdPegasusForConditionalGeneration, AutoTokenizer\n",
        "import torch\n",
        "from transformers import TrainingArguments, Trainer # Import TrainingArguments and Trainer\n",
        "\n",
        "#Instancier le tokenizer pour le modèle pegasus\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/bigbird-pegasus-large-arxiv/\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-pegasus-large-arxiv\")\n",
        "#chargement du modèle :\n",
        "# by default encoder-attention is `block_sparse` with num_random_blocks=3, block_size=64/par défaut l'encoder-attention est \"block sparse\"\n",
        "#avec num_random_blocks=3 et block_size=64\n",
        "#model = BigBirdPegasusForConditionalGeneration.from_pretrained(\"/kaggle/input/bigbird-pegasus-large-arxiv/\",\n",
        "model = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-arxiv\",\n",
        "                                                               return_dict=True,\n",
        "                                                               torch_dtype=torch.float16,\n",
        "                                                               device_map=\"auto\")\n",
        "\n",
        "######################Options pour modifier la taille et le nombre des blocs d'attention :\n",
        "# decoder attention type can't be changed & will be \"original_full\"\n",
        "# you can change `attention_type` (encoder only) to full attention like this:\n",
        "##model = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-arxiv\", attention_type=\"original_full\")\n",
        "\n",
        "# you can change `block_size` & `num_random_blocks` like this:\n",
        "##model = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-arxiv\", block_size=16, num_random_blocks=2)\n",
        "######################\n",
        "\n"
      ],
      "metadata": {
        "id": "j1WNnycYXvQM",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-30T22:31:56.244553Z",
          "iopub.execute_input": "2024-12-30T22:31:56.244812Z",
          "iopub.status.idle": "2024-12-30T22:31:56.263509Z",
          "shell.execute_reply.started": "2024-12-30T22:31:56.244791Z",
          "shell.execute_reply": "2024-12-30T22:31:56.262562Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrainement du modèle bigbird-pegasus-large-arxiv et conclusions sur ce modèle pour notre compétition kaggle"
      ],
      "metadata": {
        "id": "g4wdT-g9MA0X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On paramètre l'entraînement du modèle.\n",
        "\n",
        "Problème : même en modifiant les paramètres de training_args comme ci-dessous, pour tenter de consommer moins de mémoire, l'entraînement de ce modèle fait planter la session kaggle car il consomme trop de mémoire - y compris en utilisant les GPU 100, ceux qui comportent le plus de mémoire sur kaggle.\n",
        "Si bigbird-pegasus-large-arxiv est performant en temps de calcul, en revanche, il ne l'est pas encore assez pour ce qui est de l'utilisation de la mémoire - et ce modèle n'existe que dans sa version \"large\".\n",
        "\n",
        "Nous allons donc tester un autre modèle : bart-large-cnn."
      ],
      "metadata": {
        "id": "s9i3nuomeifY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Appliquer la tokenisation\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Définir les arguments d'entraînement\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./bigbird_pegasus_finetuned\",  # Répertoire de sortie pour enregistrer les résultats\n",
        "    evaluation_strategy=\"epoch\",  # Évaluer après chaque époque\n",
        "    learning_rate=2e-5,  # Taux d'apprentissage\n",
        "    per_device_train_batch_size=1, #2 je réduis pour éviter de planter kaggle,  # Taille du batch pour l'entraînement\n",
        "    per_device_eval_batch_size=1, #2 idem,   # Taille du batch pour l'évaluation\n",
        "    gradient_accumulation_steps=8,  # Accumuler les gradients sur 8 batches avant la mise à jour (réduit l'utilisation de la mémoire, toujours pour éviter de planter kaggle)\n",
        "                                    #cela \"simule\" une taille de batch plus grande que ce qu'elle n'est en réalité ici\n",
        "    fp16=False,  # Activer l'entraînement en 16-bit - toujours pour éviter de planter kaggle\n",
        "    num_train_epochs=3,  # Nombre d'époques d'entraînement\n",
        "    dataloader_num_workers=4,  # Utiliser 4 processus pour charger les données en parallèle\n",
        "    gradient_checkpointing=True,  # Activer gradient checkpointing\n",
        "    save_steps=500,  # Sauvegarder moins souvent\n",
        "    weight_decay=0.01,  # L2 regularization\n",
        "    save_total_limit=2,  # Limite du nombre de sauvegardes du modèle\n",
        "    logging_dir=\"./logs\",  # Répertoire pour les logs\n",
        "    logging_steps=100,  # Fréquence des logs\n",
        "    report_to=\"tensorboard\",  # Optionnel : Utiliser TensorBoard pour la visualisation des métriques\n",
        ")\n",
        "\n",
        "# Créer un objet Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,  # Le modèle à fine-tuner\n",
        "    args=training_args,  # Les arguments d'entraînement\n",
        "    train_dataset=train_dataset,  # Jeu d'entraînement\n",
        "    eval_dataset=val_dataset,  # Jeu de validation\n",
        "    compute_metrics=compute_metrics,  # Fonction pour calculer les métriques - elle nous permettra d'utiliser le score \"ROUGE\"\n",
        ")\n",
        "\n",
        "# Lancer l'entraînement\n",
        "trainer.train()\n",
        "\n",
        "\n",
        "# Sauvegarder le modèle fine-tuné\n",
        "trainer.save_model(\"./bigbird_pegasus_finetuned\")\n",
        "\n"
      ],
      "metadata": {
        "id": "9_UU-nmJqoiQ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-30T22:31:56.265505Z",
          "iopub.execute_input": "2024-12-30T22:31:56.265739Z",
          "iopub.status.idle": "2024-12-30T22:31:56.282975Z",
          "shell.execute_reply.started": "2024-12-30T22:31:56.265719Z",
          "shell.execute_reply": "2024-12-30T22:31:56.282172Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Essai du modèle Bart-large-cnn seul"
      ],
      "metadata": {
        "id": "7WiDRR17MA0Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BART est un modèle de transformer encodeur-décodeur (seq2seq : \"sequence to séquence\") avec un encodeur bidirectionnel (similaire à BERT) et un décodeur autoregressif (similaire à GPT).\n",
        "Il est donc pré-entraîné en masquant des parties du texte et en apprenant à prédire ces parties - comme si on lui demandait de débruiter un texte bruité.\n",
        "Il utilise une attention dense classique pour un transformer, où chaque token de l'entrée prend en compte tous les autres tokens de la séquence. Cela le handicape donc pour nos articles longs (comme tous les modèles à attention dense classique).\n",
        "\n",
        "Mais il peut néanmoins rester un parti intéressant, étant donnée sa consommation de mémoire inférieure à bigbird-pegasus.\n",
        "\n",
        "Sera-t-il suffisamment efficace pour la tâche envisagée ? Nous le testons \"à la volée\", sans le fine-tuner, sur quelques articles, pour commencer."
      ],
      "metadata": {
        "id": "_ogvtW3usO7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from datasets import Dataset\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "from rouge_score import rouge_scorer\n",
        "import torch\n",
        "\n",
        "# Charge le modèle BART et le tokenizer\n",
        "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "512pj33xMA0Z"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ici on utilise donc simplement le modèle préentraîné BART pour générer des résumés à partir d'articles scientifiques du train set, qu'on compare ensuite avec leurs résumés de référence (que nous avons, puisque nous sommes dans le train set). On calcule ensuite leur score ROUGE-2 pour évaluer la qualité des résumés générés."
      ],
      "metadata": {
        "id": "JPz0LKgbMA0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "from rouge_score import rouge_scorer\n",
        "import os\n",
        "import random\n",
        "\n",
        "# Charger le modèle BART et le tokenizer\n",
        "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Chemins vers les dossiers des articles et des résumés de référence\n",
        "dossier_articles = \"/kaggle/input/m-2-maliash-resume-darticles-scientifiques/train/OBS/articles_OBS/\"\n",
        "dossier_resumes = \"/kaggle/input/m-2-maliash-resume-darticles-scientifiques/train/OBS/abstracts_OBS/\"\n",
        "\n",
        "# Charger les articles et leurs résumés de référence\n",
        "fichiers_articles = [f for f in os.listdir(dossier_articles) if f.startswith(\"article-\")]\n",
        "articles = {}\n",
        "resumes_reference = {}\n",
        "\n",
        "def lire_fichier(filepath):\n",
        "    try:\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            return f.read()\n",
        "    except UnicodeDecodeError:\n",
        "        with open(filepath, 'r', encoding='latin-1') as f:\n",
        "            return f.read()\n",
        "\n",
        "for fichier in fichiers_articles:\n",
        "    identifiant = fichier.split(\"-\")[1].split(\".\")[0]\n",
        "    articles[identifiant] = lire_fichier(os.path.join(dossier_articles, fichier))\n",
        "    resumes_reference[identifiant] = lire_fichier(os.path.join(dossier_resumes, f\"abstract-{identifiant}.txt\"))\n",
        "\n",
        "# Sélectionner 5 articles aléatoires\n",
        "random.seed(42)  # Pour garantir la reproductibilité\n",
        "selected_ids = random.sample(list(articles.keys()), 5)\n",
        "\n",
        "# Stocker les articles, résumés générés et scores\n",
        "resumes_generes = []\n",
        "rouge_scores = []\n",
        "\n",
        "# Fonction pour résumer un texte avec BART\n",
        "def resumer_texte(texte, max_length=130, min_length=30, length_penalty=2.0, num_beams=4):\n",
        "    inputs = tokenizer.encode(\"summarize: \" + texte, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "    summary_ids = model.generate(\n",
        "        inputs,\n",
        "        max_length=max_length,\n",
        "        min_length=min_length,\n",
        "        length_penalty=length_penalty,\n",
        "        num_beams=num_beams,\n",
        "        early_stopping=True,\n",
        "    )\n",
        "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Calcul des résumés et des scores ROUGE-2\n",
        "scorer = rouge_scorer.RougeScorer([\"rouge2\"], use_stemmer=True)\n",
        "\n",
        "for identifiant in selected_ids:\n",
        "    article = articles[identifiant]\n",
        "    resume_reference = resumes_reference[identifiant]\n",
        "\n",
        "    # Générer le résumé\n",
        "    resume_genere = resumer_texte(article)\n",
        "    resumes_generes.append((identifiant, resume_genere))\n",
        "\n",
        "    # Calculer le score ROUGE-2\n",
        "    scores = scorer.score(resume_reference, resume_genere)\n",
        "    rouge_scores.append(scores[\"rouge2\"].fmeasure)\n",
        "\n",
        "# Afficher les résultats\n",
        "for i, (identifiant, resume_genere) in enumerate(resumes_generes):\n",
        "    print(f\"Article ID: {identifiant}\")\n",
        "    print(f\"Résumé généré: {resume_genere}\")\n",
        "    print(f\"ROUGE-2 Score: {rouge_scores[i]:.4f}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# Afficher le score ROUGE-2 moyen\n",
        "avg_rouge2 = sum(rouge_scores) / len(rouge_scores)\n",
        "print(\"Score ROUGE-2 moyen:\")\n",
        "print(f\"ROUGE-2: {avg_rouge2:.4f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "5B_cK1-yMA0a"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "On obtient, sur ce petit échantillon d'articles, une moyenne de score rouge de 0,04 : c'est exécrable.\n",
        "En plus de son attention non sparse, surtout, ici, le souci est que BART a une limite de 1024 tokens. Les articles plus longs sont donc tronqués (et nos articles scientifiques le sont tous, plus longs), ce qui peut entraîner une perte d'informations essentielles. Les résumés générés seront donc effectués uniquement à partir d'une petite partie du contenu de l'article. Or, toutes les informations importantes pour faire un bon résumé ne se trouvent pas forcément au début des articles.\n",
        "\n",
        "Devant ces premiers résultats, nous n'allons pas plus loin dans l'expérimentation avec ce modèle seul, et décidons d'explorer les modèles longformer."
      ],
      "metadata": {
        "id": "HKnw8TcxMA0a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Confection d'un modèle composé à partir de longformer et bart"
      ],
      "metadata": {
        "id": "hdL4Ri86MA0b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chargement du modèle  allenai/longformer-base-4096\n",
        "\n"
      ],
      "metadata": {
        "id": "mH6ykae_MA0b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le modèle bigbird-pegasus-large-arivx demande trop de mémoire pour notre plateforme de compétition kaggle, et le modèle bart donne des scores rouges trop faibles (même si en le fine-tunant, ils auraient sans doute été légèrement améliorés).\n",
        "\n",
        "Nous devons donc nous orienter vers un modèle de compromis, néanmoins adapté au traitement d'articles scientifiques longs.\n",
        "Il nous faut donc chercher dans la famille des modèles à attention \"sparse\", les seuls vraiment adaptés pour ces textes longs.\n",
        "\n",
        "allenai/longformer-large-4096 partage avec bigbird ce mécanisme d'attention sparse : tout comme lui, il utilise une fenêtre d'attention \"glissante\". Tout comme lui, il y ajoute une attention à plus longue portée sur les tokens importants, ce qui lui permet d'interagir avec tous les autres tokens de la séquence pour capturer des relations à long terme. Mais contrairement à lui, il n'a pas de mécanisme d'attention aléatoire qui se surajoute à ces deux couches d'attention - mécanisme qui permettait à bigbird-pegasus de mieux capter des relations globales...mais consommait de la mémoire vive, lors de son entraînement (fine tuning) notamment.\n",
        "\n",
        "En revanche, allenai/longformer-large-4096 n'est pas un générateur de texte : il nous faut donc l'associer à un modèle de type GPT ou autre qui permet quant à lui la génération. Ce, dans le but de bonifier les performances du modèle générateur par les bonnes performances de longformer sur les textes longs.\n",
        "On l'associe donc, in fine, avec le modèle générateur bart-large-cnn, association qu'on crée \"à la main\" via la confection d'une classe spécifique.\n",
        "\n",
        "Comme le modèle ainsi créé reste trop volumineux, on y remplace finalement allenai/longformer-large-4096 par une version plus légère : allenai/longformer-base-4096.\n",
        "\n"
      ],
      "metadata": {
        "id": "9l_ctMwTMA0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import LongformerForSequenceClassification, AutoTokenizer\n",
        "import torch\n",
        "from transformers import TrainingArguments, Trainer # Import TrainingArguments and Trainer\n",
        "\n",
        "# Instancier le tokenizer pour le modèle Longformer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
        "\n",
        "# Chargement du modèle Longformer\n",
        "model = LongformerForSequenceClassification.from_pretrained(\"allenai/longformer-base-4096\",\n",
        "                                                           return_dict=True,\n",
        "                                                           torch_dtype=torch.float16,\n",
        "                                                           device_map=\"auto\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-11T16:00:54.819217Z",
          "iopub.execute_input": "2025-01-11T16:00:54.819569Z",
          "iopub.status.idle": "2025-01-11T16:01:13.191383Z",
          "shell.execute_reply.started": "2025-01-11T16:00:54.819527Z",
          "shell.execute_reply": "2025-01-11T16:01:13.190533Z"
        },
        "id": "4DoWRTxTMA0b"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Création du nouveau modèle \"cousu main\" à partir de BART et LONGFORMER"
      ],
      "metadata": {
        "id": "JdZto02EMA0c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On est obligés de faire un peu de \"cuisine\" (en code informatique) pour associer les 2 modèles en un seul : par exemple, les sorties de longformer n'ont pas la même taille que les entrées de bart. Il nous faut donc insérer dans la classe une \"couche de projection\".\n",
        "Il faut également dupliquer leurs poids, pour éviter les (mauvais...) mélanges entre eux par la suite.\n",
        "\n",
        "On ne peut pas utiliser la tokenisation de l'un des deux modèles non plus : il nous faut en créer une, tout aussi hybride que notre modèle \"fait main\" à partir de la \"couture\" de ces deux modèles issus de hugging face."
      ],
      "metadata": {
        "id": "qJe9CqhwMA0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import LongformerTokenizer, BartTokenizer, BartForConditionalGeneration, LongformerModel, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Charger le tokenizer et le modèle Longformer pour l'encodage\n",
        "longformer_tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
        "longformer_model = LongformerModel.from_pretrained('allenai/longformer-base-4096')\n",
        "\n",
        "# Charger le tokenizer et le modèle BART-CNN pour la génération\n",
        "bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
        "\n",
        "# Tokenisation des entrées et des sorties\n",
        "def tokenize_function(examples):\n",
        "    # Tokeniser les articles avec Longformer\n",
        "    model_inputs = longformer_tokenizer(\n",
        "        examples[\"article\"],\n",
        "        #max_length=4096, #trop long pour le GPU de kaggle...nous en revenons donc aux fameux 1024 token max, d'où nous partions avec BART seul.\n",
        "        max_length=1024,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",  # Assurer le padding\n",
        "        return_tensors=\"pt\"  # Retourner des tensors PyTorch\n",
        "    )\n",
        "\n",
        "    # Tokeniser les résumés (labels) avec BART tokenizer\n",
        "    labels = bart_tokenizer(\n",
        "        #examples[\"highlights\"],\n",
        "        examples[\"abstract\"],\n",
        "        #max_length=200,\n",
        "        max_length=1024,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",  # Assurer le padding\n",
        "        return_tensors=\"pt\"  # Retourner des tensors PyTorch\n",
        "    )\n",
        "\n",
        "    # Ajouter les labels au dictionnaire de sortie\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# Définir un modèle combiné qui utilise Longformer comme encodeur et BART pour la génération\n",
        "class LongformerBart(nn.Module):\n",
        "    def __init__(self, longformer_model, bart_model):\n",
        "        super(LongformerBart, self).__init__()\n",
        "        self.longformer = longformer_model\n",
        "        self.bart = bart_model\n",
        "\n",
        " # Ajouter une couche linéaire pour ajuster la dimension des sorties de Longformer\n",
        "        self.longformer_projection = nn.Linear(768, 1024)  # Projeter de 768 à 1024\n",
        "\n",
        " # Dupliquer les poids partagés de BART et Longformer\n",
        "        with torch.no_grad():\n",
        "            # Dupliquer les poids partagés de BART\n",
        "            bart_model.model.shared = torch.nn.Embedding.from_pretrained(bart_model.model.shared.weight.clone())\n",
        "            bart_model.model.encoder.embed_tokens = torch.nn.Embedding.from_pretrained(bart_model.model.encoder.embed_tokens.weight.clone())\n",
        "            bart_model.model.decoder.embed_tokens = torch.nn.Embedding.from_pretrained(bart_model.model.decoder.embed_tokens.weight.clone())\n",
        "\n",
        "            # Dupliquer les poids partagés de Longformer\n",
        "            longformer_model.embeddings.word_embeddings = torch.nn.Embedding.from_pretrained(longformer_model.embeddings.word_embeddings.weight.clone())\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, decoder_input_ids=None, labels=None):\n",
        "        # Encoder la séquence avec Longformer\n",
        "        encoder_outputs = self.longformer(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        encoder_hidden_states = encoder_outputs.last_hidden_state\n",
        "\n",
        "  # Appliquer la projection pour ajuster la dimension\n",
        "        encoder_hidden_states = self.longformer_projection(encoder_hidden_states)\n",
        "\n",
        "        # Passer les représentations encodées à BART pour la génération\n",
        "        decoder_outputs = self.bart(\n",
        "            input_ids=decoder_input_ids,\n",
        "            labels=labels,\n",
        "            encoder_outputs=encoder_hidden_states\n",
        "        )\n",
        "\n",
        "        return decoder_outputs"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-11T16:16:03.575421Z",
          "iopub.execute_input": "2025-01-11T16:16:03.575769Z",
          "iopub.status.idle": "2025-01-11T16:16:14.689112Z",
          "shell.execute_reply.started": "2025-01-11T16:16:03.575739Z",
          "shell.execute_reply": "2025-01-11T16:16:14.688456Z"
        },
        "id": "C6MIaRLRMA0d"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrainement de ce modèle sur les données OBS, puis sauvegarde de ce modèle ainsi fine-tuné\n"
      ],
      "metadata": {
        "id": "Aua6g7tMMA0f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On effectue la spécialisation (\"fine tuning\") de ce modèle, pour la tâche de générer des résumés pour des articles scientifiques de type \"OBS\"."
      ],
      "metadata": {
        "id": "h6hBgEFnMA0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Appliquer la tokenisation\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Diviser l'ensemble de données en train et validation\n",
        "dataset_split = tokenized_datasets.train_test_split(test_size=0.2)\n",
        "\n",
        "# Extraire les ensembles d'entraînement et de validation\n",
        "train_dataset = dataset_split['train']\n",
        "val_dataset = dataset_split['test']\n",
        "\n",
        "\n",
        "\n",
        "# Initialiser le modèle combiné\n",
        "model = LongformerBart(longformer_model, bart_model)\n",
        "\n",
        "# Définir les arguments d'entraînement\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./longformer_bart_finetuned\",  # Répertoire de sortie pour enregistrer les résultats\n",
        "    evaluation_strategy=\"epoch\",  # Évaluer après chaque époque\n",
        "    learning_rate=2e-5,  # Taux d'apprentissage\n",
        "    per_device_train_batch_size=2,  # Taille du batch pour l'entraînement\n",
        "    per_device_eval_batch_size=2,   # Taille du batch pour l'évaluation\n",
        "    num_train_epochs=3,  # Nombre d'époques d'entraînement\n",
        "    save_steps=500,  # Sauvegarder moins souvent\n",
        "    weight_decay=0.01,  # L2 regularization\n",
        "    logging_dir=\"./logs\",  # Répertoire pour les logs\n",
        "    logging_steps=10,  # Fréquence des logs\n",
        "    report_to=\"tensorboard\",  # Optionnel : Utiliser TensorBoard pour la visualisation des métriques\n",
        ")\n",
        "\n",
        "\n",
        "# Créer un objet Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,  # Le modèle combiné Longformer+BART\n",
        "    args=training_args,  # Les arguments d'entraînement\n",
        "    train_dataset=train_dataset,  # Jeu d'entraînement\n",
        "    eval_dataset=val_dataset,  # Jeu de validation\n",
        "    tokenizer=longformer_tokenizer,  # Tokenizer pour le pré-traitement des données\n",
        ")\n",
        "\n",
        "# Lancer l'entraînement\n",
        "trainer.train()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-30T22:32:27.461065Z",
          "iopub.execute_input": "2024-12-30T22:32:27.461401Z",
          "iopub.status.idle": "2024-12-30T22:42:31.324020Z",
          "shell.execute_reply.started": "2024-12-30T22:32:27.461364Z",
          "shell.execute_reply": "2024-12-30T22:42:31.323249Z"
        },
        "id": "6guNYF10MA0g"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le modèle fine-tuné nécessite quelques ajustements au niveau de son enregistrement, pour pouvoir le sauvegarder correctement :"
      ],
      "metadata": {
        "id": "ks4HxsVNMA0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dupliquer les poids partagés pour éviter la duplication de mémoire\n",
        "# Cela doit être fait après l'entraînement et avant la sauvegarde\n",
        "\n",
        "import safetensors.torch as st\n",
        "\n",
        "# Pour Longformer - dupliquer les embeddings\n",
        "with torch.no_grad():  # Empêche la modification des gradients\n",
        "    new_word_embeddings = model.longformer.embeddings.word_embeddings.weight.clone()\n",
        "    model.longformer.embeddings.word_embeddings = torch.nn.Embedding.from_pretrained(new_word_embeddings)\n",
        "\n",
        "# Pour BART - dupliquer les embeddings de l'encodeur et du décodeur\n",
        "with torch.no_grad():  # Empêche la modification des gradients\n",
        "    new_shared_weight = model.bart.model.shared.weight.clone()\n",
        "    model.bart.model.shared = torch.nn.Embedding.from_pretrained(new_shared_weight)\n",
        "\n",
        "    new_encoder_embed = model.bart.model.encoder.embed_tokens.weight.clone()\n",
        "    model.bart.model.encoder.embed_tokens = torch.nn.Embedding.from_pretrained(new_encoder_embed)\n",
        "\n",
        "    new_decoder_embed = model.bart.model.decoder.embed_tokens.weight.clone()\n",
        "    model.bart.model.decoder.embed_tokens = torch.nn.Embedding.from_pretrained(new_decoder_embed)\n",
        "\n",
        "# Sauvegarder les poids dans un fichier classique\n",
        "torch.save(model.state_dict(), 'longformer_bart_finetuned/model_weights.pth')\n",
        "\n",
        "# Charger manuellement avec safetensors\n",
        "import safetensors.torch as st\n",
        "\n",
        "# Sauvegarder avec safetensors après avoir dupliqué les poids\n",
        "st.save_file(model.state_dict(), 'longformer_bart_finetuned/model_weights.safetensors')\n",
        "\n",
        "import json\n",
        "\n",
        "# Définir la configuration\n",
        "config = {\n",
        "    \"architectures\": [\"LongformerForSequenceClassification\"],\n",
        "    \"hidden_size\": 1024,\n",
        "    \"num_attention_heads\": 16,\n",
        "    \"num_hidden_layers\": 12,\n",
        "    \"vocab_size\": 50264,\n",
        "    \"max_position_embeddings\": 4098,\n",
        "    \"embedding_size\": 1024,\n",
        "    \"layer_norm_eps\": 1e-5,\n",
        "    \"pad_token_id\": 1,\n",
        "    \"activation_function\": \"gelu\",\n",
        "    \"initializer_range\": 0.02,\n",
        "    \"layer_norm_affine\": True,\n",
        "    \"attention_probs_dropout_prob\": 0.1,\n",
        "    \"hidden_dropout_prob\": 0.1,\n",
        "    \"max_position_embeddings\": 4098,\n",
        "    \"type_vocab_size\": 2,\n",
        "    \"attention_dropout\": 0.1,\n",
        "    \"longformer_projection\": {\n",
        "        \"in_features\": 768,\n",
        "        \"out_features\": 1024\n",
        "    }\n",
        "}\n",
        "\n",
        "# Sauvegarder le fichier config.json\n",
        "#config_path = '/kaggle/working/config.json'\n",
        "with open(\"longformer_bart_finetuned/config.json\", 'w') as f:\n",
        "    json.dump(config, f)\n",
        "\n",
        "print(\"Le fichier config.json a été sauvegardé \")\n",
        "\n",
        "# Sauvegarder le modèle fine-tuné\n",
        "trainer.save_model(\"./longformer_bart_finetuned\")\n",
        "\"\"\"\n",
        "# Sauvegarde du modèle\n",
        "torch.save(longformer_bart_model.state_dict(), \"longformer_bart_finetuned.pth\")\n",
        "\n",
        "# Sauvegarde de la configuration\n",
        "longformer_bart_model.config.to_json_file(\"longformer_bart_config.json\")\n",
        "\"\"\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-11T16:41:06.655830Z",
          "iopub.execute_input": "2025-01-11T16:41:06.656181Z",
          "iopub.status.idle": "2025-01-11T16:41:29.108937Z",
          "shell.execute_reply.started": "2025-01-11T16:41:06.656152Z",
          "shell.execute_reply": "2025-01-11T16:41:29.108204Z"
        },
        "id": "Gvy1_kl8MA0h"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "...et les contraintes de capacité de stockage du dossier \"working\" de kaggle nous obligent encore à quelques manipulations (ici, suppression de checkpoint) pour pouvoir sauvegarder notre modèle fine-tuné dans un fichier zip :"
      ],
      "metadata": {
        "id": "2EvXMnUkMA0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "# Chemin du dossier à supprimer\n",
        "directory_to_delete = \"/kaggle/working/longformer_bart_finetuned/checkpoint-483\"\n",
        "# Vérifier si le dossier existe\n",
        "if os.path.exists(directory_to_delete):\n",
        "    # Supprimer le dossier et tout son contenu\n",
        "    shutil.rmtree(directory_to_delete)\n",
        "    print(f\"Le dossier '{directory_to_delete}' a été supprimé avec succès.\")\n",
        "else:\n",
        "    print(f\"Le dossier '{directory_to_delete}' n'existe pas.\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-30T22:42:52.637526Z",
          "iopub.execute_input": "2024-12-30T22:42:52.637753Z",
          "iopub.status.idle": "2024-12-30T22:43:13.534881Z",
          "shell.execute_reply.started": "2024-12-30T22:42:52.637732Z",
          "shell.execute_reply": "2024-12-30T22:43:13.534111Z"
        },
        "id": "q4x629o_MA0l",
        "outputId": "e4d596d8-6971-4f63-fa26-0698695da886"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Le dossier '/kaggle/working/longformer_bart_finetuned/checkpoint-483' a été supprimé avec succès.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "On zipe, enfin, le dossier du modèle ainsi créé et entraîné, pour pouvoir le sauvegarder et le réutiliser ensuite sur notre ensemble de test sans risquer de le perdre à chaque interruption de session :\n"
      ],
      "metadata": {
        "id": "ScVKykxoMA0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(os.listdir(\"./\"))\n",
        "\n",
        "shutil.make_archive('/kaggle/working/longformer_bart_finetuned', 'zip', './longformer_bart_finetuned')\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "wffnMFnxMA0p"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entraînement de ce modèle sur les données RCT, puis sauvegarde de ce modèle fine-tuné"
      ],
      "metadata": {
        "id": "av78ADRQMA0q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On procède exactement de même pour le fine-tuning, puis la sauvegarde, du modèle spécialisé dans le résumé d'articles scientifiques de type \"RCT\" :"
      ],
      "metadata": {
        "id": "4AYU4YJLMA0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Appliquer la tokenisation\n",
        "tokenized_datasets = dataset_RCT.map(tokenize_function, batched=True)\n",
        "\n",
        "# Diviser l'ensemble de données en train et validation\n",
        "dataset_split = tokenized_datasets.train_test_split(test_size=0.2)\n",
        "\n",
        "# Extraire les ensembles d'entraînement et de validation\n",
        "train_dataset = dataset_split['train']\n",
        "val_dataset = dataset_split['test']\n",
        "\n",
        "\n",
        "\n",
        "# Initialiser le modèle combiné\n",
        "model = LongformerBart(longformer_model, bart_model)\n",
        "\n",
        "# Définir les arguments d'entraînement\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./longformer_bart_finetuned_rct\",  # Répertoire de sortie pour enregistrer les résultats\n",
        "    evaluation_strategy=\"epoch\",  # Évaluer après chaque époque\n",
        "    learning_rate=2e-5,  # Taux d'apprentissage\n",
        "    per_device_train_batch_size=2,  # Taille du batch pour l'entraînement\n",
        "    per_device_eval_batch_size=2,   # Taille du batch pour l'évaluation\n",
        "    num_train_epochs=3,  # Nombre d'époques d'entraînement\n",
        "    save_steps=500,  # Sauvegarder moins souvent\n",
        "    weight_decay=0.01,  # L2 regularization\n",
        "    logging_dir=\"./logs\",  # Répertoire pour les logs\n",
        "    logging_steps=10,  # Fréquence des logs\n",
        "    report_to=\"tensorboard\",  # Optionnel : Utiliser TensorBoard pour la visualisation des métriques\n",
        ")\n",
        "\n",
        "\n",
        "# Créer un objet Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,  # Le modèle combiné Longformer+BART\n",
        "    args=training_args,  # Les arguments d'entraînement\n",
        "    train_dataset=train_dataset,  # Jeu d'entraînement\n",
        "    eval_dataset=val_dataset,  # Jeu de validation\n",
        "    tokenizer=longformer_tokenizer,  # Tokenizer pour le pré-traitement des données\n",
        ")\n",
        "\n",
        "# Lancer l'entraînement\n",
        "trainer.train()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-30T22:43:13.541763Z",
          "iopub.execute_input": "2024-12-30T22:43:13.542009Z",
          "iopub.status.idle": "2024-12-30T22:43:13.557210Z",
          "shell.execute_reply.started": "2024-12-30T22:43:13.541978Z",
          "shell.execute_reply": "2024-12-30T22:43:13.556514Z"
        },
        "id": "w8slc4xOMA0r"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Dupliquer les poids partagés pour éviter la duplication de mémoire\n",
        "# Cela doit être fait après l'entraînement et avant la sauvegarde\n",
        "\n",
        "import safetensors.torch as st\n",
        "\n",
        "# Pour Longformer - dupliquer les embeddings\n",
        "with torch.no_grad():  # Empêche la modification des gradients\n",
        "    new_word_embeddings = model.longformer.embeddings.word_embeddings.weight.clone()\n",
        "    model.longformer.embeddings.word_embeddings = torch.nn.Embedding.from_pretrained(new_word_embeddings)\n",
        "\n",
        "# Pour BART - dupliquer les embeddings de l'encodeur et du décodeur\n",
        "with torch.no_grad():  # Empêche la modification des gradients\n",
        "    new_shared_weight = model.bart.model.shared.weight.clone()\n",
        "    model.bart.model.shared = torch.nn.Embedding.from_pretrained(new_shared_weight)\n",
        "\n",
        "    new_encoder_embed = model.bart.model.encoder.embed_tokens.weight.clone()\n",
        "    model.bart.model.encoder.embed_tokens = torch.nn.Embedding.from_pretrained(new_encoder_embed)\n",
        "\n",
        "    new_decoder_embed = model.bart.model.decoder.embed_tokens.weight.clone()\n",
        "    model.bart.model.decoder.embed_tokens = torch.nn.Embedding.from_pretrained(new_decoder_embed)\n",
        "\n",
        "# Sauvegarder les poids dans un fichier classique\n",
        "torch.save(model.state_dict(), 'longformer_bart_finetuned_rct/model_weights.pth')\n",
        "\n",
        "# Sauvegarder avec safetensors après avoir dupliqué les poids\n",
        "st.save_file(model.state_dict(), 'longformer_bart_finetuned_rct/model_weights.safetensors')\n",
        "\n",
        "\n",
        "from transformers import PretrainedConfig\n",
        "\n",
        "import json\n",
        "\n",
        "# Définir la configuration\n",
        "config = {\n",
        "    \"architectures\": [\"LongformerForSequenceClassification\"],\n",
        "    \"hidden_size\": 1024,\n",
        "    \"num_attention_heads\": 16,\n",
        "    \"num_hidden_layers\": 12,\n",
        "    \"vocab_size\": 50264,\n",
        "    \"max_position_embeddings\": 4098,\n",
        "    \"embedding_size\": 1024,\n",
        "    \"layer_norm_eps\": 1e-5,\n",
        "    \"pad_token_id\": 1,\n",
        "    \"activation_function\": \"gelu\",\n",
        "    \"initializer_range\": 0.02,\n",
        "    \"layer_norm_affine\": True,\n",
        "    \"attention_probs_dropout_prob\": 0.1,\n",
        "    \"hidden_dropout_prob\": 0.1,\n",
        "    \"max_position_embeddings\": 4098,\n",
        "    \"type_vocab_size\": 2,\n",
        "    \"attention_dropout\": 0.1,\n",
        "    \"longformer_projection\": {\n",
        "        \"in_features\": 768,\n",
        "        \"out_features\": 1024\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "# Sauvegarder le fichier config.json\n",
        "with open(\"longformer_bart_finetuned_rct/config.json\", 'w') as f:\n",
        "    json.dump(config, f)\n",
        "\n",
        "print(\"Le fichier config.json a été sauvegardé \")\n",
        "\n",
        "\n",
        "# Sauvegarder le modèle fine-tuné\n",
        "trainer.save_model(\"./longformer_bart_finetuned_rct\")\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "# Sauvegarde du modèle\n",
        "torch.save(longformer_bart_model.state_dict(), \"longformer_bart_finetuned.pth\")\n",
        "\n",
        "# Sauvegarde de la configuration\n",
        "longformer_bart_model.config.to_json_file(\"longformer_bart_config.json\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-30T22:43:13.558126Z",
          "iopub.execute_input": "2024-12-30T22:43:13.558464Z",
          "iopub.status.idle": "2024-12-30T22:43:13.577474Z",
          "shell.execute_reply.started": "2024-12-30T22:43:13.558432Z",
          "shell.execute_reply": "2024-12-30T22:43:13.576640Z"
        },
        "id": "9FT-nFG4MA0r"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "# Chemin du dossier à supprimer\n",
        "directory_to_delete = \"/kaggle/working/longformer_bart_finetuned_rct/checkpoint-393\"\n",
        "# Vérifier si le dossier existe\n",
        "if os.path.exists(directory_to_delete):\n",
        "    # Supprimer le dossier et tout son contenu\n",
        "    shutil.rmtree(directory_to_delete)\n",
        "    print(f\"Le dossier '{directory_to_delete}' a été supprimé avec succès.\")\n",
        "else:\n",
        "    print(f\"Le dossier '{directory_to_delete}' n'existe pas.\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-30T22:43:13.578229Z",
          "iopub.execute_input": "2024-12-30T22:43:13.578481Z",
          "iopub.status.idle": "2024-12-30T22:43:13.597519Z",
          "shell.execute_reply.started": "2024-12-30T22:43:13.578462Z",
          "shell.execute_reply": "2024-12-30T22:43:13.596689Z"
        },
        "id": "hrZ2rUDPMA0s",
        "outputId": "f37a9943-8eb2-4b07-f4eb-99af417c7758"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Le dossier '/kaggle/working/longformer_bart_finetuned_rct/checkpoint-393' n'existe pas.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "print(os.listdir(\"./\"))\n",
        "\n",
        "shutil.make_archive('/kaggle/working/longformer_bart_finetuned_rct', 'zip', './longformer_bart_finetuned_rct')\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-30T22:43:13.598256Z",
          "iopub.execute_input": "2024-12-30T22:43:13.598586Z",
          "iopub.status.idle": "2024-12-30T22:43:13.613521Z",
          "shell.execute_reply.started": "2024-12-30T22:43:13.598551Z",
          "shell.execute_reply": "2024-12-30T22:43:13.612797Z"
        },
        "id": "TXs3rxMZMA0t"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Génération des résumés des articles de test"
      ],
      "metadata": {
        "id": "A8dyTzskMA0t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chargement de notre modèle fine-tuné sur les données \"OBS\" et constitué à partir des deux modèles bart et longformer, pour pouvoir l'utiliser pour l'ensemble de test et soumettre nos résumés pour la compétition"
      ],
      "metadata": {
        "id": "pIU7Rou_MA0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Charger le modèle fine tuné afin de l'utiliser\n",
        "import torch\n",
        "from transformers import LongformerModel, BartForConditionalGeneration, LongformerConfig\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "# Définir la classe LongformerBart (on la redéfinit ici pour éviter d'avoir à naviguer entre cellules très distantes en pratique - et on n'en reprend que les parties nécessaires\n",
        "#à l'usage qu'on va en avoir là)\n",
        "class LongformerBart(nn.Module):\n",
        "    def __init__(self, longformer_model, bart_model):\n",
        "        super(LongformerBart, self).__init__()\n",
        "        self.longformer = longformer_model\n",
        "        self.bart = bart_model\n",
        "        self.longformer_projection = nn.Linear(768, 1024)  # Projeter de 768 à 1024\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, decoder_input_ids=None, labels=None):\n",
        "        encoder_outputs = self.longformer(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        encoder_hidden_states = encoder_outputs.last_hidden_state\n",
        "        encoder_hidden_states = self.longformer_projection(encoder_hidden_states)  # Appliquer la projection\n",
        "        decoder_outputs = self.bart(input_ids=decoder_input_ids, labels=labels, encoder_outputs=encoder_hidden_states)\n",
        "        return decoder_outputs\n",
        "\n",
        "# Charger Longformer et BART\n",
        "longformer_model = LongformerModel.from_pretrained('allenai/longformer-base-4096')\n",
        "bart = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Charger les poids de notre modèle fine-tuné\n",
        "model = LongformerBart(longformer_model, bart)\n",
        "model.load_state_dict(torch.load('/kaggle/input/longformer_bart_fine_tuned_obs/pytorch/default/1/model_weights.pth'))\n",
        "#model.load_state_dict(torch.load('/kaggle/working/longformer_bart_finetuned/model_weights.pth'))  # Remplacez par le bon chemin et la ligne du dessus pour utiliser le modèle déjà réentrainé\n",
        "\n",
        "# Charger la configuration personnalisée de notre modèle \"sur mesure\"\n",
        "config = LongformerConfig.from_json_file('/kaggle/input/longformer_bart_fine_tuned_obs/pytorch/default/1/config.json')\n",
        "#config = LongformerConfig.from_json_file('/kaggle/working/longformer_bart_finetuned/config.json') #idem : # Remplacez par le bon chemin et la ligne du dessus pour utiliser le modèle déjà réentrainé\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-11T16:42:24.268870Z",
          "iopub.execute_input": "2025-01-11T16:42:24.269197Z",
          "iopub.status.idle": "2025-01-11T16:42:30.106315Z",
          "shell.execute_reply.started": "2025-01-11T16:42:24.269167Z",
          "shell.execute_reply": "2025-01-11T16:42:30.105648Z"
        },
        "id": "fueSunYeMA0u"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mise en forme des données \"OBS\" de test pour le test du modèle."
      ],
      "metadata": {
        "id": "MkPDMrsIMA0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Définir le chemin du dossier contenant les articles\n",
        "dossier_articles_test = \"/kaggle/input/m-2-maliash-resume-darticles-scientifiques/test/OBS_test/articles_OBS_test\"\n",
        "\n",
        "# Liste des fichiers dans le dossier\n",
        "fichiers_articles = [f for f in os.listdir(dossier_articles_test) if f.startswith(\"article-\")]\n",
        "\n",
        "# Dictionnaire pour stocker les articles par identifiant (avec contenu des fichiers)\n",
        "articles_test = {}\n",
        "\n",
        "# Remplir le dictionnaire avec les fichiers en fonction des identifiants\n",
        "for fichier in fichiers_articles:\n",
        "    # Extraire l'identifiant du fichier article\n",
        "    identifiant = fichier.split(\"-\")[1].split(\".\")[0]\n",
        "\n",
        "    # Lire le contenu du fichier\n",
        "    with open(os.path.join(dossier_articles_test, fichier), 'r') as f:\n",
        "        contenu_article = f.read()\n",
        "\n",
        "    # Ajouter l'article et son identifiant au dictionnaire\n",
        "    articles_test[identifiant] = {\"article\": contenu_article}\n",
        "\n",
        "# Convertir le dictionnaire en DataFrame\n",
        "df_articles_test = pd.DataFrame.from_dict(articles_test, orient='index')\n",
        "\n",
        "# Réinitialiser l'index pour que 'identifiant' devienne une colonne normale\n",
        "df_articles_test.reset_index(inplace=True)\n",
        "\n",
        "# Renommer les colonnes\n",
        "df_articles_test.columns = ['identifiant', 'article']\n",
        "\n",
        "# Vérifier le DataFrame\n",
        "print(df_articles_test.head())\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-11T16:45:32.080682Z",
          "iopub.execute_input": "2025-01-11T16:45:32.081038Z",
          "iopub.status.idle": "2025-01-11T16:45:32.284027Z",
          "shell.execute_reply.started": "2025-01-11T16:45:32.081009Z",
          "shell.execute_reply": "2025-01-11T16:45:32.283179Z"
        },
        "id": "DZVpO4XiMA0w"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenisation des données et génération des résumés pour les articles de test de type \"OBS\""
      ],
      "metadata": {
        "id": "cUhNn-U7MA0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import LongformerTokenizer, BartTokenizer, BartForConditionalGeneration\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "# Initialiser les tokenizers Longformer et BART\n",
        "longformer_tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
        "bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "\n",
        "# Charger un modèle BART pour la génération de résumé\n",
        "#model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
        "model = model\n",
        "\n",
        "\n",
        "def generer_resume(article, model, tokenizer, max_length=150):\n",
        "    \"\"\"\n",
        "    Fonction pour générer un résumé d'un article en utilisant notre modèle Longformer-BART.\n",
        "    \"\"\"\n",
        "    # Tokenisation de l'article\n",
        "    inputs = tokenizer(article, return_tensors=\"pt\", truncation=True, padding=True, max_length=1024)\n",
        "\n",
        "    # Si le modèle est en mode CPU, déplacer les entrées et le modèle sur le bon périphérique\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "    # Obtenir les sorties du modèle\n",
        "    with torch.no_grad():\n",
        "        # Appeler la méthode forward du modèle en utilisant les bons paramètres\n",
        "        outputs = model(input_ids=inputs['input_ids'],\n",
        "                        attention_mask=inputs['attention_mask'],  # Remarque: utiliser 'attention_mask' ici\n",
        "                        decoder_input_ids=inputs['input_ids'],  # Utiliser les input_ids comme entrée pour le décodeur\n",
        "                        labels=None)\n",
        "\n",
        "    # Décoder le résumé depuis la sortie du modèle\n",
        "    # Utilisation de la méthode de génération du modèle BART sur les sorties de Longformer\n",
        "    summary_ids = model.bart.generate(\n",
        "        inputs['input_ids'],\n",
        "        max_length=max_length,\n",
        "        num_beams=4,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    # Décoder les IDs générés pour obtenir le résumé\n",
        "    resume = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return resume\n",
        "\n",
        "def traiter_ensemble_test(dataset_RCT, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Fonction qui traite l'ensemble du dataset_RCT et génère des résumés pour chaque article.\n",
        "    \"\"\"\n",
        "    # Convertir le dataset en DataFrame\n",
        "    df = pd.DataFrame(dataset_RCT)\n",
        "\n",
        "    # Créer une nouvelle colonne pour les résumés générés\n",
        "    df['résumé'] = df['article'].apply(lambda x: generer_resume(x, model, tokenizer))\n",
        "\n",
        "    return df[['identifiant', 'article', 'résumé']]\n",
        "\n",
        "\n",
        "# Traiter l'ensemble de test et générer les résumés\n",
        "resultats_obs = traiter_ensemble_test(df_articles_test, model, bart_tokenizer)\n",
        "\n",
        "# Afficher les résultats\n",
        "print(resultats_obs)\n",
        "\n",
        "# Supprimer la colonne 'article' et renommer les colonnes dans 'resultats' (pour mise au bon format pour la compétition)\n",
        "resultats_obs = resultats_obs.drop(columns=['article'])  # Supprimer la colonne 'article'\n",
        "resultats_obs = resultats_obs.rename(columns={'identifiant': 'id', 'résumé': 'abstract'})  # Renommer les colonnes\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-11T00:10:19.283949Z",
          "iopub.execute_input": "2025-01-11T00:10:19.284294Z",
          "iopub.status.idle": "2025-01-11T00:10:20.046720Z",
          "shell.execute_reply.started": "2025-01-11T00:10:19.284267Z",
          "shell.execute_reply": "2025-01-11T00:10:20.045554Z"
        },
        "id": "uHwyoBb2MA0x"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chargement de notre modèle fine-tuné sur les données \"RCT\" et constitué à partir des deux modèles bart et longformer, pour pouvoir l'utiliser pour l'ensemble de test et soumettre nos résumés pour la compétition"
      ],
      "metadata": {
        "id": "7ErDIRn6MA0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Charger le modèle fine tuné afin de l'utiliser\n",
        "import torch\n",
        "from transformers import LongformerModel, BartForConditionalGeneration, LongformerConfig\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "# Définir la classe LongformerBart\n",
        "class LongformerBart(nn.Module):\n",
        "    def __init__(self, longformer_model, bart_model):\n",
        "        super(LongformerBart, self).__init__()\n",
        "        self.longformer = longformer_model\n",
        "        self.bart = bart_model\n",
        "        self.longformer_projection = nn.Linear(768, 1024)  # Projeter de 768 à 1024\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, decoder_input_ids=None, labels=None):\n",
        "        encoder_outputs = self.longformer(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        encoder_hidden_states = encoder_outputs.last_hidden_state\n",
        "        encoder_hidden_states = self.longformer_projection(encoder_hidden_states)  # Appliquer la projection\n",
        "        decoder_outputs = self.bart(input_ids=decoder_input_ids, labels=labels, encoder_outputs=encoder_hidden_states)\n",
        "        return decoder_outputs\n",
        "\n",
        "# Charger Longformer et BART\n",
        "longformer_model = LongformerModel.from_pretrained('allenai/longformer-base-4096')\n",
        "bart = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Charger les poids de notre modèle fine-tuné\n",
        "model = LongformerBart(longformer_model, bart)\n",
        "model.load_state_dict(torch.load('/kaggle/input/longformer_bart_fine_tuned_rct/pytorch/default/1/model_weights.pth'))\n",
        "#model.load_state_dict(torch.load('/kaggle/working/longformer_bart_finetuned_rct/model_weigths.pth')) # Remplacez par le bon chemin et cette ligne si vous réentrainez le modèle\n",
        "\n",
        "\n",
        "\n",
        "# Charger la configuration\n",
        "# Notez que vous devez avoir la configuration JSON pour le modèle personnalisé\n",
        "config = LongformerConfig.from_json_file('/kaggle/input/longformer_bart_fine_tuned_rct/pytorch/default/1/config.json')\n",
        "#config = LongformerConfig.from_json_file('/kaggle/working/longformer_bart_finetuned_rct/config.json') #idem : Remplacez par le bon chemin et cette ligne si vous réentrainez le modèle\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-11T17:02:06.506168Z",
          "iopub.execute_input": "2025-01-11T17:02:06.506543Z",
          "iopub.status.idle": "2025-01-11T17:02:30.353611Z",
          "shell.execute_reply.started": "2025-01-11T17:02:06.506486Z",
          "shell.execute_reply": "2025-01-11T17:02:30.352896Z"
        },
        "id": "MmBqIbG4MA0y"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mise en forme des données \"RCT\" de test pour le test du modèle."
      ],
      "metadata": {
        "id": "IIXLnp5HMA0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Mise en forme des données à tester\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Définir le chemin du dossier contenant les articles\n",
        "dossier_articles_test = \"/kaggle/input/m-2-maliash-resume-darticles-scientifiques/test/RCT_test/articles_RCT_test\"\n",
        "\n",
        "# Liste des fichiers dans le dossier\n",
        "fichiers_articles = [f for f in os.listdir(dossier_articles_test) if f.startswith(\"article-\")]\n",
        "\n",
        "# Dictionnaire pour stocker les articles par identifiant (avec contenu des fichiers)\n",
        "articles_test = {}\n",
        "\n",
        "# Remplir le dictionnaire avec les fichiers en fonction des identifiants\n",
        "for fichier in fichiers_articles:\n",
        "    # Extraire l'identifiant du fichier article\n",
        "    identifiant = fichier.split(\"-\")[1].split(\".\")[0]\n",
        "\n",
        "    # Lire le contenu du fichier\n",
        "    with open(os.path.join(dossier_articles_test, fichier), 'r') as f:\n",
        "        contenu_article = f.read()\n",
        "\n",
        "    # Ajouter l'article et son identifiant au dictionnaire\n",
        "    articles_test[identifiant] = {\"article\": contenu_article}\n",
        "\n",
        "# Convertir le dictionnaire en DataFrame\n",
        "df_articles_test = pd.DataFrame.from_dict(articles_test, orient='index')\n",
        "\n",
        "# Réinitialiser l'index pour que 'identifiant' devienne une colonne normale\n",
        "df_articles_test.reset_index(inplace=True)\n",
        "\n",
        "# Renommer les colonnes\n",
        "df_articles_test.columns = ['identifiant', 'article']\n",
        "# Vérifier le DataFrame\n",
        "print(df_articles_test.head())"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-11T17:02:40.627556Z",
          "iopub.execute_input": "2025-01-11T17:02:40.627865Z",
          "iopub.status.idle": "2025-01-11T17:02:40.881294Z",
          "shell.execute_reply.started": "2025-01-11T17:02:40.627842Z",
          "shell.execute_reply": "2025-01-11T17:02:40.880588Z"
        },
        "id": "1t-4JBATMA0y"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenisation des données et génération des résumés pour les articles de test de type \"RCT\""
      ],
      "metadata": {
        "id": "4FW4ZwZqMA0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import LongformerTokenizer, BartTokenizer, BartForConditionalGeneration\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "# Initialiser les tokenizers Longformer et BART\n",
        "longformer_tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
        "bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "\n",
        "# Charger un modèle BART pour la génération de résumé\n",
        "#model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
        "model = model\n",
        "\n",
        "def generer_resume(article, model, tokenizer, max_length=150):\n",
        "    \"\"\"\n",
        "    Fonction pour générer un résumé d'un article en utilisant notre modèle Longformer-BART.\n",
        "    \"\"\"\n",
        "    # Tokenisation de l'article\n",
        "    inputs = tokenizer(article, return_tensors=\"pt\", truncation=True, padding=True, max_length=1024)\n",
        "\n",
        "    # Si le modèle est en mode CPU, déplacer les entrées et le modèle sur le bon périphérique\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "    # Obtenir les sorties du modèle\n",
        "    with torch.no_grad():\n",
        "        # Appeler la méthode forward du modèle en utilisant les bons paramètres\n",
        "        outputs = model(input_ids=inputs['input_ids'],\n",
        "                        attention_mask=inputs['attention_mask'],  # Remarque: utilisez 'attention_mask' ici\n",
        "                        decoder_input_ids=inputs['input_ids'],  # Utiliser les input_ids comme entrée pour le décodeur\n",
        "                        labels=None)\n",
        "\n",
        "    # Décoder le résumé depuis la sortie du modèle\n",
        "    # Utilisation de la méthode de génération du modèle BART sur les sorties de Longformer\n",
        "    summary_ids = model.bart.generate(\n",
        "        inputs['input_ids'],\n",
        "        max_length=max_length,\n",
        "        num_beams=4,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    # Décoder les IDs générés pour obtenir le résumé\n",
        "    resume = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return resume\n",
        "\n",
        "def traiter_ensemble_test(dataset_RCT, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Fonction qui traite l'ensemble du dataset_RCT et génère des résumés pour chaque article.\n",
        "    \"\"\"\n",
        "    # Convertir le dataset en DataFrame\n",
        "    df = pd.DataFrame(dataset_RCT)\n",
        "\n",
        "    # Créer une nouvelle colonne pour les résumés générés\n",
        "    df['résumé'] = df['article'].apply(lambda x: generer_resume(x, model, tokenizer))\n",
        "\n",
        "    return df[['identifiant', 'article', 'résumé']]\n",
        "\n",
        "\n",
        "# Traiter l'ensemble de test et générer les résumés\n",
        "resultats_rct = traiter_ensemble_test(df_articles_test, model, bart_tokenizer)\n",
        "\n",
        "# Afficher les résultats\n",
        "print(resultats_rct)\n",
        "\n",
        "# Supprimer la colonne 'article' et renommer les colonnes dans 'resultats' (pour mise au bon format pour la compétition)\n",
        "resultats_rct = resultats_rct.drop(columns=['article'])  # Supprimer la colonne 'article'\n",
        "resultats_rct = resultats_rct.rename(columns={'identifiant': 'id', 'résumé': 'abstract'})  # Renommer les colonnes"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-30T22:44:08.142043Z",
          "iopub.execute_input": "2024-12-30T22:44:08.142326Z",
          "iopub.status.idle": "2024-12-30T22:44:38.602767Z",
          "shell.execute_reply.started": "2024-12-30T22:44:08.142269Z",
          "shell.execute_reply": "2024-12-30T22:44:38.601886Z"
        },
        "id": "o_DfGsymMA00"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fusion de nos deux fichiers de résumés générés (resultats_obs et resultats_rct) en un seul fichier submission, puis conversion en csv pour soumission à la compétition (permet d'avoir le score rouge)."
      ],
      "metadata": {
        "id": "A931oDBSMA02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Concaténer les deux DataFrames\n",
        "resultats = pd.concat([resultats_obs, resultats_rct], ignore_index=True)\n",
        "\n",
        "# Sauvegarder le DataFrame concaténé dans un fichier CSV, si nécessaire\n",
        "resultats.to_csv('submission.csv', index=False)\n",
        "\n",
        "# Afficher le DataFrame concaténé\n",
        "print(resultats)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-01-11T17:16:40.901187Z",
          "iopub.execute_input": "2025-01-11T17:16:40.901487Z",
          "iopub.status.idle": "2025-01-11T17:16:40.922638Z",
          "shell.execute_reply.started": "2025-01-11T17:16:40.901463Z",
          "shell.execute_reply": "2025-01-11T17:16:40.921965Z"
        },
        "trusted": true,
        "id": "6_7zf68lMA03"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Résultats du modèle composé à partir de longformer + bart\n",
        "\n",
        "Le score rouge obtenu lors de la soumission à la compétition est de 0.065 : c'est à peine au-dessus du modèle \"BART\" seul (et non fine-tuné quant à lui) !\n",
        "\n",
        "Comment s'explique cela ?\n",
        "\n",
        "Nous avons vu que, pour ne pas outrepasser les capacités de la mémoire disponible via les GPU de Kaggle, nous étions obligés de nous limiter à 1024 tokens. Ce, tant lors du fine-tuning du modèle (fonction tokenize_function(examples):\n",
        "    \n",
        "    \"model_inputs = longformer_tokenizer(\n",
        "        examples[\"article\"],\n",
        "        #max_length=4096, #trop long pour le GPU de kaggle...nous en revenons donc aux fameux 1024 token max, d'où nous partions avec BART seul.\n",
        "        max_length=1024,\n",
        "        truncation=True, \")\n",
        "\n",
        "que lors de son utilisation sur les articles de test.\n",
        "\n",
        "Or, l'ensemble de ces articles sont des articles longs - environ 20000 mots de longueur par article.\n",
        "\n",
        "En pratique, les modèles de langage Longformer utilisent des tokenizers basés sur des sous-mots : certains mots peuvent être décomposés en plusieurs token, notamment les mots rares ou composés (nota : le modèle Llama - plus généraliste que les nôtres - utilisé par notre enseignant pour obtenir son score dans la compétition, utilise le même type de tokenisation basée sur des sous-mots). Inversement, les mots courants sont tokenisés par des token uniques, ce qui permet d'obtenir une représentation très compacte par ces tokenizers : une longueur de 20000 mots équivaut à entre 3000 et 5000 token pour un article.\n",
        "Ce n'est pas supérieur aux 4096 token que peut prendre en entrée le modèle longformer que nous avons utilisé - mais c'est très supérieur aux 1024 token maximum, auxquels nous avons été obligés de \"brider\" ce modèle sur la plateforme kaggle, du fait des contraintes de GPU.\n",
        "En clair : nos résumés sont faits sur la base uniquement (à peu près) du premier tiers de chaque article scientifique !\n",
        "Ce, tant lors de l'entrainement, que lors du test.\n",
        "Ainsi, les deux autres tiers de l'article ne sont en fait pas résumés du tout : ils ne sont tout simplement pas pris en compte, alors qu'ils contiennent généralement des éléments tout aussi importants que le début de l'article, à inclure dans le résumé.\n",
        "Et cette situation est finalement identique avec BART seul (qui est naturellement plafonné à 1024 token), et avec notre modèle longformer-bart (que nous sommes obligés de \"brider\" à 1024 token du fait des contraintes de mémoire du GPU sur kaggle).\n",
        "\n",
        "Ceci explique les scores très médiocres obtenus.\n",
        "\n",
        "Pour faire mieux, tout en tenant compte des limites de notre plateforme (kaggle), il nous faut donc nous diriger vers d'autres stratagèmes, applicables dans le temps qu'il nous reste pour cette compétition et le rendu de ce projet.\n",
        "\n",
        "Nous choisissons donc de tester le \"chunking\" des articles scientifiques de l'ensemble de test, qui est le plus simple à mettre en oeuvre.\n",
        "\n",
        "\n",
        "\n",
        "        "
      ],
      "metadata": {
        "id": "8oKXqOLdMA05"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test de la méthode du résumé par morceaux des articles de l'ensemble de test\n",
        "\n",
        "Concrètement, nous allons donc subdiviser chaque article à résumer en \"morceaux\" de 1024 token chacun, puis faire résumer par notre modèle chaque morceau, et, simplement, concaténer à la suite tous ces résumés entre eux pour former un résumé de l'article entier.\n",
        "\n",
        "La méthode est frustre, car on n'aura pas les liens et la fluidité entre les différents \"morceaux\".\n",
        "En outre, couper à 1024 token peut couper au milieu d'une phrase, par exemple - mais c'est toujours mieux que de prendre le résumé d'un seul tiers de l'article pour le résumé de l'article tout entier ! Et, surtout, c'est faisable dans le délai et avec les moyens matériels et humains dont nous disposons.\n"
      ],
      "metadata": {
        "id": "c7dFLaRoMA05"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## articles \"OBS\""
      ],
      "metadata": {
        "id": "Lf72FodlMA05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import LongformerTokenizer, BartTokenizer, BartForConditionalGeneration\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Initialiser les tokenizers Longformer et BART\n",
        "longformer_tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
        "bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "\n",
        "# Charger un modèle BART pour la génération de résumé\n",
        "#model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
        "model = model\n",
        "\n",
        "\n",
        "def generer_resume(article, model, tokenizer, max_length=150,chunk_size=1024):\n",
        "    \"\"\"\n",
        "    Fonction pour générer un résumé d'un article en utilisant notre modèle Longformer-BART.\n",
        "    \"\"\"\n",
        "\n",
        "    # Diviser le texte en morceaux\n",
        "    #morceaux = [article[i:i + chunk_size] for i in range(0, len(article), chunk_size)]\n",
        "    morceaux = [\n",
        "    (article[i:i + chunk_size] + ' ' * (chunk_size - len(article[i:i + chunk_size])))\n",
        "    for i in range(0, len(article), chunk_size)\n",
        "]\n",
        "    resumes = []\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    for morceau in morceaux:\n",
        "        # Générer un résumé pour chaque morceau\n",
        "        inputs = tokenizer(morceau, padding='max_length', return_tensors=\"pt\", truncation=True, max_length=chunk_size)\n",
        "        # Si le modèle est en mode CPU, déplacer les entrées et le modèle sur le bon périphérique\n",
        "\n",
        "        inputs = {key: (value.to(device) if isinstance(value, torch.Tensor) else value) for key, value in inputs.items()}\n",
        "        #inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "\n",
        "         # Obtenir les sorties du modèle\n",
        "        with torch.no_grad():\n",
        "            # Appeler la méthode forward du modèle en utilisant les bons paramètres\n",
        "            outputs = model(input_ids=inputs['input_ids'],\n",
        "                            attention_mask=inputs['attention_mask'],  # Remarque: utiliser 'attention_mask' ici\n",
        "                            decoder_input_ids=inputs['input_ids'],  # Utiliser les input_ids comme entrée pour le décodeur\n",
        "                            labels=None)\n",
        "        # Décoder le résumé depuis la sortie du modèle\n",
        "        # Utilisation de la méthode de génération du modèle BART sur les sorties de Longformer\n",
        "        summary_ids = model.bart.generate(\n",
        "            inputs['input_ids'],\n",
        "            max_length=max_length,\n",
        "            num_beams=4,\n",
        "            early_stopping=True\n",
        "            )\n",
        "\n",
        "\n",
        "        resumes.append(tokenizer.decode(summary_ids[0], skip_special_tokens=True))\n",
        "\n",
        "    # Combiner les résumés partiels en un seul texte\n",
        "    resume = \" \".join(resumes)\n",
        "\n",
        "    return resume\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def traiter_ensemble_test(dataset_RCT, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Fonction qui traite l'ensemble du dataset_RCT et génère des résumés pour chaque article.\n",
        "    \"\"\"\n",
        "    # Convertir le dataset en DataFrame\n",
        "    df = pd.DataFrame(dataset_RCT)\n",
        "\n",
        "    # Créer une nouvelle colonne pour les résumés générés\n",
        "    df['résumé'] = df['article'].apply(lambda x: generer_resume(x, model, tokenizer))\n",
        "\n",
        "    return df[['identifiant', 'article', 'résumé']]\n",
        "\n",
        "\n",
        "\n",
        "# Traiter l'ensemble de test et générer les résumés\n",
        "resultats_obs = traiter_ensemble_test(df_articles_test, model, bart_tokenizer)\n",
        "\n",
        "# Afficher les résultats\n",
        "print(resultats_obs)\n",
        "\n",
        "# Supprimer la colonne 'article' et renommer les colonnes dans 'resultats' (pour mise au bon format pour la compétition)\n",
        "resultats_obs = resultats_obs.drop(columns=['article'])  # Supprimer la colonne 'article'\n",
        "resultats_obs = resultats_obs.rename(columns={'identifiant': 'id', 'résumé': 'abstract'})  # Renommer les colonnes\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "v5mW1GcqMA06"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Articles \"RCT\""
      ],
      "metadata": {
        "id": "8cAXKO9TMA08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import LongformerTokenizer, BartTokenizer, BartForConditionalGeneration\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Initialiser les tokenizers Longformer et BART\n",
        "longformer_tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
        "bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "\n",
        "# Charger un modèle BART pour la génération de résumé\n",
        "#model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
        "model = model\n",
        "\n",
        "\n",
        "def generer_resume(article, model, tokenizer, max_length=150,chunk_size=1024):\n",
        "    \"\"\"\n",
        "    Fonction pour générer un résumé d'un article en utilisant notre modèle Longformer-BART.\n",
        "    \"\"\"\n",
        "\n",
        "    # Diviser le texte en morceaux\n",
        "    #morceaux = [article[i:i + chunk_size] for i in range(0, len(article), chunk_size)]\n",
        "    morceaux = [\n",
        "    (article[i:i + chunk_size] + ' ' * (chunk_size - len(article[i:i + chunk_size])))\n",
        "    for i in range(0, len(article), chunk_size)\n",
        "]\n",
        "    resumes = []\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    for morceau in morceaux:\n",
        "        # Générer un résumé pour chaque morceau\n",
        "        inputs = tokenizer(morceau, padding='max_length', return_tensors=\"pt\", truncation=True, max_length=chunk_size)\n",
        "        # Si le modèle est en mode CPU, déplacer les entrées et le modèle sur le bon périphérique\n",
        "\n",
        "        inputs = {key: (value.to(device) if isinstance(value, torch.Tensor) else value) for key, value in inputs.items()}\n",
        "        #inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "\n",
        "         # Obtenir les sorties du modèle\n",
        "        with torch.no_grad():\n",
        "            # Appeler la méthode forward du modèle en utilisant les bons paramètres\n",
        "            outputs = model(input_ids=inputs['input_ids'],\n",
        "                            attention_mask=inputs['attention_mask'],  # Remarque: utiliser 'attention_mask' ici\n",
        "                            decoder_input_ids=inputs['input_ids'],  # Utiliser les input_ids comme entrée pour le décodeur\n",
        "                            labels=None)\n",
        "        # Décoder le résumé depuis la sortie du modèle\n",
        "        # Utilisation de la méthode de génération du modèle BART sur les sorties de Longformer\n",
        "        summary_ids = model.bart.generate(\n",
        "            inputs['input_ids'],\n",
        "            max_length=max_length,\n",
        "            num_beams=4,\n",
        "            early_stopping=True\n",
        "            )\n",
        "\n",
        "\n",
        "        resumes.append(tokenizer.decode(summary_ids[0], skip_special_tokens=True))\n",
        "\n",
        "    # Combiner les résumés partiels en un seul texte\n",
        "    resume = \" \".join(resumes)\n",
        "\n",
        "    return resume\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def traiter_ensemble_test(dataset_RCT, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Fonction qui traite l'ensemble du dataset_RCT et génère des résumés pour chaque article.\n",
        "    \"\"\"\n",
        "    # Convertir le dataset en DataFrame\n",
        "    df = pd.DataFrame(dataset_RCT)\n",
        "\n",
        "    # Créer une nouvelle colonne pour les résumés générés\n",
        "    df['résumé'] = df['article'].apply(lambda x: generer_resume(x, model, tokenizer))\n",
        "\n",
        "    return df[['identifiant', 'article', 'résumé']]\n",
        "\n",
        "\n",
        "\n",
        "# Traiter l'ensemble de test et générer les résumés\n",
        "resultats_rct = traiter_ensemble_test(df_articles_test, model, bart_tokenizer)\n",
        "\n",
        "# Afficher les résultats\n",
        "print(resultats_rct)\n",
        "\n",
        "# Supprimer la colonne 'article' et renommer les colonnes dans 'resultats' (pour mise au bon format pour la compétition)\n",
        "resultats_rct = resultats_rct.drop(columns=['article'])  # Supprimer la colonne 'article'\n",
        "resultats_rct = resultats_rct.rename(columns={'identifiant': 'id', 'résumé': 'abstract'})  # Renommer les colonnes"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-11T17:02:56.202754Z",
          "iopub.execute_input": "2025-01-11T17:02:56.203067Z",
          "iopub.status.idle": "2025-01-11T17:14:58.522648Z",
          "shell.execute_reply.started": "2025-01-11T17:02:56.203041Z",
          "shell.execute_reply": "2025-01-11T17:14:58.521807Z"
        },
        "id": "1d7xjxaAMA08"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fusion de nos deux fichiers de résumés générés (resultats_obs et resultats_rct) en un seul fichier submission, puis conversion en csv pour soumission à la compétition (permet d'avoir le score rouge)."
      ],
      "metadata": {
        "id": "dEfKXXy0MA0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Concaténer les deux DataFrames\n",
        "resultats = pd.concat([resultats_obs, resultats_rct], ignore_index=True)\n",
        "\n",
        "# Sauvegarder le DataFrame concaténé dans un fichier CSV, si nécessaire\n",
        "resultats.to_csv('submission.csv', index=False)\n",
        "\n",
        "# Afficher le DataFrame concaténé\n",
        "print(resultats)"
      ],
      "metadata": {
        "trusted": true,
        "id": "E-BCguRyMA1A"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Résultats du \"chunking\" des articles à résumer\n",
        "\n",
        "Le score rouge est doublé (nous passons de 0.065 à 0.135 sur l'échantillon testé lors de la soumission à la compétition - 0.151 sur la totalité de la soumission, résultat visible une fois la compétition close).\n",
        "La méthode, pour \"frustre\" qu'elle soit, est donc efficace.\n",
        "\n",
        "Elle est \"frustre\", parce que comme dit en introduction, on n'implémente rien qui permette au modèle de faire du lien entre les différents \"chunk\" de chaque article, et parce que côté résumé, on se contente de mettre bout à bout les sous-résumés obtenus, pour construire le résumé total de l'article entier.\n",
        "\n",
        "Elle est \"frustre\" aussi pour une autre raison : nous n'avons effectué cette subdivision des articles que sur l'ensemble de test. Mais en revanche, nous fine-tunons toujours notre modèle sur les articles du train set sans les avoir, quant à eux, subdivisés en morceaux de 1024 token chacun !\n",
        "En d'autres termes, nous fine-tunons toujours notre modèle à une tâche de résumé du premier tiers de chaque article scienfique...en lui fournissant comme étiquette le résumé total de ce même article.\n",
        "Mais on a estimé, et le score nous donne raison, que le modèle pré-entrainé était déjà pré-entraîné, et que le fine-tuner avec cette approximation était en fait déjà \"pas trop mal\". Surtout, le coût de l'implémentation du chunking sur l'ensemble d'entrainement, plus lourd en durée de travail humain et en temps de travail de la machine nécessaires, que sur l'ensemble de test (*), était peu compatible avec le délai nous 0restant alors pour participer à la compétition.\n",
        "\n",
        "\n",
        "(*) Il nous faut préciser ici le caractère, malheureusement chronophage, de la prise en mains de la plateforme kaggle, que nous n'avions jamais utilisée, ainsi que le caractère malcommode, pour les novices que nous étions, de son dossier \"working\", à la capacité réduite par rapport à nos besoins, et dont la touche \"download\" buguait, nous forçant à reprendre de zéro, et ce à plusieurs reprises, le fine-tuning de nos modèles durant de longues heures... Ce temps n'a pu être utilisé à des explorations plus en rapport direct avec les LLM, ce qui est source de regrets pour nous."
      ],
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "Iw9ALXegMA1B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Limites et perspectives\n",
        "\n",
        "\n",
        "Comme dit à l'instant, nous pourrions améliorer encore le score en approfondissant et affinant la stratégie de chunking de nos articles.\n",
        "Par exemple, nous apprenons que nous pourrions implémenter une couche d'attention hiérarchique qui résume les chunks avant de les passer à la partie BART de notre modèle, afin que ce dernier puisse avoir une compréhension des relations entre ces différents chunk.\n",
        "Bien évidemment, nous pourrions mettre en place le même chunking pour améliorer le fine-tuning sur les articles de l'ensemble d'entrainement du modèle, car celui-ci consisterait alors réellement à mettre en lien chaque article en entier, avec son résumé complet.\n",
        "\n",
        "Nous avons construit un modèle utilisant les transformer \"longformer\" et \"bart\", ce qui peut poser plusieurs questions :\n",
        "\n",
        "- la première est que, au vu du faible écart entre le score rouge (exécrable) sur bart non fine-tuné, et le score de notre modèle \"hybride\" fine-tuné, le choix du modèle et son fine-tuning ont semblé finalement, dans l'environnement informatique à notre disposition, avec ses limites (1024 token...), bien moins importants que la bonne préparation (chunking...) des textes à résumer.\n",
        "Si nous avions eu le temps, nous aurions pu effectuer le fine-tuning et le test de bart seul sur les articles de test découpés en chunk, pour comparer !\n",
        "\n",
        "- ce faible écart entre bart non fine-tuné, et notre modèle \"hybride\" (dument fine-tuné), aurait sans doute été bien plus important dans un environnement de travail (capacité GPU, etc) nous permettant d'utiliser réellement les capacités de longformer (4096 token). Dit autrement, la construction du modèle \"hybride\", pour une utilisation dans l'environnement kaggle, a été du temps de travail inutilement utilisé (sauf pour notre apprentissage bien sûr, mais si nous étions en entreprise ce temps aurait constitué une perte inutile - et nous y avons consacré un temps assez long).\n",
        "\n",
        "En revanche, dans un autre environnement informatique, plus capacitaire, ce temps de construction aurait été très utile, puisqu'évitant de devoir effectuer le moindre chunking ensuite.\n",
        "\n",
        "Un apprentissage pratique issu de la réalisation de ce projet est donc, pour nous, qu'il faut en premier bien connaître l'environnement informatique dans lequel nous travaillons, et ses capacités, puis construire nos solutions dans ce cadre, en les y adaptant dès le départ. Et non construire des solutions puis se rendre compte que l'environnement nous contraint à les brider.\n",
        "\n",
        "- la seconde est que nous avons estimé qu'un résumé \"de qualité\" était un résumé effectué avec l'aide d'un modèle pouvant générer du texte.\n",
        "C'est à dire qu'un résumé \"de qualité\" était ce qu'on appelle un résumé abstractif, qui a en effet pour qualité d'être plus fluide que la simple juxtaposition des phrases jugées importantes par le modèle, et par suite (simplement) copiées du texte par ses soins (cas des résumés extractifs).\n",
        "\n",
        "Cependant, dans un article récent, Adrien Guille et Saïd Raoufdine remarquent que : \"Toutefois, les résumés abstractifs générés par ces outils peuvent être\n",
        "biaisés, non informatifs voire même mensongers. Ainsi, dans certains contextes sensibles (e.g. résumé d’articles scientifiques ou d’articles de presse), le résumé\n",
        "extractif reste une approche plus fiable.\" (Raoufdine Said, Adrien Guille. Résumé interactif de documents. 24ème Conférence sur l’Extraction\n",
        "et la Gestion des Connaissances (EGC), Jan 2024, Dijon, France. hal-04448464 - https://hal.science/hal-04448464v1/file/1002963.pdf)\n",
        "\n",
        "En outre, notre chunking \"frustre\" fait probablement perdre pas mal de ses qualités de fluidité au résumé abstractif tel que généré en pratique sur nos articles de test...\n",
        "\n",
        "Deux raisons pour lesquelles finalement, le modèle longformer seul (qui n'est pas un modèle générateur de texte), aurait pu être pertinent, plutôt qu'un modèle générant du texte (comme bart) ou comportant un sous-modèle générant du texte (comme notre hybride \"longformer-bart\"). En effet, il avait toutes les qualités requises pour être un bon modèle de résumé extractif sur nos articles scientifiques longs."
      ],
      "metadata": {
        "id": "Ft7LUqk4MA1B"
      }
    }
  ]
}