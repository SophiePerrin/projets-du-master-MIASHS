{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4","include_colab_link":true},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"590d7930119a40ec80852b61f1d745f1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c7494184df6a4d76be440b3aa8e450ed","IPY_MODEL_20f371d49459494dab86ffabc022bd8b","IPY_MODEL_6f42d7009347405b9f171086ff5d006f"],"layout":"IPY_MODEL_e15000d777124cf1a0d716f9a5dfaf94"}},"c7494184df6a4d76be440b3aa8e450ed":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_087cef66d27d488aaaaed55dd832b0e8","placeholder":"​","style":"IPY_MODEL_8c661e2b93de4efeaef3280a2fec9e24","value":"Map: 100%"}},"20f371d49459494dab86ffabc022bd8b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1c18877d0dc04869878441ec29cebda9","max":402,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1895bff15c9a4779a1e15f01e88dd63d","value":402}},"6f42d7009347405b9f171086ff5d006f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f47d699049e94e018ef36107abf72b43","placeholder":"​","style":"IPY_MODEL_4eb60c52894a421ca2644f12b86a1a62","value":" 402/402 [00:00&lt;00:00, 432.53 examples/s]"}},"e15000d777124cf1a0d716f9a5dfaf94":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"087cef66d27d488aaaaed55dd832b0e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c661e2b93de4efeaef3280a2fec9e24":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1c18877d0dc04869878441ec29cebda9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1895bff15c9a4779a1e15f01e88dd63d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f47d699049e94e018ef36107abf72b43":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4eb60c52894a421ca2644f12b86a1a62":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":90248,"databundleVersionId":10480827,"sourceType":"competition"},{"sourceId":214728,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":183064,"modelId":205264},{"sourceId":215510,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":183739,"modelId":205927}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Confection de résumés automatiques d'articles scientifiques par un modèle de langue pré-entraîné : étude et comparaison des performances de plusieurs modèles dans l'environnement informatique de kaggle.\n\n**Auteur.e.s :** Sophie Perrin, Emmanuelle Kouadio, Belbaron Nzuego, Jean Marius Kombou, Sylvano Kpedotossi, tous et toutes étudiant.e.s du master 2 MIASHS, pour le cours *Representation learning for NLP* @ Master 2 MALIA et MIASHS.\n\n**Equipe :** les léopards","metadata":{"id":"Jm3aCEp_w2w2"}},{"cell_type":"markdown","source":"## Préparation de l'environnement\n\n1. Se connecter ou se créer un compte sur https://huggingface.co/\n2. Se créer un nouveau token d'accès : https://huggingface.co/settings/tokens\n3. Exécuter le code ci-dessous\n\nNote importante pour la vitesse de calcul : pour activer l'accélération GPU dans votre notebook Kaggle :\n\n    Ouvrez le menu « Settings » : Dans votre notebook Kaggle, cliquez sur le menu « Accelerator » en haut de l'écran, et choisissez un GPU.\n\n    Dans ce même menu, vérifiez qu'il n'est pas écrit \"turn off internet\" - sinon cliquer dessus pour rétablir l'accès au reste d'internet depuis kaggle. Si vous n'avez ni \"turn off internet\" ni \"turn on internet\" d'écrit, alors il faut ajouter votre téléphone et votre photo pour certifier votre identité pour le compte, avant de pouvoir accéder au reste d'internet depuis kaggle (nécessaire pour charger les modèles de huggingface et même les packages de python...!).\n\n","metadata":{"id":"oiCaaZ8YC8hZ"}},{"cell_type":"code","source":"# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\nimport kagglehub\nkagglehub.login()","metadata":{"id":"3pp0itMoj78l","outputId":"6fc6780e-9cd4-447f-c06d-f7f325e10549","trusted":true,"execution":{"iopub.status.busy":"2025-01-12T01:26:23.494678Z","iopub.execute_input":"2025-01-12T01:26:23.494996Z","iopub.status.idle":"2025-01-12T01:26:23.512380Z","shell.execute_reply.started":"2025-01-12T01:26:23.494971Z","shell.execute_reply":"2025-01-12T01:26:23.511407Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://www.kaggle.com/static/images/site-logo.png\\nalt=\\'Kaggle…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a1246ae4116445a99ebf37524c7ff26"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"\n# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n# THEN FEEL FREE TO DELETE THIS CELL.\n# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n# NOTEBOOK.\n\nm_2_maliash_resume_darticles_scientifiques_path = kagglehub.competition_download('m-2-maliash-resume-darticles-scientifiques')\n\nprint('Data source import complete.')","metadata":{"id":"z-S3zGw4BfUX","outputId":"b91c7e68-5d81-41a5-e693-63e126680a97","trusted":true,"execution":{"iopub.status.busy":"2025-01-12T01:26:41.275755Z","iopub.execute_input":"2025-01-12T01:26:41.276042Z","iopub.status.idle":"2025-01-12T01:26:42.382260Z","shell.execute_reply.started":"2025-01-12T01:26:41.276020Z","shell.execute_reply":"2025-01-12T01:26:42.381559Z"}},"outputs":[{"name":"stdout","text":"Data source import complete.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"#Pour créer le \"HF_TOKEN\" dans Kaggle, aller dans \"Add-ons\" (dans les menus de ce notebook, juste à gauche de \"Help\").\n#Une fois là, aller dans \"secrets\", puis ça marche à peu près comme dans colab : il faut nommer la variable (HF_TOKEN) et \n#y copier sa valeur.\n\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"HF_TOKEN\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-12T01:27:33.133435Z","iopub.execute_input":"2025-01-12T01:27:33.133769Z","iopub.status.idle":"2025-01-12T01:27:33.234930Z","shell.execute_reply.started":"2025-01-12T01:27:33.133741Z","shell.execute_reply":"2025-01-12T01:27:33.233876Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mBackendError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-f552d14275a0>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkaggle_secrets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUserSecretsClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0muser_secrets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUserSecretsClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0msecret_value_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_secrets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_secret\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"HF_TOKEN\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kaggle_secrets.py\u001b[0m in \u001b[0;36mget_secret\u001b[0;34m(self, label)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;34m'Label'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         }\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mresponse_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweb_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_post_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET_USER_SECRET_BY_LABEL_ENDPOINT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'secret'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             raise BackendError(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kaggle_web_client.py\u001b[0m in \u001b[0;36mmake_post_request\u001b[0;34m(self, data, endpoint, timeout)\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mresponse_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wasSuccessful'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'result'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                     raise BackendError(\n\u001b[0m\u001b[1;32m     50\u001b[0m                         f'Unexpected response from the service. Response: {response_json}.')\n\u001b[1;32m     51\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'result'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mBackendError\u001b[0m: Unexpected response from the service. Response: {'errors': ['No user secrets exist for kernel id 74487876 and label HF_TOKEN.'], 'error': {'code': 5, 'details': []}, 'wasSuccessful': False}."],"ename":"BackendError","evalue":"Unexpected response from the service. Response: {'errors': ['No user secrets exist for kernel id 74487876 and label HF_TOKEN.'], 'error': {'code': 5, 'details': []}, 'wasSuccessful': False}.","output_type":"error"}],"execution_count":7},{"cell_type":"markdown","source":"# Préparation des données d'entraînement pour leur utilisation par le modèle","metadata":{"id":"hoSW39T9djGs"}},{"cell_type":"markdown","source":"On a à faire les résumés automatisés de deux types d'articles différents : \n\n- les articles de type \"OBS\"\n\n- et les articles de type \"RCT\"\n\nNous allons donc fine-tuner le modèle spécifiquement sur chaque catégorie d'article, afin que sa spécialisation soit, pour chaque catégorie, au plus proche pour elle.","metadata":{}},{"cell_type":"markdown","source":"## Données du dossier OBS","metadata":{}},{"cell_type":"markdown","source":"On crée un dictionnaire qui apparie les articles de l'ensemble d'entraînement (\"fine tuning\" puisque le modèle est déjà pré-entraîné) et leurs résumés par leur identifiant commun.\n\nA noter que quelques résumés n'ont pas d'article qui leur correspond !\nIls ne sont pas inclus dans le dictionnaire qui nous servira d'ensemble d'entrainement.","metadata":{"id":"iSOUl2N1dtAz"}},{"cell_type":"code","source":"import os\n\n# Chemins dans Kaggle des dossiers contenant les fichiers\ndossier_abstracts = \"/kaggle/input/m-2-maliash-resume-darticles-scientifiques/train/OBS/abstracts_OBS/\"\ndossier_articles = \"/kaggle/input/m-2-maliash-resume-darticles-scientifiques/train/OBS/articles_OBS/\"\n\n\n# Liste des fichiers dans chaque dossier\nfichiers_abstracts = [f for f in os.listdir(dossier_abstracts) if f.startswith(\"abstract-\")]\nfichiers_articles = [f for f in os.listdir(dossier_articles) if f.startswith(\"article-\")]\n\n# Dictionnaires pour stocker les fichiers par identifiant\nabstracts = {}\narticles = {}\n\n# Remplir les dictionnaires avec les fichiers en fonction des identifiants\nfor fichier in fichiers_abstracts:\n    # Extraire l'identifiant du fichier abstract\n    identifiant = fichier.split(\"-\")[1].split(\".\")[0]\n     #fichier.split(\"-\") : La méthode split(\"-\") divise le nom du fichier\n    #en une liste de sous-chaînes en utilisant le caractère \"-\" comme séparateur.\n    #Par exemple, si le fichier est \"abstract-123.txt\", fichier.split(\"-\") renverra la liste [\"abstract\", \"123.txt\"].\n    #fichier.split(\"-\")[1] : En prenant l'élément d'indice 1 de cette liste (c'est-à-dire \"123.txt\"), on obtient la partie du nom du fichier après le préfixe \"abstract-\".\n    #split(\".\")[0] : Ensuite, on divise cette chaîne \"123.txt\" avec split(\".\"), ce qui donne [\"123\", \"txt\"].\n    #On prend le premier élément de la liste (c'est-à-dire \"123\") qui est l'identifiant unique de l'abstract.\n    abstracts[identifiant] = fichier\n    #Cette ligne ajoute une entrée dans le dictionnaire abstracts, où la clé est : identifiant (par exemple \"123\")\n    #et la valeur est le nom du fichier : fichier (par exemple \"abstract-123.txt\").\n\nfor fichier in fichiers_articles:\n    # Extraire l'identifiant du fichier article\n    identifiant = fichier.split(\"-\")[1].split(\".\")[0]\n    articles[identifiant] = fichier\n\n# Dictionnaire pour stocker les appariements\nappariements = {}\n\n# Liste des abstracts et articles non-appariés\nnon_apparies_abstracts = []\nnon_apparies_articles = []\n\n# Appariement des fichiers abstracts et articles par identifiant\nfor identifiant in abstracts:\n    if identifiant in articles:\n        # Ajouter l'appariement au dictionnaire\n        appariements[identifiant] = {\n            \"abstract\": abstracts[identifiant],\n            \"article\": articles[identifiant]\n        }\n    else:\n        # Ajouter à la liste des abstracts non appariés\n        non_apparies_abstracts.append(abstracts[identifiant])\n\n# Vérifier les articles non-appariés\nfor identifiant in articles:\n    if identifiant not in abstracts:\n        # Ajouter à la liste des articles non appariés\n        non_apparies_articles.append(articles[identifiant])\n\n# Affichage des appariements\n\"\"\"\nprint(\"Appariements :\")\nfor identifiant, fichiers in appariements.items():\n    print(f\"Identifiant {identifiant}:\")\n    print(f\"  Abstract: {fichiers['abstract']}\")\n    print(f\"  Article: {fichiers['article']}\")\n\"\"\"\n# Affichage des abstracts non-appariés\nprint(\"\\nAbstracts non-appariés :\")\nfor abstract in non_apparies_abstracts:\n    print(abstract)\n\n# Affichage des articles non-appariés\nprint(\"\\nArticles non-appariés :\")\nfor article in non_apparies_articles:\n    print(article)","metadata":{"id":"_5wciJ57jnhk","outputId":"b9096f10-f7a2-4209-fc66-42350f644ec5","trusted":true,"execution":{"iopub.status.busy":"2025-01-11T16:00:23.434403Z","iopub.execute_input":"2025-01-11T16:00:23.434767Z","iopub.status.idle":"2025-01-11T16:00:23.524244Z","shell.execute_reply.started":"2025-01-11T16:00:23.434739Z","shell.execute_reply":"2025-01-11T16:00:23.523588Z"}},"outputs":[{"name":"stdout","text":"\nAbstracts non-appariés :\nabstract-36404343.txt\nabstract-38022520.txt\nabstract-37614109.txt\nabstract-36944082.txt\nabstract-32017677.txt\nabstract-36944050.txt\nabstract-36891751.txt\nabstract-35081022.txt\nabstract-32091358.txt\nabstract-31625835.txt\nabstract-36066965.txt\nabstract-36519326.txt\nabstract-36525381.txt\nabstract-36314570.txt\nabstract-37833688.txt\nabstract-27423055.txt\nabstract-37796016.txt\nabstract-37026745.txt\nabstract-36068298.txt\nabstract-35324507.txt\nabstract-37726286.txt\nabstract-37102598.txt\nabstract-36951168.txt\nabstract-24279685.txt\nabstract-36944043.txt\nabstract-32946618.txt\nabstract-36372692.txt\n\nArticles non-appariés :\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"On retravaille un peu la forme de ce dictionnaire pour pouvoir le convertir au format \"dataset\" de Hugging face, nécessaire pour pouvoir entraîner le modèle choisi dessus.\n","metadata":{"id":"7axwPxrAd7Di"}},{"cell_type":"code","source":"!pip install -U datasets #le \"-U\" demande l'installation de la toute dernière version du package\nfrom datasets import Dataset #importe la classe Dataset de la bibliothèque datasets de Hugging Face.\n#Cette classe est utilisée pour manipuler des ensembles de données dans un format qui peut être utilisé pour l'entraînement de modèles issus d'Hugging Face.\n\n# Limiter le nombre de threads de JAX à 1 (pour éviter les problèmes dans kaggle)\nos.environ[\"JAX_NUM_THREADS\"] = \"1\"\n\n#Notre dictionnaire appariements n'est pas tout à fait sous la bonne forme pour être transformé en dataset de Hugging Face :\n#on le remanie donc pour qu'il ait cette bonne forme.\n\n# Initialisation des listes vides\nidentifiants = []\narticles = []\nabstracts = []\n\n# Remplir les listes avec les données extraites du dictionnaire appariements\nfor identifiant, values in appariements.items():\n    identifiants.append(identifiant)         # Ajout de l'identifiant\n    articles.append(values[\"article\"])      # Ajout de l'article\n    abstracts.append(values[\"abstract\"])    # Ajout de l'abstract\n\n# Créer un dictionnaire avec les listes\ndata = {\n    \"identifiant\": identifiants,\n    \"article\": articles,\n    \"abstract\": abstracts\n}\n\n# Créer le Dataset en convertissant notre nouveau dictionnaire via l'instruction Dataset.from_dict() de Dataset\ndataset = Dataset.from_dict(data)\n\nprint(dataset)","metadata":{"id":"wJ-fvqMz7ahj","outputId":"5cc78080-3b1a-48ba-9db2-27872f4ec33c","trusted":true,"execution":{"iopub.status.busy":"2025-01-11T16:00:28.291025Z","iopub.execute_input":"2025-01-11T16:00:28.291331Z","iopub.status.idle":"2025-01-11T16:00:34.104733Z","shell.execute_reply.started":"2025-01-11T16:00:28.291307Z","shell.execute_reply":"2025-01-11T16:00:34.103917Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (18.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\nRequirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nDataset({\n    features: ['identifiant', 'article', 'abstract'],\n    num_rows: 402\n})\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## le fichier à utiliser pour l'entraînement du modèle sur les articles de type OBS sera donc \"dataset\"","metadata":{}},{"cell_type":"markdown","source":"## Données du dossier RCT","metadata":{}},{"cell_type":"markdown","source":"On procède de même pour les données des articles de type \"RCT\"","metadata":{}},{"cell_type":"code","source":"import os\n\n# Chemins dans Kaggle des dossiers contenant les fichiers\ndossier_abstracts = \"/kaggle/input/m-2-maliash-resume-darticles-scientifiques/train/RCT/abstracts_RCT/\"\ndossier_articles = \"/kaggle/input/m-2-maliash-resume-darticles-scientifiques/train/RCT/articles_RCT/\"\n\n\n# Liste des fichiers dans chaque dossier\nfichiers_abstracts = [f for f in os.listdir(dossier_abstracts) if f.startswith(\"abstract-\")]\nfichiers_articles = [f for f in os.listdir(dossier_articles) if f.startswith(\"article-\")]\n\n# Dictionnaires pour stocker les fichiers par identifiant\nabstracts = {}\narticles = {}\n\n# Remplir les dictionnaires avec les fichiers en fonction des identifiants\nfor fichier in fichiers_abstracts:\n    # Extraire l'identifiant du fichier abstract\n    identifiant = fichier.split(\"-\")[1].split(\".\")[0]\n     #fichier.split(\"-\") : La méthode split(\"-\") divise le nom du fichier\n    #en une liste de sous-chaînes en utilisant le caractère \"-\" comme séparateur.\n    #Par exemple, si le fichier est \"abstract-123.txt\", fichier.split(\"-\") renverra la liste [\"abstract\", \"123.txt\"].\n    #fichier.split(\"-\")[1] : En prenant l'élément d'indice 1 de cette liste (c'est-à-dire \"123.txt\"), on obtient la partie du nom du fichier après le préfixe \"abstract-\".\n    #split(\".\")[0] : Ensuite, on divise cette chaîne \"123.txt\" avec split(\".\"), ce qui donne [\"123\", \"txt\"].\n    #On prend le premier élément de la liste (c'est-à-dire \"123\") qui est l'identifiant unique de l'abstract.\n    abstracts[identifiant] = fichier\n    #Cette ligne ajoute une entrée dans le dictionnaire abstracts, où la clé est : identifiant (par exemple \"123\")\n    #et la valeur est le nom du fichier : fichier (par exemple \"abstract-123.txt\").\n\nfor fichier in fichiers_articles:\n    # Extraire l'identifiant du fichier article\n    identifiant = fichier.split(\"-\")[1].split(\".\")[0]\n    articles[identifiant] = fichier\n\n# Dictionnaire pour stocker les appariements\nappariements_RCT = {}\n\n# Liste des abstracts et articles non-appariés\nnon_apparies_abstracts_RCT = []\nnon_apparies_articles_RCT = []\n\n# Appariement des fichiers abstracts et articles par identifiant\nfor identifiant in abstracts:\n    if identifiant in articles:\n        # Ajouter l'appariement au dictionnaire\n        appariements_RCT[identifiant] = {\n            \"abstract\": abstracts[identifiant],\n            \"article\": articles[identifiant]\n        }\n    else:\n        # Ajouter à la liste des abstracts non appariés\n        non_apparies_abstracts_RCT.append(abstracts[identifiant])\n\n# Vérifier les articles non-appariés\nfor identifiant in articles:\n    if identifiant not in abstracts:\n        # Ajouter à la liste des articles non appariés\n        non_apparies_articles_RCT.append(articles[identifiant])\n\n# Affichage des appariements\n\"\"\"\nprint(\"Appariements_RCT :\")\nfor identifiant, fichiers in appariements_RCT.items():\n    print(f\"Identifiant {identifiant}:\")\n    print(f\"  Abstract: {fichiers['abstract']}\")\n    print(f\"  Article: {fichiers['article']}\")\n\"\"\"\n# Affichage des abstracts non-appariés\nprint(\"\\nAbstracts non-appariés :\")\nfor abstract in non_apparies_abstracts_RCT:\n    print(abstract)\n\n# Affichage des articles non-appariés\nprint(\"\\nArticles non-appariés :\")\nfor article in non_apparies_articles_RCT:\n    print(article)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T22:31:52.628490Z","iopub.execute_input":"2024-12-30T22:31:52.628827Z","iopub.status.idle":"2024-12-30T22:31:52.691396Z","shell.execute_reply.started":"2024-12-30T22:31:52.628802Z","shell.execute_reply":"2024-12-30T22:31:52.690382Z"}},"outputs":[{"name":"stdout","text":"\nAbstracts non-appariés :\nabstract-24164420.txt\nabstract-36082590.txt\nabstract-36872899.txt\nabstract-37030393.txt\nabstract-37722926.txt\nabstract-36451616.txt\nabstract-36094045.txt\nabstract-37157134.txt\nabstract-36877135.txt\nabstract-36289532.txt\nabstract-24266855.txt\nabstract-37565064.txt\nabstract-37614106.txt\nabstract-33153517.txt\nabstract-37562034.txt\nabstract-37690911.txt\nabstract-31431084.txt\nabstract-33984185.txt\nabstract-24268098.txt\nabstract-36542086.txt\nabstract-36053287.txt\nabstract-32418064.txt\nabstract-37302021.txt\nabstract-37463508.txt\nabstract-31507265.txt\nabstract-33674243.txt\nabstract-36271420.txt\nabstract-36129998.txt\nabstract-36525381.txt\nabstract-29172800.txt\nabstract-37392348.txt\nabstract-32868525.txt\nabstract-28961557.txt\nabstract-37551774.txt\nabstract-35986699.txt\nabstract-27421672.txt\nabstract-34713539.txt\nabstract-36503413.txt\nabstract-35751625.txt\nabstract-35982483.txt\nabstract-36309392.txt\nabstract-35974668.txt\nabstract-30394151.txt\nabstract-37217019.txt\nabstract-32869931.txt\nabstract-37639901.txt\nabstract-37379733.txt\nabstract-34724172.txt\nabstract-34984792.txt\nabstract-37605171.txt\nabstract-36321647.txt\n\nArticles non-appariés :\narticle- 35751625.txt\narticle- 32868525.txt\narticle- 35982483.txt\narticle- 26959498.txt\narticle- 30982467.txt\narticle- 24747297.txt\narticle- 32482570.txt\narticle-37728775.txt\narticle- 31402562.txt\narticle- 36068298.txt\narticle- 36084331.txt\narticle- 31431084.txt\narticle- 36129998.txt\narticle- 25003802.txt\narticle- 24859440.txt\narticle- 37030393.txt\narticle- 27671861 .txt\narticle- 31076287.txt\narticle- 33984185.txt\narticle- 32162274.txt\narticle- 24401143.txt\narticle- 24368640.txt\narticle- 24593269.txt\narticle- 36059000.txt\narticle- 24708472.txt\narticle- 34713539.txt\narticle- 24533998.txt\narticle-25474530.txt\narticle- 34984792.txt\narticle- 31507265.txt\narticle- 27421971.txt\narticle- 24671488.txt\narticle- 24700992.txt\narticle- 36525381.txt\narticle- 36542086.txt\narticle- 25616223.txt\narticle-35671123 .txt\narticle- 36891751.txt\narticle- 25398189.txt\narticle- 37302021.txt\narticle- 36503413.txt\narticle- 30394151.txt\narticle- 24045440.txt\narticle- 36878722.txt\narticle- 29172800.txt\narticle- 31436896.txt\narticle- 29212509.txt\narticle-32988385 .txt\narticle- 24983707.txt\narticle- 24379118.txt\narticle- 24447332.txt\narticle- 25123116.txt\narticle- 24341382.txt\narticle- 25038794.txt\narticle- 29041941.txt\narticle- 24666558.txt\narticle- 24322060.txt\narticle-35713933 .txt\narticle-36631450.txt\narticle-36095045.txt\narticle-36469329.txt\narticle- 26229189.txt\narticle- 37562034.txt\narticle- 30997149.txt\narticle- 25588847.txt\narticle- 36872899.txt\narticle- 36066965.txt\narticle- 37551774.txt\narticle- 24859367.txt\narticle- 30793375.txt\narticle- 28671938.txt\narticle- 31625835.txt\narticle- 30997153.txt\narticle-37766477.txt\narticle- 36289532.txt\narticle- 25123172.txt\narticle- 25514459.txt\narticle- 29490675.txt\narticle- 36271420.txt\narticle- 26993316.txt\narticle- 27423055.txt\narticle- 36082590.txt\narticle- 33674243.txt\narticle- 24354476 .txt\narticle- 24618231 .txt\narticle- 35974668.txt\narticle- 24708443 .txt\narticle- 32017677.txt\narticle- 25083614.txt\narticle- 29208026.txt\narticle- 26468632.txt\narticle- 26666236.txt\narticle- 37463508.txt\narticle- 34138698.txt\narticle- 31969108.txt\narticle- 24108196 .txt\narticle- 32091358.txt\narticle- 27645694.txt\narticle- 30713721.txt\narticle- 23732262.txt\narticle- 24747090.txt\narticle- 24999963.txt\narticle- 24568148.txt\narticle- 37379733.txt\narticle- 24630150.txt\narticle- 24506815.txt\narticle- 29464843.txt\narticle- 36321647.txt\narticle- 32869931.txt\narticle- 37392348.txt\narticle- 24825528.txt\narticle- 36669352.txt\narticle- 37565064.txt\narticle- 28961557.txt\narticle- 24296904.txt\narticle- 36631450.txt\narticle- 30713722.txt\narticle- 24597619.txt\narticle-32562807.txt\narticle- 34724172.txt\narticle- 30008335.txt\narticle- 24597650.txt\narticle- 36451616.txt\narticle- 30008335.txt\narticle- 30886732 .txt\narticle- 36053287.txt\narticle- 30846389.txt\narticle- 37217019.txt\narticle- 37052929.txt\narticle- 34184312.txt\narticle- 37605171.txt\narticle- 24378686.txt\narticle- 35986699.txt\narticle- 24781374.txt\narticle- 37690911.txt\narticle- 24521771.txt\narticle- 33153517.txt\narticle- 37157134.txt\narticle- 24333515.txt\narticle- 37722926.txt\narticle-36446771.txt\narticle- 24263324.txt\narticle-38012031.txt\narticle- 27600280.txt\narticle- 24519331.txt\narticle- 36309392.txt\narticle- 25110374.txt\narticle- 24036678.txt\narticle- 24700991.txt\narticle- 32316911.txt\narticle- 36877135.txt\narticle- 24518774.txt\narticle- 24248149.txt\narticle- 24448650.txt\narticle- 25193416.txt\narticle-37614109.txt\narticle- 30554670.txt\narticle- 25936892.txt\narticle- 32164522.txt\narticle- 24279685.txt\narticle- 25474530.txt\narticle- 29258549.txt\narticle- 24184336.txt\narticle- 24637869.txt\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!pip install -U datasets #le \"-U\" demande l'installation de la toute dernière version du package\nfrom datasets import Dataset #importe la classe Dataset de la bibliothèque datasets de Hugging Face.\n#Cette classe est utilisée pour manipuler des ensembles de données dans un format qui peut être utilisé pour l'entraînement de modèles issus d'Hugging Face.\n\n# Limiter le nombre de threads de JAX à 1 (pour éviter les problèmes dans kaggle)\nos.environ[\"JAX_NUM_THREADS\"] = \"1\"\n\n#Notre dictionnaire appariements_RCT n'est pas tout à fait sous la bonne forme pour être transformé en dataset de Hugging Face :\n#on le remanie donc pour qu'il ait cette bonne forme.\n\n# Initialisation des listes vides\nidentifiants = []\narticles = []\nabstracts = []\n\n# Remplir les listes avec les données extraites du dictionnaire appariements\nfor identifiant, values in appariements_RCT.items():\n    identifiants.append(identifiant)         # Ajout de l'identifiant\n    articles.append(values[\"article\"])      # Ajout de l'article\n    abstracts.append(values[\"abstract\"])    # Ajout de l'abstract\n\n# Créer un dictionnaire avec les listes\ndata = {\n    \"identifiant\": identifiants,\n    \"article\": articles,\n    \"abstract\": abstracts\n}\n\n# Créer le Dataset en convertissant notre nouveau dictionnaire via l'instruction Dataset.from_dict() de Dataset\ndataset_RCT = Dataset.from_dict(data)\n\n#print(dataset_RCT)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T22:31:52.692252Z","iopub.execute_input":"2024-12-30T22:31:52.692542Z","iopub.status.idle":"2024-12-30T22:31:56.234047Z","shell.execute_reply.started":"2024-12-30T22:31:52.692507Z","shell.execute_reply":"2024-12-30T22:31:56.232991Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (18.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\nRequirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## Le fichier à utiliser pour l'entrainement sur les données RCT sera donc \"dataset_RCT\"","metadata":{}},{"cell_type":"markdown","source":"# Essai du modèle bigbird-pegasus-large-arxiv","metadata":{}},{"cell_type":"markdown","source":"Notre corpus d'articles scientifiques à résumer est constitué de textes longs.\nC'est un facteur déterminant dans le choix du modèle - et dans ses difficultés...\n\nLe modèle bigbird-pegasus-large-arxiv a pour singularité, par rapport à des modèles de type BERT, d'utiliser une version modifiée de l'architecture Transformer : il a un mécanisme d'attention sparse (attention réduite) par blocs. Concrètement, cela signifie qu'au lieu de calculer l'attention entre toutes les positions du texte, il divise la séquence (le texte) en blocs et applique l'attention uniquement au sein de ces blocs, ainsi qu'entre certains blocs (par exemple les blocs voisins ou un échantillonnage aléatoire de blocs).\n\nCela permet d'être économe en temps de calcul : avec un modèle de type BERT, ce dernier croissait en $O(N^2)$, où $N$ représente la taille du texte (ou document). Avec le mécanisme d'attention sparse par blocs, on tombe à des temps de calcul en $O(N.log(N))$ ou en $O(N)$.\n\nPour nos articles scientifiques, qui sont des documents longs, ce type de modèles, avec attention sparse par blocs, peut donc être particulièrement pertinent.","metadata":{"id":"fUiZk0D9q-c1"}},{"cell_type":"markdown","source":"On convertit le texte des articles et des résumés (ici ceux de type \"OBS\") en séquences de tokens pour le modèle bigbird-pegasus-large-arxiv.","metadata":{"id":"eDcpedpneMHC"}},{"cell_type":"code","source":"\n# Fonction pour préparer les données (tokenisation) pour un modèle chargé depuis huggingface\ndef tokenize_function(examples):\n    # Tokeniser les articles et les résumés\n    inputs = tokenizer(examples['article'], truncation=True, padding=\"max_length\", max_length=1024) #données en entrée\n    #examples['article'] : C'est la donnée brute (l'article) fournie à la fonction sous forme de texte.\n    #Cette donnée vient d'un Dataset ou d'une DataLoader et est passée sous forme de liste de textes (articles).\n    #tokenizer(examples['article']) : Cette ligne utilise le tokenizer pour convertir le texte brut des articles en une séquence de tokens.\n    #Le tokenizer transforme le texte en un format compréhensible par le modèle (i.e., des IDs de tokens).\n\n    #truncation=True : Si le texte est plus long que la longueur maximale définie, il sera tronqué pour correspondre à cette longueur maximale.\n\n    #padding=\"max_length\" : Cela permet de remplir (padd) le texte pour qu'il atteigne la longueur maximale spécifiée.\n    #Si un article est plus court que la longueur maximale, des tokens de remplissage seront ajoutés.\n\n    #max_length=1024 : La longueur maximale des séquences d'entrée est fixée à 1024 tokens. Si un article est plus long que cela, il sera tronqué à 1024 tokens.\n\n    targets = tokenizer(examples['abstract'], truncation=True, padding=\"max_length\", max_length=256)#données cible de l'entrainement du modèle\n\n    # Retourner les inputs et targets sous la forme de dictionnaires\n    inputs[\"labels\"] = targets[\"input_ids\"]\n    #inputs[\"labels\"] : Dans un modèle de génération de texte comme BigBirdPegasus, les labels sont les séquences que le modèle doit prédire\n    #(les résumés dans ce cas). En d'autres termes, le modèle apprend à prédire les tokens du résumé à partir des tokens de l'article.\n    #targets[\"input_ids\"] : input_ids est la représentation des tokens du résumé (les IDs numériques des tokens). Ces tokens sont utilisés comme labels lors de l'entraînement,\n    #c'est-à-dire que le modèle essaiera de prédire ces input_ids lorsqu'il verra les tokens des articles.\n    #Cette ligne ajoute donc les input_ids des résumés dans la clé \"labels\" des données d'entrée, qui est utilisée pendant l'entraînement pour calculer la perte\n    #entre les prédictions du modèle et les résumés réels.\n    return inputs\n","metadata":{"id":"41fMtp6-pHgH","trusted":true,"execution":{"iopub.status.busy":"2024-12-30T22:31:56.235227Z","iopub.execute_input":"2024-12-30T22:31:56.235573Z","iopub.status.idle":"2024-12-30T22:31:56.241697Z","shell.execute_reply.started":"2024-12-30T22:31:56.235542Z","shell.execute_reply":"2024-12-30T22:31:56.240793Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"'\\n# Fonction pour préparer les données (tokenisation) pour un modèle chargé depuis huggingface\\ndef tokenize_function(examples):\\n    # Tokeniser les articles et les résumés\\n    inputs = tokenizer(examples[\\'article\\'], truncation=True, padding=\"max_length\", max_length=1024) #données en entrée\\n    #examples[\\'article\\'] : C\\'est la donnée brute (l\\'article) fournie à la fonction sous forme de texte.\\n    #Cette donnée vient d\\'un Dataset ou d\\'une DataLoader et est passée sous forme de liste de textes (articles).\\n    #tokenizer(examples[\\'article\\']) : Cette ligne utilise le tokenizer pour convertir le texte brut des articles en une séquence de tokens.\\n    #Le tokenizer transforme le texte en un format compréhensible par le modèle (i.e., des IDs de tokens).\\n\\n    #truncation=True : Si le texte est plus long que la longueur maximale définie, il sera tronqué pour correspondre à cette longueur maximale.\\n\\n    #padding=\"max_length\" : Cela permet de remplir (padd) le texte pour qu\\'il atteigne la longueur maximale spécifiée.\\n    #Si un article est plus court que la longueur maximale, des tokens de remplissage seront ajoutés.\\n\\n    #max_length=1024 : La longueur maximale des séquences d\\'entrée est fixée à 1024 tokens. Si un article est plus long que cela, il sera tronqué à 1024 tokens.\\n\\n    targets = tokenizer(examples[\\'abstract\\'], truncation=True, padding=\"max_length\", max_length=256)#données cible de l\\'entrainement du modèle\\n\\n    # Retourner les inputs et targets sous la forme de dictionnaires\\n    inputs[\"labels\"] = targets[\"input_ids\"]\\n    #inputs[\"labels\"] : Dans un modèle de génération de texte comme BigBirdPegasus, les labels sont les séquences que le modèle doit prédire\\n    #(les résumés dans ce cas). En d\\'autres termes, le modèle apprend à prédire les tokens du résumé à partir des tokens de l\\'article.\\n    #targets[\"input_ids\"] : input_ids est la représentation des tokens du résumé (les IDs numériques des tokens). Ces tokens sont utilisés comme labels lors de l\\'entraînement,\\n    #c\\'est-à-dire que le modèle essaiera de prédire ces input_ids lorsqu\\'il verra les tokens des articles.\\n    #Cette ligne ajoute donc les input_ids des résumés dans la clé \"labels\" des données d\\'entrée, qui est utilisée pendant l\\'entraînement pour calculer la perte\\n    #entre les prédictions du modèle et les résumés réels.\\n    return inputs\\n'"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"## Chargement du modèle bigbird-pegasus-large-arxiv\n\n","metadata":{"id":"TxbTu-oNDOC4"}},{"cell_type":"code","source":"\nfrom transformers import BigBirdPegasusForConditionalGeneration, AutoTokenizer\nimport torch\nfrom transformers import TrainingArguments, Trainer # Import TrainingArguments and Trainer\n\n#Instancier le tokenizer pour le modèle pegasus\n#tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/bigbird-pegasus-large-arxiv/\")\ntokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-pegasus-large-arxiv\")\n#chargement du modèle :\n# by default encoder-attention is `block_sparse` with num_random_blocks=3, block_size=64/par défaut l'encoder-attention est \"block sparse\"\n#avec num_random_blocks=3 et block_size=64\n#model = BigBirdPegasusForConditionalGeneration.from_pretrained(\"/kaggle/input/bigbird-pegasus-large-arxiv/\",\nmodel = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-arxiv\",\n                                                               return_dict=True,\n                                                               torch_dtype=torch.float16,\n                                                               device_map=\"auto\")\n\n######################Options pour modifier la taille et le nombre des blocs d'attention :\n# decoder attention type can't be changed & will be \"original_full\"\n# you can change `attention_type` (encoder only) to full attention like this:\n##model = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-arxiv\", attention_type=\"original_full\")\n\n# you can change `block_size` & `num_random_blocks` like this:\n##model = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-arxiv\", block_size=16, num_random_blocks=2)\n######################\n\n","metadata":{"id":"j1WNnycYXvQM","outputId":"89d58390-0900-4b3b-f348-a00bb02a5e7c","trusted":true,"execution":{"iopub.status.busy":"2024-12-30T22:31:56.244553Z","iopub.execute_input":"2024-12-30T22:31:56.244812Z","iopub.status.idle":"2024-12-30T22:31:56.263509Z","shell.execute_reply.started":"2024-12-30T22:31:56.244791Z","shell.execute_reply":"2024-12-30T22:31:56.262562Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"'\\nfrom transformers import BigBirdPegasusForConditionalGeneration, AutoTokenizer\\nimport torch\\nfrom transformers import TrainingArguments, Trainer # Import TrainingArguments and Trainer\\n\\n#Instancier le tokenizer pour le modèle pegasus\\n#tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/bigbird-pegasus-large-arxiv/\")\\ntokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-pegasus-large-arxiv\")\\n#chargement du modèle :\\n# by default encoder-attention is `block_sparse` with num_random_blocks=3, block_size=64/par défaut l\\'encoder-attention est \"block sparse\"\\n#avec num_random_blocks=3 et block_size=64\\n#model = BigBirdPegasusForConditionalGeneration.from_pretrained(\"/kaggle/input/bigbird-pegasus-large-arxiv/\",\\nmodel = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-arxiv\",\\n                                                               return_dict=True,\\n                                                               torch_dtype=torch.float16,\\n                                                               device_map=\"auto\")\\n'"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"## Entrainement du modèle bigbird-pegasus-large-arxiv et conclusions sur ce modèle pour notre compétition kaggle","metadata":{}},{"cell_type":"markdown","source":"On paramètre l'entraînement du modèle.\n\nProblème : même en modifiant les paramètres de training_args comme ci-dessous, pour tenter de consommer moins de mémoire, l'entraînement de ce modèle fait planter la session kaggle car il consomme trop de mémoire - y compris en utilisant les GPU 100, ceux qui comportent le plus de mémoire sur kaggle.\nSi bigbird-pegasus-large-arxiv est performant en temps de calcul, en revanche, il ne l'est pas encore assez pour ce qui est de l'utilisation de la mémoire - et ce modèle n'existe que dans sa version \"large\".\n\nNous allons donc tester un autre modèle : bart-large-cnn. ","metadata":{"id":"s9i3nuomeifY"}},{"cell_type":"code","source":"\n# Appliquer la tokenisation\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n\n# Définir les arguments d'entraînement\ntraining_args = TrainingArguments(\n    output_dir=\"./bigbird_pegasus_finetuned\",  # Répertoire de sortie pour enregistrer les résultats\n    evaluation_strategy=\"epoch\",  # Évaluer après chaque époque\n    learning_rate=2e-5,  # Taux d'apprentissage\n    per_device_train_batch_size=1, #2 je réduis pour éviter de planter kaggle,  # Taille du batch pour l'entraînement\n    per_device_eval_batch_size=1, #2 idem,   # Taille du batch pour l'évaluation\n    gradient_accumulation_steps=8,  # Accumuler les gradients sur 8 batches avant la mise à jour (réduit l'utilisation de la mémoire, toujours pour éviter de planter kaggle)\n                                    #cela \"simule\" une taille de batch plus grande que ce qu'elle n'est en réalité ici\n    fp16=False,  # Activer l'entraînement en 16-bit - toujours pour éviter de planter kaggle\n    num_train_epochs=3,  # Nombre d'époques d'entraînement\n    dataloader_num_workers=4,  # Utiliser 4 processus pour charger les données en parallèle\n    gradient_checkpointing=True,  # Activer gradient checkpointing\n    save_steps=500,  # Sauvegarder moins souvent\n    weight_decay=0.01,  # L2 regularization\n    save_total_limit=2,  # Limite du nombre de sauvegardes du modèle\n    logging_dir=\"./logs\",  # Répertoire pour les logs\n    logging_steps=100,  # Fréquence des logs\n    report_to=\"tensorboard\",  # Optionnel : Utiliser TensorBoard pour la visualisation des métriques\n)\n\n# Créer un objet Trainer\ntrainer = Trainer(\n    model=model,  # Le modèle à fine-tuner\n    args=training_args,  # Les arguments d'entraînement\n    train_dataset=train_dataset,  # Jeu d'entraînement\n    eval_dataset=val_dataset,  # Jeu de validation\n    compute_metrics=compute_metrics,  # Fonction pour calculer les métriques - elle nous permettra d'utiliser le score \"ROUGE\"\n)\n\n# Lancer l'entraînement\ntrainer.train()\n\n\n# Sauvegarder le modèle fine-tuné\ntrainer.save_model(\"./bigbird_pegasus_finetuned\")\n\n","metadata":{"id":"9_UU-nmJqoiQ","outputId":"08af7291-0207-4cbf-b546-3431e5c94376","trusted":true,"execution":{"iopub.status.busy":"2024-12-30T22:31:56.265505Z","iopub.execute_input":"2024-12-30T22:31:56.265739Z","iopub.status.idle":"2024-12-30T22:31:56.282975Z","shell.execute_reply.started":"2024-12-30T22:31:56.265719Z","shell.execute_reply":"2024-12-30T22:31:56.282172Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"'\\n# Appliquer la tokenisation\\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\\n\\n# Définir les arguments d\\'entraînement\\ntraining_args = TrainingArguments(\\n    output_dir=\"./bigbird_pegasus_finetuned\",  # Répertoire de sortie pour enregistrer les résultats\\n    evaluation_strategy=\"epoch\",  # Évaluer après chaque époque\\n    learning_rate=2e-5,  # Taux d\\'apprentissage\\n    per_device_train_batch_size=1, #2 je réduis pour éviter de planter kaggle,  # Taille du batch pour l\\'entraînement\\n    per_device_eval_batch_size=1, #2 idem,   # Taille du batch pour l\\'évaluation\\n    gradient_accumulation_steps=8,  # Accumuler les gradients sur 8 batches avant la mise à jour (réduit l\\'utilisation de la mémoire, toujours pour éviter de planter kaggle)\\n                                    #cela \"simule\" une taille de batch plus grande que ce qu\\'elle n\\'est en réalité ici\\n    fp16=False,  # Activer l\\'entraînement en 16-bit - toujours pour éviter de planter kaggle\\n    num_train_epochs=3,  # Nombre d\\'époques d\\'entraînement\\n    dataloader_num_workers=4,  # Utiliser 4 processus pour charger les données en parallèle\\n    gradient_checkpointing=True,  # Activer gradient checkpointing\\n    save_steps=500,  # Sauvegarder moins souvent\\n    weight_decay=0.01,  # L2 regularization\\n    save_total_limit=2,  # Limite du nombre de sauvegardes du modèle\\n    logging_dir=\"./logs\",  # Répertoire pour les logs\\n    logging_steps=100,  # Fréquence des logs\\n    report_to=\"tensorboard\",  # Optionnel : Utiliser TensorBoard pour la visualisation des métriques\\n)\\n\\n# Créer un objet Trainer\\ntrainer = Trainer(\\n    model=model,  # Le modèle à fine-tuner\\n    args=training_args,  # Les arguments d\\'entraînement\\n    train_dataset=train_dataset,  # Jeu d\\'entraînement\\n    eval_dataset=val_dataset,  # Jeu de validation\\n    compute_metrics=compute_metrics,  # Fonction pour calculer les métriques - elle nous permettra d\\'utiliser le score \"ROUGE\"\\n)\\n\\n# Lancer l\\'entraînement\\ntrainer.train()\\n\\n\\n# Sauvegarder le modèle fine-tuné\\ntrainer.save_model(\"./bigbird_pegasus_finetuned\")\\n\\n'"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"# Essai du modèle Bart-large-cnn seul","metadata":{}},{"cell_type":"markdown","source":"BART est un modèle de transformer encodeur-décodeur (seq2seq : \"sequence to séquence\") avec un encodeur bidirectionnel (similaire à BERT) et un décodeur autoregressif (similaire à GPT).\nIl est donc pré-entraîné en masquant des parties du texte et en apprenant à prédire ces parties - comme si on lui demandait de débruiter un texte bruité.\nIl utilise une attention dense classique pour un transformer, où chaque token de l'entrée prend en compte tous les autres tokens de la séquence. Cela le handicape donc pour nos articles longs (comme tous les modèles à attention dense classique).\n\nMais il peut néanmoins rester un parti intéressant, étant donnée sa consommation de mémoire inférieure à bigbird-pegasus.\n\nSera-t-il suffisamment efficace pour la tâche envisagée ? Nous le testons \"à la volée\", sans le fine-tuner, sur quelques articles, pour commencer.","metadata":{"id":"_ogvtW3usO7O"}},{"cell_type":"code","source":"import random\nfrom datasets import Dataset\nfrom transformers import BartForConditionalGeneration, BartTokenizer\nfrom rouge_score import rouge_scorer\nimport torch\n\n# Charge le modèle BART et le tokenizer\ntokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\nmodel = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Ici on utilise donc simplement le modèle préentraîné BART pour générer des résumés à partir d'articles scientifiques du train set, qu'on compare ensuite avec leurs résumés de référence (que nous avons, puisque nous sommes dans le train set). On calcule ensuite leur score ROUGE-2 pour évaluer la qualité des résumés générés.","metadata":{}},{"cell_type":"code","source":"from transformers import BartForConditionalGeneration, BartTokenizer\nfrom rouge_score import rouge_scorer\nimport os\nimport random\n\n# Charger le modèle BART et le tokenizer\ntokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\nmodel = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n\n# Chemins vers les dossiers des articles et des résumés de référence\ndossier_articles = \"/kaggle/input/m-2-maliash-resume-darticles-scientifiques/train/OBS/articles_OBS/\"\ndossier_resumes = \"/kaggle/input/m-2-maliash-resume-darticles-scientifiques/train/OBS/abstracts_OBS/\"\n\n# Charger les articles et leurs résumés de référence\nfichiers_articles = [f for f in os.listdir(dossier_articles) if f.startswith(\"article-\")]\narticles = {}\nresumes_reference = {}\n\ndef lire_fichier(filepath):\n    try:\n        with open(filepath, 'r', encoding='utf-8') as f:\n            return f.read()\n    except UnicodeDecodeError:\n        with open(filepath, 'r', encoding='latin-1') as f:\n            return f.read()\n\nfor fichier in fichiers_articles:\n    identifiant = fichier.split(\"-\")[1].split(\".\")[0]\n    articles[identifiant] = lire_fichier(os.path.join(dossier_articles, fichier))\n    resumes_reference[identifiant] = lire_fichier(os.path.join(dossier_resumes, f\"abstract-{identifiant}.txt\"))\n\n# Sélectionner 5 articles aléatoires\nrandom.seed(42)  # Pour garantir la reproductibilité\nselected_ids = random.sample(list(articles.keys()), 5)\n\n# Stocker les articles, résumés générés et scores\nresumes_generes = []\nrouge_scores = []\n\n# Fonction pour résumer un texte avec BART\ndef resumer_texte(texte, max_length=130, min_length=30, length_penalty=2.0, num_beams=4):\n    inputs = tokenizer.encode(\"summarize: \" + texte, return_tensors=\"pt\", max_length=1024, truncation=True)\n    summary_ids = model.generate(\n        inputs,\n        max_length=max_length,\n        min_length=min_length,\n        length_penalty=length_penalty,\n        num_beams=num_beams,\n        early_stopping=True,\n    )\n    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n\n# Calcul des résumés et des scores ROUGE-2\nscorer = rouge_scorer.RougeScorer([\"rouge2\"], use_stemmer=True)\n\nfor identifiant in selected_ids:\n    article = articles[identifiant]\n    resume_reference = resumes_reference[identifiant]\n\n    # Générer le résumé\n    resume_genere = resumer_texte(article)\n    resumes_generes.append((identifiant, resume_genere))\n\n    # Calculer le score ROUGE-2\n    scores = scorer.score(resume_reference, resume_genere)\n    rouge_scores.append(scores[\"rouge2\"].fmeasure)\n\n# Afficher les résultats\nfor i, (identifiant, resume_genere) in enumerate(resumes_generes):\n    print(f\"Article ID: {identifiant}\")\n    print(f\"Résumé généré: {resume_genere}\")\n    print(f\"ROUGE-2 Score: {rouge_scores[i]:.4f}\")\n    print(\"-\" * 50)\n\n# Afficher le score ROUGE-2 moyen\navg_rouge2 = sum(rouge_scores) / len(rouge_scores)\nprint(\"Score ROUGE-2 moyen:\")\nprint(f\"ROUGE-2: {avg_rouge2:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"On obtient, sur ce petit échantillon d'articles, une moyenne de score rouge de 0,04 : c'est exécrable. \nEn plus de son attention non sparse, surtout, ici, le souci est que BART a une limite de 1024 tokens. Les articles plus longs sont donc tronqués (et nos articles scientifiques le sont tous, plus longs), ce qui peut entraîner une perte d'informations essentielles. Les résumés générés seront donc effectués uniquement à partir d'une petite partie du contenu de l'article. Or, toutes les informations importantes pour faire un bon résumé ne se trouvent pas forcément au début des articles.\n\nDevant ces premiers résultats, nous n'allons pas plus loin dans l'expérimentation avec ce modèle seul, et décidons d'explorer les modèles longformer.","metadata":{}},{"cell_type":"markdown","source":"# Confection d'un modèle composé à partir de longformer et bart","metadata":{}},{"cell_type":"markdown","source":"## Chargement du modèle  allenai/longformer-base-4096\n\n","metadata":{}},{"cell_type":"markdown","source":"Le modèle bigbird-pegasus-large-arivx demande trop de mémoire pour notre plateforme de compétition kaggle, et le modèle bart donne des scores rouges trop faibles (même si en le fine-tunant, ils auraient sans doute été légèrement améliorés).\n\nNous devons donc nous orienter vers un modèle de compromis, néanmoins adapté au traitement d'articles scientifiques longs.\nIl nous faut donc chercher dans la famille des modèles à attention \"sparse\", les seuls vraiment adaptés pour ces textes longs.\n\nallenai/longformer-large-4096 partage avec bigbird ce mécanisme d'attention sparse : tout comme lui, il utilise une fenêtre d'attention \"glissante\". Tout comme lui, il y ajoute une attention à plus longue portée sur les tokens importants, ce qui lui permet d'interagir avec tous les autres tokens de la séquence pour capturer des relations à long terme. Mais contrairement à lui, il n'a pas de mécanisme d'attention aléatoire qui se surajoute à ces deux couches d'attention - mécanisme qui permettait à bigbird-pegasus de mieux capter des relations globales...mais consommait de la mémoire vive, lors de son entraînement (fine tuning) notamment.\n\nEn revanche, allenai/longformer-large-4096 n'est pas un générateur de texte : il nous faut donc l'associer à un modèle de type GPT ou autre qui permet quant à lui la génération. Ce, dans le but de bonifier les performances du modèle générateur par les bonnes performances de longformer sur les textes longs.\nOn l'associe donc, in fine, avec le modèle générateur bart-large-cnn, association qu'on crée \"à la main\" via la confection d'une classe spécifique.\n\nComme le modèle ainsi créé reste trop volumineux, on y remplace finalement allenai/longformer-large-4096 par une version plus légère : allenai/longformer-base-4096.\n\n","metadata":{}},{"cell_type":"code","source":"from transformers import LongformerForSequenceClassification, AutoTokenizer\nimport torch\nfrom transformers import TrainingArguments, Trainer # Import TrainingArguments and Trainer\n\n# Instancier le tokenizer pour le modèle Longformer\ntokenizer = AutoTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n\n# Chargement du modèle Longformer\nmodel = LongformerForSequenceClassification.from_pretrained(\"allenai/longformer-base-4096\", \n                                                           return_dict=True,\n                                                           torch_dtype=torch.float16,\n                                                           device_map=\"auto\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T16:00:54.819217Z","iopub.execute_input":"2025-01-11T16:00:54.819569Z","iopub.status.idle":"2025-01-11T16:01:13.191383Z","shell.execute_reply.started":"2025-01-11T16:00:54.819527Z","shell.execute_reply":"2025-01-11T16:01:13.190533Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e81d14b7fd24c739eed0a82210d9ee3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c628f10c2bcf4b3187beed1457e00efa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f814abe64fc0418787ea9d2e8bb8ef15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a285802a88543e59198fd307b87f277"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/597M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79aa15ef8deb4a448be962a586b103cd"}},"metadata":{}},{"name":"stderr","text":"Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Création du nouveau modèle \"cousu main\" à partir de BART et LONGFORMER","metadata":{}},{"cell_type":"markdown","source":"On est obligés de faire un peu de \"cuisine\" (en code informatique) pour associer les 2 modèles en un seul : par exemple, les sorties de longformer n'ont pas la même taille que les entrées de bart. Il nous faut donc insérer dans la classe une \"couche de projection\".\nIl faut également dupliquer leurs poids, pour éviter les (mauvais...) mélanges entre eux par la suite.\n\nOn ne peut pas utiliser la tokenisation de l'un des deux modèles non plus : il nous faut en créer une, tout aussi hybride que notre modèle \"fait main\" à partir de la \"couture\" de ces deux modèles issus de hugging face.","metadata":{}},{"cell_type":"code","source":"from transformers import LongformerTokenizer, BartTokenizer, BartForConditionalGeneration, LongformerModel, Trainer, TrainingArguments\nfrom datasets import load_dataset\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ntorch.cuda.empty_cache()\n\n# Charger le tokenizer et le modèle Longformer pour l'encodage\nlongformer_tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\nlongformer_model = LongformerModel.from_pretrained('allenai/longformer-base-4096')\n\n# Charger le tokenizer et le modèle BART-CNN pour la génération\nbart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\nbart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n\n# Tokenisation des entrées et des sorties\ndef tokenize_function(examples):\n    # Tokeniser les articles avec Longformer\n    model_inputs = longformer_tokenizer(\n        examples[\"article\"], \n        #max_length=4096, #trop long pour le GPU de kaggle...nous en revenons donc aux fameux 1024 token max, d'où nous partions avec BART seul.\n        max_length=1024,\n        truncation=True, \n        padding=\"max_length\",  # Assurer le padding\n        return_tensors=\"pt\"  # Retourner des tensors PyTorch\n    )\n    \n    # Tokeniser les résumés (labels) avec BART tokenizer\n    labels = bart_tokenizer(\n        #examples[\"highlights\"],\n        examples[\"abstract\"],\n        #max_length=200,\n        max_length=1024,\n        truncation=True, \n        padding=\"max_length\",  # Assurer le padding\n        return_tensors=\"pt\"  # Retourner des tensors PyTorch\n    )\n    \n    # Ajouter les labels au dictionnaire de sortie\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n  \n# Définir un modèle combiné qui utilise Longformer comme encodeur et BART pour la génération\nclass LongformerBart(nn.Module):\n    def __init__(self, longformer_model, bart_model):\n        super(LongformerBart, self).__init__()\n        self.longformer = longformer_model\n        self.bart = bart_model\n        \n # Ajouter une couche linéaire pour ajuster la dimension des sorties de Longformer\n        self.longformer_projection = nn.Linear(768, 1024)  # Projeter de 768 à 1024\n\n # Dupliquer les poids partagés de BART et Longformer\n        with torch.no_grad():\n            # Dupliquer les poids partagés de BART\n            bart_model.model.shared = torch.nn.Embedding.from_pretrained(bart_model.model.shared.weight.clone())\n            bart_model.model.encoder.embed_tokens = torch.nn.Embedding.from_pretrained(bart_model.model.encoder.embed_tokens.weight.clone())\n            bart_model.model.decoder.embed_tokens = torch.nn.Embedding.from_pretrained(bart_model.model.decoder.embed_tokens.weight.clone())\n\n            # Dupliquer les poids partagés de Longformer\n            longformer_model.embeddings.word_embeddings = torch.nn.Embedding.from_pretrained(longformer_model.embeddings.word_embeddings.weight.clone())\n\n    def forward(self, input_ids, attention_mask, decoder_input_ids=None, labels=None):\n        # Encoder la séquence avec Longformer\n        encoder_outputs = self.longformer(input_ids=input_ids, attention_mask=attention_mask)\n        encoder_hidden_states = encoder_outputs.last_hidden_state\n        \n  # Appliquer la projection pour ajuster la dimension\n        encoder_hidden_states = self.longformer_projection(encoder_hidden_states)       \n        \n        # Passer les représentations encodées à BART pour la génération\n        decoder_outputs = self.bart(\n            input_ids=decoder_input_ids,\n            labels=labels,\n            encoder_outputs=encoder_hidden_states\n        )\n        \n        return decoder_outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T16:16:03.575421Z","iopub.execute_input":"2025-01-11T16:16:03.575769Z","iopub.status.idle":"2025-01-11T16:16:14.689112Z","shell.execute_reply.started":"2025-01-11T16:16:03.575739Z","shell.execute_reply":"2025-01-11T16:16:14.688456Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b72163f652d4deab693643e28c34f78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97a54069e0734e57912c192d73fe8d2a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e1e5557b0344bc299fc45064624153e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91e0b204afc44a33af18d0d3f09a1c20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1d4cb64c58141ab872859a1956d5cc3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abaaa6be2ab0475eb6acc669967f5a70"}},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"## Entrainement de ce modèle sur les données OBS, puis sauvegarde de ce modèle ainsi fine-tuné\n","metadata":{}},{"cell_type":"markdown","source":"On effectue la spécialisation (\"fine tuning\") de ce modèle, pour la tâche de générer des résumés pour des articles scientifiques de type \"OBS\".","metadata":{}},{"cell_type":"code","source":"# Appliquer la tokenisation\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n\n# Diviser l'ensemble de données en train et validation\ndataset_split = tokenized_datasets.train_test_split(test_size=0.2)\n\n# Extraire les ensembles d'entraînement et de validation\ntrain_dataset = dataset_split['train']\nval_dataset = dataset_split['test']\n\n        \n\n# Initialiser le modèle combiné\nmodel = LongformerBart(longformer_model, bart_model)\n\n# Définir les arguments d'entraînement\ntraining_args = TrainingArguments(\n    output_dir=\"./longformer_bart_finetuned\",  # Répertoire de sortie pour enregistrer les résultats\n    evaluation_strategy=\"epoch\",  # Évaluer après chaque époque\n    learning_rate=2e-5,  # Taux d'apprentissage\n    per_device_train_batch_size=2,  # Taille du batch pour l'entraînement\n    per_device_eval_batch_size=2,   # Taille du batch pour l'évaluation\n    num_train_epochs=3,  # Nombre d'époques d'entraînement\n    save_steps=500,  # Sauvegarder moins souvent\n    weight_decay=0.01,  # L2 regularization\n    logging_dir=\"./logs\",  # Répertoire pour les logs\n    logging_steps=10,  # Fréquence des logs\n    report_to=\"tensorboard\",  # Optionnel : Utiliser TensorBoard pour la visualisation des métriques\n)\n\n\n# Créer un objet Trainer\ntrainer = Trainer(\n    model=model,  # Le modèle combiné Longformer+BART\n    args=training_args,  # Les arguments d'entraînement\n    train_dataset=train_dataset,  # Jeu d'entraînement\n    eval_dataset=val_dataset,  # Jeu de validation\n    tokenizer=longformer_tokenizer,  # Tokenizer pour le pré-traitement des données\n)\n\n# Lancer l'entraînement\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T22:32:27.461065Z","iopub.execute_input":"2024-12-30T22:32:27.461401Z","iopub.status.idle":"2024-12-30T22:42:31.324020Z","shell.execute_reply.started":"2024-12-30T22:32:27.461364Z","shell.execute_reply":"2024-12-30T22:42:31.323249Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/402 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"152e62026b82487eb29c7346135254a3"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='483' max='483' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [483/483 09:56, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.061900</td>\n      <td>1.215846</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.044400</td>\n      <td>6.686377</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.042600</td>\n      <td>2.663265</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=483, training_loss=0.5426618188434507, metrics={'train_runtime': 598.6016, 'train_samples_per_second': 1.609, 'train_steps_per_second': 0.807, 'total_flos': 0.0, 'train_loss': 0.5426618188434507, 'epoch': 3.0})"},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"Le modèle fine-tuné nécessite quelques ajustements au niveau de son enregistrement, pour pouvoir le sauvegarder correctement :","metadata":{}},{"cell_type":"code","source":"# Dupliquer les poids partagés pour éviter la duplication de mémoire\n# Cela doit être fait après l'entraînement et avant la sauvegarde\n\nimport safetensors.torch as st\n\n# Pour Longformer - dupliquer les embeddings\nwith torch.no_grad():  # Empêche la modification des gradients\n    new_word_embeddings = model.longformer.embeddings.word_embeddings.weight.clone()\n    model.longformer.embeddings.word_embeddings = torch.nn.Embedding.from_pretrained(new_word_embeddings)\n\n# Pour BART - dupliquer les embeddings de l'encodeur et du décodeur\nwith torch.no_grad():  # Empêche la modification des gradients\n    new_shared_weight = model.bart.model.shared.weight.clone()\n    model.bart.model.shared = torch.nn.Embedding.from_pretrained(new_shared_weight)\n    \n    new_encoder_embed = model.bart.model.encoder.embed_tokens.weight.clone()\n    model.bart.model.encoder.embed_tokens = torch.nn.Embedding.from_pretrained(new_encoder_embed)\n    \n    new_decoder_embed = model.bart.model.decoder.embed_tokens.weight.clone()\n    model.bart.model.decoder.embed_tokens = torch.nn.Embedding.from_pretrained(new_decoder_embed)\n\n# Sauvegarder les poids dans un fichier classique\ntorch.save(model.state_dict(), 'longformer_bart_finetuned/model_weights.pth')\n\n# Charger manuellement avec safetensors \nimport safetensors.torch as st\n\n# Sauvegarder avec safetensors après avoir dupliqué les poids\nst.save_file(model.state_dict(), 'longformer_bart_finetuned/model_weights.safetensors')\n\nimport json\n\n# Définir la configuration\nconfig = {\n    \"architectures\": [\"LongformerForSequenceClassification\"],\n    \"hidden_size\": 1024,\n    \"num_attention_heads\": 16,\n    \"num_hidden_layers\": 12,\n    \"vocab_size\": 50264,\n    \"max_position_embeddings\": 4098,\n    \"embedding_size\": 1024,\n    \"layer_norm_eps\": 1e-5,\n    \"pad_token_id\": 1,\n    \"activation_function\": \"gelu\",\n    \"initializer_range\": 0.02,\n    \"layer_norm_affine\": True,\n    \"attention_probs_dropout_prob\": 0.1,\n    \"hidden_dropout_prob\": 0.1,\n    \"max_position_embeddings\": 4098,\n    \"type_vocab_size\": 2,\n    \"attention_dropout\": 0.1,\n    \"longformer_projection\": {\n        \"in_features\": 768,\n        \"out_features\": 1024\n    }\n}\n\n# Sauvegarder le fichier config.json\n#config_path = '/kaggle/working/config.json'\nwith open(\"longformer_bart_finetuned/config.json\", 'w') as f:\n    json.dump(config, f)\n\nprint(\"Le fichier config.json a été sauvegardé \")\n\n# Sauvegarder le modèle fine-tuné\ntrainer.save_model(\"./longformer_bart_finetuned\")\n\"\"\"\n# Sauvegarde du modèle\ntorch.save(longformer_bart_model.state_dict(), \"longformer_bart_finetuned.pth\")\n\n# Sauvegarde de la configuration\nlongformer_bart_model.config.to_json_file(\"longformer_bart_config.json\")\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T16:41:06.655830Z","iopub.execute_input":"2025-01-11T16:41:06.656181Z","iopub.status.idle":"2025-01-11T16:41:29.108937Z","shell.execute_reply.started":"2025-01-11T16:41:06.656152Z","shell.execute_reply":"2025-01-11T16:41:29.108204Z"}},"outputs":[{"name":"stdout","text":"Le fichier config.json a été sauvegardé \n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"'\\n# Sauvegarde du modèle\\ntorch.save(longformer_bart_model.state_dict(), \"longformer_bart_finetuned.pth\")\\n\\n# Sauvegarde de la configuration\\nlongformer_bart_model.config.to_json_file(\"longformer_bart_config.json\")\\n'"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"...et les contraintes de capacité de stockage du dossier \"working\" de kaggle nous obligent encore à quelques manipulations (ici, suppression de checkpoint) pour pouvoir sauvegarder notre modèle fine-tuné dans un fichier zip :","metadata":{}},{"cell_type":"code","source":"import shutil\n# Chemin du dossier à supprimer\ndirectory_to_delete = \"/kaggle/working/longformer_bart_finetuned/checkpoint-483\"\n# Vérifier si le dossier existe\nif os.path.exists(directory_to_delete):\n    # Supprimer le dossier et tout son contenu\n    shutil.rmtree(directory_to_delete)\n    print(f\"Le dossier '{directory_to_delete}' a été supprimé avec succès.\")\nelse:\n    print(f\"Le dossier '{directory_to_delete}' n'existe pas.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T22:42:52.637526Z","iopub.execute_input":"2024-12-30T22:42:52.637753Z","iopub.status.idle":"2024-12-30T22:43:13.534881Z","shell.execute_reply.started":"2024-12-30T22:42:52.637732Z","shell.execute_reply":"2024-12-30T22:43:13.534111Z"}},"outputs":[{"name":"stdout","text":"Le dossier '/kaggle/working/longformer_bart_finetuned/checkpoint-483' a été supprimé avec succès.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"On zipe, enfin, le dossier du modèle ainsi créé et entraîné, pour pouvoir le sauvegarder et le réutiliser ensuite sur notre ensemble de test sans risquer de le perdre à chaque interruption de session :\n","metadata":{}},{"cell_type":"code","source":"\nprint(os.listdir(\"./\"))\n\nshutil.make_archive('/kaggle/working/longformer_bart_finetuned', 'zip', './longformer_bart_finetuned')\n","metadata":{"trusted":true},"outputs":[{"name":"stdout","text":"['.virtual_documents', 'logs', 'longformer_bart_finetuned', 'submission.csv']\n","output_type":"stream"},{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/longformer_bart_finetuned.zip'"},"metadata":{}}],"execution_count":31},{"cell_type":"markdown","source":"## Entraînement de ce modèle sur les données RCT, puis sauvegarde de ce modèle fine-tuné","metadata":{}},{"cell_type":"markdown","source":"On procède exactement de même pour le fine-tuning, puis la sauvegarde, du modèle spécialisé dans le résumé d'articles scientifiques de type \"RCT\" : ","metadata":{}},{"cell_type":"code","source":"# Appliquer la tokenisation\ntokenized_datasets = dataset_RCT.map(tokenize_function, batched=True)\n\n# Diviser l'ensemble de données en train et validation\ndataset_split = tokenized_datasets.train_test_split(test_size=0.2)\n\n# Extraire les ensembles d'entraînement et de validation\ntrain_dataset = dataset_split['train']\nval_dataset = dataset_split['test']\n\n        \n\n# Initialiser le modèle combiné\nmodel = LongformerBart(longformer_model, bart_model)\n\n# Définir les arguments d'entraînement\ntraining_args = TrainingArguments(\n    output_dir=\"./longformer_bart_finetuned_rct\",  # Répertoire de sortie pour enregistrer les résultats\n    evaluation_strategy=\"epoch\",  # Évaluer après chaque époque\n    learning_rate=2e-5,  # Taux d'apprentissage\n    per_device_train_batch_size=2,  # Taille du batch pour l'entraînement\n    per_device_eval_batch_size=2,   # Taille du batch pour l'évaluation\n    num_train_epochs=3,  # Nombre d'époques d'entraînement\n    save_steps=500,  # Sauvegarder moins souvent\n    weight_decay=0.01,  # L2 regularization\n    logging_dir=\"./logs\",  # Répertoire pour les logs\n    logging_steps=10,  # Fréquence des logs\n    report_to=\"tensorboard\",  # Optionnel : Utiliser TensorBoard pour la visualisation des métriques\n)\n\n\n# Créer un objet Trainer\ntrainer = Trainer(\n    model=model,  # Le modèle combiné Longformer+BART\n    args=training_args,  # Les arguments d'entraînement\n    train_dataset=train_dataset,  # Jeu d'entraînement\n    eval_dataset=val_dataset,  # Jeu de validation\n    tokenizer=longformer_tokenizer,  # Tokenizer pour le pré-traitement des données\n)\n\n# Lancer l'entraînement\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T22:43:13.541763Z","iopub.execute_input":"2024-12-30T22:43:13.542009Z","iopub.status.idle":"2024-12-30T22:43:13.557210Z","shell.execute_reply.started":"2024-12-30T22:43:13.541978Z","shell.execute_reply":"2024-12-30T22:43:13.556514Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"'\\n# Appliquer la tokenisation\\ntokenized_datasets = dataset_RCT.map(tokenize_function, batched=True)\\n\\n# Diviser l\\'ensemble de données en train et validation\\ndataset_split = tokenized_datasets.train_test_split(test_size=0.2)\\n\\n# Extraire les ensembles d\\'entraînement et de validation\\ntrain_dataset = dataset_split[\\'train\\']\\nval_dataset = dataset_split[\\'test\\']\\n\\n        \\n\\n# Initialiser le modèle combiné\\nmodel = LongformerBart(longformer_model, bart_model)\\n\\n# Définir les arguments d\\'entraînement\\ntraining_args = TrainingArguments(\\n    output_dir=\"./longformer_bart_finetuned_rct\",  # Répertoire de sortie pour enregistrer les résultats\\n    evaluation_strategy=\"epoch\",  # Évaluer après chaque époque\\n    learning_rate=2e-5,  # Taux d\\'apprentissage\\n    per_device_train_batch_size=2,  # Taille du batch pour l\\'entraînement\\n    per_device_eval_batch_size=2,   # Taille du batch pour l\\'évaluation\\n    num_train_epochs=3,  # Nombre d\\'époques d\\'entraînement\\n    save_steps=500,  # Sauvegarder moins souvent\\n    weight_decay=0.01,  # L2 regularization\\n    logging_dir=\"./logs\",  # Répertoire pour les logs\\n    logging_steps=10,  # Fréquence des logs\\n    report_to=\"tensorboard\",  # Optionnel : Utiliser TensorBoard pour la visualisation des métriques\\n)\\n\\n\\n# Créer un objet Trainer\\ntrainer = Trainer(\\n    model=model,  # Le modèle combiné Longformer+BART\\n    args=training_args,  # Les arguments d\\'entraînement\\n    train_dataset=train_dataset,  # Jeu d\\'entraînement\\n    eval_dataset=val_dataset,  # Jeu de validation\\n    tokenizer=longformer_tokenizer,  # Tokenizer pour le pré-traitement des données\\n)\\n\\n# Lancer l\\'entraînement\\ntrainer.train()\\n'"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"# Dupliquer les poids partagés pour éviter la duplication de mémoire\n# Cela doit être fait après l'entraînement et avant la sauvegarde\n\nimport safetensors.torch as st\n\n# Pour Longformer - dupliquer les embeddings\nwith torch.no_grad():  # Empêche la modification des gradients\n    new_word_embeddings = model.longformer.embeddings.word_embeddings.weight.clone()\n    model.longformer.embeddings.word_embeddings = torch.nn.Embedding.from_pretrained(new_word_embeddings)\n\n# Pour BART - dupliquer les embeddings de l'encodeur et du décodeur\nwith torch.no_grad():  # Empêche la modification des gradients\n    new_shared_weight = model.bart.model.shared.weight.clone()\n    model.bart.model.shared = torch.nn.Embedding.from_pretrained(new_shared_weight)\n    \n    new_encoder_embed = model.bart.model.encoder.embed_tokens.weight.clone()\n    model.bart.model.encoder.embed_tokens = torch.nn.Embedding.from_pretrained(new_encoder_embed)\n    \n    new_decoder_embed = model.bart.model.decoder.embed_tokens.weight.clone()\n    model.bart.model.decoder.embed_tokens = torch.nn.Embedding.from_pretrained(new_decoder_embed)\n\n# Sauvegarder les poids dans un fichier classique\ntorch.save(model.state_dict(), 'longformer_bart_finetuned_rct/model_weights.pth')\n\n# Sauvegarder avec safetensors après avoir dupliqué les poids\nst.save_file(model.state_dict(), 'longformer_bart_finetuned_rct/model_weights.safetensors')\n\n\nfrom transformers import PretrainedConfig\n\nimport json\n\n# Définir la configuration\nconfig = {\n    \"architectures\": [\"LongformerForSequenceClassification\"],\n    \"hidden_size\": 1024,\n    \"num_attention_heads\": 16,\n    \"num_hidden_layers\": 12,\n    \"vocab_size\": 50264,\n    \"max_position_embeddings\": 4098,\n    \"embedding_size\": 1024,\n    \"layer_norm_eps\": 1e-5,\n    \"pad_token_id\": 1,\n    \"activation_function\": \"gelu\",\n    \"initializer_range\": 0.02,\n    \"layer_norm_affine\": True,\n    \"attention_probs_dropout_prob\": 0.1,\n    \"hidden_dropout_prob\": 0.1,\n    \"max_position_embeddings\": 4098,\n    \"type_vocab_size\": 2,\n    \"attention_dropout\": 0.1,\n    \"longformer_projection\": {\n        \"in_features\": 768,\n        \"out_features\": 1024\n    }\n}\n\n\n\n# Sauvegarder le fichier config.json\nwith open(\"longformer_bart_finetuned_rct/config.json\", 'w') as f:\n    json.dump(config, f)\n\nprint(\"Le fichier config.json a été sauvegardé \")\n\n\n# Sauvegarder le modèle fine-tuné\ntrainer.save_model(\"./longformer_bart_finetuned_rct\")\n\"\"\"\n\"\"\"\n# Sauvegarde du modèle\ntorch.save(longformer_bart_model.state_dict(), \"longformer_bart_finetuned.pth\")\n\n# Sauvegarde de la configuration\nlongformer_bart_model.config.to_json_file(\"longformer_bart_config.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T22:43:13.558126Z","iopub.execute_input":"2024-12-30T22:43:13.558464Z","iopub.status.idle":"2024-12-30T22:43:13.577474Z","shell.execute_reply.started":"2024-12-30T22:43:13.558432Z","shell.execute_reply":"2024-12-30T22:43:13.576640Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"'\\n# Sauvegarde du modèle\\ntorch.save(longformer_bart_model.state_dict(), \"longformer_bart_finetuned.pth\")\\n\\n# Sauvegarde de la configuration\\nlongformer_bart_model.config.to_json_file(\"longformer_bart_config.json\")\\n'"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"import shutil\n# Chemin du dossier à supprimer\ndirectory_to_delete = \"/kaggle/working/longformer_bart_finetuned_rct/checkpoint-393\"\n# Vérifier si le dossier existe\nif os.path.exists(directory_to_delete):\n    # Supprimer le dossier et tout son contenu\n    shutil.rmtree(directory_to_delete)\n    print(f\"Le dossier '{directory_to_delete}' a été supprimé avec succès.\")\nelse:\n    print(f\"Le dossier '{directory_to_delete}' n'existe pas.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T22:43:13.578229Z","iopub.execute_input":"2024-12-30T22:43:13.578481Z","iopub.status.idle":"2024-12-30T22:43:13.597519Z","shell.execute_reply.started":"2024-12-30T22:43:13.578462Z","shell.execute_reply":"2024-12-30T22:43:13.596689Z"}},"outputs":[{"name":"stdout","text":"Le dossier '/kaggle/working/longformer_bart_finetuned_rct/checkpoint-393' n'existe pas.\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"import shutil\n\nprint(os.listdir(\"./\"))\n\nshutil.make_archive('/kaggle/working/longformer_bart_finetuned_rct', 'zip', './longformer_bart_finetuned_rct')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T22:43:13.598256Z","iopub.execute_input":"2024-12-30T22:43:13.598586Z","iopub.status.idle":"2024-12-30T22:43:13.613521Z","shell.execute_reply.started":"2024-12-30T22:43:13.598551Z","shell.execute_reply":"2024-12-30T22:43:13.612797Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"'\\nimport shutil\\n\\nprint(os.listdir(\"./\"))\\n\\nshutil.make_archive(\\'/kaggle/working/longformer_bart_finetuned_rct\\', \\'zip\\', \\'./longformer_bart_finetuned_rct\\')\\n'"},"metadata":{}}],"execution_count":20},{"cell_type":"markdown","source":"# Génération des résumés des articles de test","metadata":{}},{"cell_type":"markdown","source":"## Chargement de notre modèle fine-tuné sur les données \"OBS\" et constitué à partir des deux modèles bart et longformer, pour pouvoir l'utiliser pour l'ensemble de test et soumettre nos résumés pour la compétition","metadata":{}},{"cell_type":"code","source":"#Charger le modèle fine tuné afin de l'utiliser\nimport torch\nfrom transformers import LongformerModel, BartForConditionalGeneration, LongformerConfig\nimport torch.nn as nn\n\n\n# Définir la classe LongformerBart (on la redéfinit ici pour éviter d'avoir à naviguer entre cellules très distantes en pratique - et on n'en reprend que les parties nécessaires\n#à l'usage qu'on va en avoir là)\nclass LongformerBart(nn.Module):\n    def __init__(self, longformer_model, bart_model):\n        super(LongformerBart, self).__init__()\n        self.longformer = longformer_model\n        self.bart = bart_model\n        self.longformer_projection = nn.Linear(768, 1024)  # Projeter de 768 à 1024\n        \n    def forward(self, input_ids, attention_mask, decoder_input_ids=None, labels=None):\n        encoder_outputs = self.longformer(input_ids=input_ids, attention_mask=attention_mask)\n        encoder_hidden_states = encoder_outputs.last_hidden_state\n        encoder_hidden_states = self.longformer_projection(encoder_hidden_states)  # Appliquer la projection\n        decoder_outputs = self.bart(input_ids=decoder_input_ids, labels=labels, encoder_outputs=encoder_hidden_states)\n        return decoder_outputs\n\n# Charger Longformer et BART\nlongformer_model = LongformerModel.from_pretrained('allenai/longformer-base-4096')\nbart = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n\n# Charger les poids de notre modèle fine-tuné\nmodel = LongformerBart(longformer_model, bart) \nmodel.load_state_dict(torch.load('/kaggle/input/longformer_bart_fine_tuned_obs/pytorch/default/1/model_weights.pth'))  \n#model.load_state_dict(torch.load('/kaggle/working/longformer_bart_finetuned/model_weights.pth'))  # Remplacez par le bon chemin et la ligne du dessus pour utiliser le modèle déjà réentrainé\n\n# Charger la configuration personnalisée de notre modèle \"sur mesure\"\nconfig = LongformerConfig.from_json_file('/kaggle/input/longformer_bart_fine_tuned_obs/pytorch/default/1/config.json')\n#config = LongformerConfig.from_json_file('/kaggle/working/longformer_bart_finetuned/config.json') #idem : # Remplacez par le bon chemin et la ligne du dessus pour utiliser le modèle déjà réentrainé\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T16:42:24.268870Z","iopub.execute_input":"2025-01-11T16:42:24.269197Z","iopub.status.idle":"2025-01-11T16:42:30.106315Z","shell.execute_reply.started":"2025-01-11T16:42:24.269167Z","shell.execute_reply":"2025-01-11T16:42:30.105648Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-11-6b07a477c962>:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('/kaggle/working/longformer_bart_finetuned/model_weights.pth'))  # Remplacez par le bon chemin et la ligne du dessus pour utiliser le modèle déjà réentrainé\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## Mise en forme des données \"OBS\" de test pour le test du modèle.","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\n\n# Définir le chemin du dossier contenant les articles\ndossier_articles_test = \"/kaggle/input/m-2-maliash-resume-darticles-scientifiques/test/OBS_test/articles_OBS_test\"\n\n# Liste des fichiers dans le dossier\nfichiers_articles = [f for f in os.listdir(dossier_articles_test) if f.startswith(\"article-\")]\n\n# Dictionnaire pour stocker les articles par identifiant (avec contenu des fichiers)\narticles_test = {}\n\n# Remplir le dictionnaire avec les fichiers en fonction des identifiants\nfor fichier in fichiers_articles:\n    # Extraire l'identifiant du fichier article\n    identifiant = fichier.split(\"-\")[1].split(\".\")[0]\n    \n    # Lire le contenu du fichier\n    with open(os.path.join(dossier_articles_test, fichier), 'r') as f:\n        contenu_article = f.read()\n    \n    # Ajouter l'article et son identifiant au dictionnaire\n    articles_test[identifiant] = {\"article\": contenu_article}\n\n# Convertir le dictionnaire en DataFrame\ndf_articles_test = pd.DataFrame.from_dict(articles_test, orient='index')\n\n# Réinitialiser l'index pour que 'identifiant' devienne une colonne normale\ndf_articles_test.reset_index(inplace=True)\n\n# Renommer les colonnes\ndf_articles_test.columns = ['identifiant', 'article']\n\n# Vérifier le DataFrame\nprint(df_articles_test.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T16:45:32.080682Z","iopub.execute_input":"2025-01-11T16:45:32.081038Z","iopub.status.idle":"2025-01-11T16:45:32.284027Z","shell.execute_reply.started":"2025-01-11T16:45:32.081009Z","shell.execute_reply":"2025-01-11T16:45:32.283179Z"}},"outputs":[{"name":"stdout","text":"  identifiant                                            article\n0    28278130  Cardiometabolic Risk Factors Among 1.3 Million...\n1    34555924  Prostate Cancer Screening and Incidence among ...\n2    35157313  The associated burden of mental health conditi...\n3    36906849  Implementing digital systems to facilitate gen...\n4    37226713  Patient-reported treatment response in chronic...\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## Tokenisation des données et génération des résumés pour les articles de test de type \"OBS\"","metadata":{}},{"cell_type":"code","source":"from transformers import LongformerTokenizer, BartTokenizer, BartForConditionalGeneration\nimport torch\nimport pandas as pd\n\n# Initialiser les tokenizers Longformer et BART\nlongformer_tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\nbart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n\n# Charger un modèle BART pour la génération de résumé\n#model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\nmodel = model\n\n\ndef generer_resume(article, model, tokenizer, max_length=150):\n    \"\"\"\n    Fonction pour générer un résumé d'un article en utilisant notre modèle Longformer-BART.\n    \"\"\"\n    # Tokenisation de l'article\n    inputs = tokenizer(article, return_tensors=\"pt\", truncation=True, padding=True, max_length=1024)\n    \n    # Si le modèle est en mode CPU, déplacer les entrées et le modèle sur le bon périphérique\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n\n    inputs = {key: value.to(device) for key, value in inputs.items()}\n    \n    # Obtenir les sorties du modèle\n    with torch.no_grad():\n        # Appeler la méthode forward du modèle en utilisant les bons paramètres\n        outputs = model(input_ids=inputs['input_ids'], \n                        attention_mask=inputs['attention_mask'],  # Remarque: utiliser 'attention_mask' ici\n                        decoder_input_ids=inputs['input_ids'],  # Utiliser les input_ids comme entrée pour le décodeur\n                        labels=None)\n\n    # Décoder le résumé depuis la sortie du modèle\n    # Utilisation de la méthode de génération du modèle BART sur les sorties de Longformer\n    summary_ids = model.bart.generate(\n        inputs['input_ids'], \n        max_length=max_length, \n        num_beams=4, \n        early_stopping=True\n    )\n\n    # Décoder les IDs générés pour obtenir le résumé\n    resume = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n      \n    return resume\n\ndef traiter_ensemble_test(dataset_RCT, model, tokenizer):\n    \"\"\"\n    Fonction qui traite l'ensemble du dataset_RCT et génère des résumés pour chaque article.\n    \"\"\"\n    # Convertir le dataset en DataFrame \n    df = pd.DataFrame(dataset_RCT)\n\n    # Créer une nouvelle colonne pour les résumés générés\n    df['résumé'] = df['article'].apply(lambda x: generer_resume(x, model, tokenizer))\n\n    return df[['identifiant', 'article', 'résumé']]\n\n\n# Traiter l'ensemble de test et générer les résumés\nresultats_obs = traiter_ensemble_test(df_articles_test, model, bart_tokenizer)\n\n# Afficher les résultats\nprint(resultats_obs)\n\n# Supprimer la colonne 'article' et renommer les colonnes dans 'resultats' (pour mise au bon format pour la compétition)\nresultats_obs = resultats_obs.drop(columns=['article'])  # Supprimer la colonne 'article'\nresultats_obs = resultats_obs.rename(columns={'identifiant': 'id', 'résumé': 'abstract'})  # Renommer les colonnes\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T00:10:19.283949Z","iopub.execute_input":"2025-01-11T00:10:19.284294Z","iopub.status.idle":"2025-01-11T00:10:20.046720Z","shell.execute_reply.started":"2025-01-11T00:10:19.284267Z","shell.execute_reply":"2025-01-11T00:10:20.045554Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"input_ids: torch.Size([1, 1024])\nattention_mask: torch.Size([1, 1024])\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-35-2820c2e14b15>\u001b[0m in \u001b[0;36m<cell line: 68>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m# Traiter l'ensemble de test et générer les résumés\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mresultats_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraiter_ensemble_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_articles_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbart_tokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;31m# Afficher les résultats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-35-2820c2e14b15>\u001b[0m in \u001b[0;36mtraiter_ensemble_test\u001b[0;34m(dataset_RCT, model, tokenizer)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# Créer une nouvelle colonne pour les résumés générés\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'résumé'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'article'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgenerer_resume\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;31m# Afficher le résultat ou sauvegarder les résultats dans un fichier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4762\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4763\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4764\u001b[0;31m         ).apply()\n\u001b[0m\u001b[1;32m   4765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4766\u001b[0m     def _reindex_indexer(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m         \u001b[0;31m# self.func is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1209\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0;31m#  Categorical (GH51645).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1289\u001b[0;31m         mapped = obj._map_values(\n\u001b[0m\u001b[1;32m   1290\u001b[0m             \u001b[0mmapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurried\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1812\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1813\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mna_action\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1814\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1815\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1816\u001b[0m         return lib.map_infer_mask(\n","\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n","\u001b[0;32m<ipython-input-35-2820c2e14b15>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# Créer une nouvelle colonne pour les résumés générés\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'résumé'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'article'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgenerer_resume\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;31m# Afficher le résultat ou sauvegarder les résultats dans un fichier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-35-2820c2e14b15>\u001b[0m in \u001b[0;36mgenerer_resume\u001b[0;34m(article, model, tokenizer, max_length)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mresume\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \"\"\"   \n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mresume\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtraiter_ensemble_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_RCT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'resume' is not defined"],"ename":"NameError","evalue":"name 'resume' is not defined","output_type":"error"}],"execution_count":35},{"cell_type":"markdown","source":"## Chargement de notre modèle fine-tuné sur les données \"RCT\" et constitué à partir des deux modèles bart et longformer, pour pouvoir l'utiliser pour l'ensemble de test et soumettre nos résumés pour la compétition","metadata":{}},{"cell_type":"code","source":"#Charger le modèle fine tuné afin de l'utiliser\nimport torch\nfrom transformers import LongformerModel, BartForConditionalGeneration, LongformerConfig\nimport torch.nn as nn\n\n\n# Définir la classe LongformerBart\nclass LongformerBart(nn.Module):\n    def __init__(self, longformer_model, bart_model):\n        super(LongformerBart, self).__init__()\n        self.longformer = longformer_model\n        self.bart = bart_model\n        self.longformer_projection = nn.Linear(768, 1024)  # Projeter de 768 à 1024\n        \n    def forward(self, input_ids, attention_mask, decoder_input_ids=None, labels=None):\n        encoder_outputs = self.longformer(input_ids=input_ids, attention_mask=attention_mask)\n        encoder_hidden_states = encoder_outputs.last_hidden_state\n        encoder_hidden_states = self.longformer_projection(encoder_hidden_states)  # Appliquer la projection\n        decoder_outputs = self.bart(input_ids=decoder_input_ids, labels=labels, encoder_outputs=encoder_hidden_states)\n        return decoder_outputs\n\n# Charger Longformer et BART\nlongformer_model = LongformerModel.from_pretrained('allenai/longformer-base-4096')\nbart = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n\n# Charger les poids de notre modèle fine-tuné\nmodel = LongformerBart(longformer_model, bart)\nmodel.load_state_dict(torch.load('/kaggle/input/longformer_bart_fine_tuned_rct/pytorch/default/1/model_weights.pth'))  \n#model.load_state_dict(torch.load('/kaggle/working/longformer_bart_finetuned_rct/model_weigths.pth')) # Remplacez par le bon chemin et cette ligne si vous réentrainez le modèle\n\n\n\n# Charger la configuration\n# Notez que vous devez avoir la configuration JSON pour le modèle personnalisé\nconfig = LongformerConfig.from_json_file('/kaggle/input/longformer_bart_fine_tuned_rct/pytorch/default/1/config.json')\n#config = LongformerConfig.from_json_file('/kaggle/working/longformer_bart_finetuned_rct/config.json') #idem : Remplacez par le bon chemin et cette ligne si vous réentrainez le modèle\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T17:02:06.506168Z","iopub.execute_input":"2025-01-11T17:02:06.506543Z","iopub.status.idle":"2025-01-11T17:02:30.353611Z","shell.execute_reply.started":"2025-01-11T17:02:06.506486Z","shell.execute_reply":"2025-01-11T17:02:30.352896Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-14-e75a1597e4c7>:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('/kaggle/input/longformer_bart_fine_tuned_rct/pytorch/default/1/model_weights.pth'))\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## Mise en forme des données \"RCT\" de test pour le test du modèle.","metadata":{}},{"cell_type":"code","source":"#Mise en forme des données à tester\n\nimport os\nimport pandas as pd\n\n# Définir le chemin du dossier contenant les articles\ndossier_articles_test = \"/kaggle/input/m-2-maliash-resume-darticles-scientifiques/test/RCT_test/articles_RCT_test\"\n\n# Liste des fichiers dans le dossier\nfichiers_articles = [f for f in os.listdir(dossier_articles_test) if f.startswith(\"article-\")]\n\n# Dictionnaire pour stocker les articles par identifiant (avec contenu des fichiers)\narticles_test = {}\n\n# Remplir le dictionnaire avec les fichiers en fonction des identifiants\nfor fichier in fichiers_articles:\n    # Extraire l'identifiant du fichier article\n    identifiant = fichier.split(\"-\")[1].split(\".\")[0]\n    \n    # Lire le contenu du fichier\n    with open(os.path.join(dossier_articles_test, fichier), 'r') as f:\n        contenu_article = f.read()\n    \n    # Ajouter l'article et son identifiant au dictionnaire\n    articles_test[identifiant] = {\"article\": contenu_article}\n\n# Convertir le dictionnaire en DataFrame\ndf_articles_test = pd.DataFrame.from_dict(articles_test, orient='index')\n\n# Réinitialiser l'index pour que 'identifiant' devienne une colonne normale\ndf_articles_test.reset_index(inplace=True)\n\n# Renommer les colonnes\ndf_articles_test.columns = ['identifiant', 'article']\n# Vérifier le DataFrame\nprint(df_articles_test.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T17:02:40.627556Z","iopub.execute_input":"2025-01-11T17:02:40.627865Z","iopub.status.idle":"2025-01-11T17:02:40.881294Z","shell.execute_reply.started":"2025-01-11T17:02:40.627842Z","shell.execute_reply":"2025-01-11T17:02:40.880588Z"}},"outputs":[{"name":"stdout","text":"  identifiant                                            article\n0    26811968  Randomized Controlled Trial of Hospital-Based ...\n1    34737472  Behavioural intervention for adolescent uptake...\n2    34844893  Prediction of clinical response to corticoster...\n3    34969647  Feasibility and efficacy of a multidisciplinar...\n4    35532871  Iron homeostasis in heart transplant recipient...\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## Tokenisation des données et génération des résumés pour les articles de test de type \"RCT\"","metadata":{}},{"cell_type":"code","source":"from transformers import LongformerTokenizer, BartTokenizer, BartForConditionalGeneration\nimport torch\nimport pandas as pd\n\n# Initialiser les tokenizers Longformer et BART\nlongformer_tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\nbart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n\n# Charger un modèle BART pour la génération de résumé\n#model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\nmodel = model\n\ndef generer_resume(article, model, tokenizer, max_length=150):\n    \"\"\"\n    Fonction pour générer un résumé d'un article en utilisant notre modèle Longformer-BART.\n    \"\"\"\n    # Tokenisation de l'article\n    inputs = tokenizer(article, return_tensors=\"pt\", truncation=True, padding=True, max_length=1024)\n    \n    # Si le modèle est en mode CPU, déplacer les entrées et le modèle sur le bon périphérique\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    inputs = {key: value.to(device) for key, value in inputs.items()}\n    \n    # Obtenir les sorties du modèle\n    with torch.no_grad():\n        # Appeler la méthode forward du modèle en utilisant les bons paramètres\n        outputs = model(input_ids=inputs['input_ids'], \n                        attention_mask=inputs['attention_mask'],  # Remarque: utilisez 'attention_mask' ici\n                        decoder_input_ids=inputs['input_ids'],  # Utiliser les input_ids comme entrée pour le décodeur\n                        labels=None)\n\n    # Décoder le résumé depuis la sortie du modèle\n    # Utilisation de la méthode de génération du modèle BART sur les sorties de Longformer\n    summary_ids = model.bart.generate(\n        inputs['input_ids'], \n        max_length=max_length, \n        num_beams=4, \n        early_stopping=True\n    )\n\n    # Décoder les IDs générés pour obtenir le résumé\n    resume = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n    \n    return resume\n\ndef traiter_ensemble_test(dataset_RCT, model, tokenizer):\n    \"\"\"\n    Fonction qui traite l'ensemble du dataset_RCT et génère des résumés pour chaque article.\n    \"\"\"\n    # Convertir le dataset en DataFrame\n    df = pd.DataFrame(dataset_RCT)\n\n    # Créer une nouvelle colonne pour les résumés générés\n    df['résumé'] = df['article'].apply(lambda x: generer_resume(x, model, tokenizer))\n\n    return df[['identifiant', 'article', 'résumé']]\n\n\n# Traiter l'ensemble de test et générer les résumés\nresultats_rct = traiter_ensemble_test(df_articles_test, model, bart_tokenizer)\n\n# Afficher les résultats\nprint(resultats_rct)\n\n# Supprimer la colonne 'article' et renommer les colonnes dans 'resultats' (pour mise au bon format pour la compétition)\nresultats_rct = resultats_rct.drop(columns=['article'])  # Supprimer la colonne 'article'\nresultats_rct = resultats_rct.rename(columns={'identifiant': 'id', 'résumé': 'abstract'})  # Renommer les colonnes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T22:44:08.142043Z","iopub.execute_input":"2024-12-30T22:44:08.142326Z","iopub.status.idle":"2024-12-30T22:44:38.602767Z","shell.execute_reply.started":"2024-12-30T22:44:08.142269Z","shell.execute_reply":"2024-12-30T22:44:38.601886Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"   identifiant                                            article  \\\n0     26811968  Randomized Controlled Trial of Hospital-Based ...   \n1     34737472  Behavioural intervention for adolescent uptake...   \n2     34844893  Prediction of clinical response to corticoster...   \n3     34969647  Feasibility and efficacy of a multidisciplinar...   \n4     35532871  Iron homeostasis in heart transplant recipient...   \n5     35638317  The effect of duloxetine on mechanistic pain p...   \n6     36479935  Clinical Predictors of Adherence to Exercise T...   \n7     36621009  Supporting return to work after psychiatric ho...   \n8     37972955  LIGHTSITE III: 13-Month Efficacy and Safety Ev...   \n9     37991188  Evolution in Laparoscopic Gastrectomy From a R...   \n10    38052803  Long-term outcome of children with acute promy...   \n11    38060092  Efficacy and Safety of the Travoprost Intraocu...   \n12    38117526  Efficacy and Safety of PF-07038124 in Patients...   \n13    38184526  Prognostic significance of surgery and radioth...   \n14    38189649  Determinants of Subjective Mental and Function...   \n15    38197254  Daily skin-to-skin contact alters microbiota d...   \n16    38218840  The effectiveness of an m-Health intervention ...   \n17    38315470  Alirocumab in Pediatric Patients With Heterozy...   \n18    38417090  Effectiveness of Seizure Dogs for People With ...   \n19    38437855  Active vitamin D treatment in the prevention o...   \n20    38446126  Intravenous iron for heart failure, iron defic...   \n21    38531621  Guselkumab provides durable improvement across...   \n22    38591920  Multi-strain probiotics during pregnancy in wo...   \n23    38609994  The effects of telehealth-delivered mindfulnes...   \n24    38668732  A randomized controlled trial of a postdischar...   \n\n                                               résumé  \n0   Randomized Controlled Trial of Hospital-Based ...  \n1   Behavioural intervention for adolescent uptake...  \n2   Prediction of clinical response to corticoster...  \n3   Feasibility and efficacy of a multidisciplinar...  \n4   Iron homeostasis in heart transplant recipient...  \n5   abstract.com. The abstract is a comprehensive ...  \n6   Heart Failure: A Controlled Trial Investigatin...  \n7   Supporting return to work after psychiatric ho...  \n8   LIGHTSITE III: 13-month Efficacy and Safety Ev...  \n9   Evolution in Laparoscopic Gastrectomy From a R...  \n10  Arsenic-based therapy substantially improves t...  \n11  Travoprost Intraocular Implant Reduces Topical...  \n12  Atopic dermatitis and plaque psoriasis are chr...  \n13  Prognostic significance of surgery and radioth...  \n14  ICU survivors frequently suffer from impairmen...  \n15  Daily skin-to-skin contact alters microbiota d...  \n16  The effectiveness of an m-Health intervention ...  \n17  Alirocumab in Pediatric Patients With Heterozy...  \n18  Effectiveness of Seizure Dogs for People With ...  \n19  Active vitamin D treatment in the prevention o...  \n20  Intravenous iron increases haemoglobin, improv...  \n21  Guselkumab provides durable improvement across...  \n22  Multi-strain probiotics during pregnancy in wo...  \n23  The effects of telehealth-delivered mindfulnes...  \n24  A randomized controlled trial of a postdischar...  \n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"## Fusion de nos deux fichiers de résumés générés (resultats_obs et resultats_rct) en un seul fichier submission, puis conversion en csv pour soumission à la compétition (permet d'avoir le score rouge).","metadata":{}},{"cell_type":"code","source":"# Concaténer les deux DataFrames\nresultats = pd.concat([resultats_obs, resultats_rct], ignore_index=True)\n\n# Sauvegarder le DataFrame concaténé dans un fichier CSV, si nécessaire\nresultats.to_csv('submission.csv', index=False)\n\n# Afficher le DataFrame concaténé\nprint(resultats)","metadata":{"execution":{"iopub.status.busy":"2025-01-11T17:16:40.901187Z","iopub.execute_input":"2025-01-11T17:16:40.901487Z","iopub.status.idle":"2025-01-11T17:16:40.922638Z","shell.execute_reply.started":"2025-01-11T17:16:40.901463Z","shell.execute_reply":"2025-01-11T17:16:40.921965Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Résultats du modèle composé à partir de longformer + bart\n\nLe score rouge obtenu lors de la soumission à la compétition est de 0.065 : c'est à peine au-dessus du modèle \"BART\" seul (et non fine-tuné quant à lui) !\n\nComment s'explique cela ?\n\nNous avons vu que, pour ne pas outrepasser les capacités de la mémoire disponible via les GPU de Kaggle, nous étions obligés de nous limiter à 1024 tokens. Ce, tant lors du fine-tuning du modèle (fonction tokenize_function(examples):\n    \n    \"model_inputs = longformer_tokenizer(\n        examples[\"article\"], \n        #max_length=4096, #trop long pour le GPU de kaggle...nous en revenons donc aux fameux 1024 token max, d'où nous partions avec BART seul.\n        max_length=1024,\n        truncation=True, \")\n\nque lors de son utilisation sur les articles de test.\n\nOr, l'ensemble de ces articles sont des articles longs - environ 20000 mots de longueur par article.\n\nEn pratique, les modèles de langage Longformer utilisent des tokenizers basés sur des sous-mots : certains mots peuvent être décomposés en plusieurs token, notamment les mots rares ou composés (nota : le modèle Llama - plus généraliste que les nôtres - utilisé par notre enseignant pour obtenir son score dans la compétition, utilise le même type de tokenisation basée sur des sous-mots). Inversement, les mots courants sont tokenisés par des token uniques, ce qui permet d'obtenir une représentation très compacte par ces tokenizers : une longueur de 20000 mots équivaut à entre 3000 et 5000 token pour un article.\nCe n'est pas supérieur aux 4096 token que peut prendre en entrée le modèle longformer que nous avons utilisé - mais c'est très supérieur aux 1024 token maximum, auxquels nous avons été obligés de \"brider\" ce modèle sur la plateforme kaggle, du fait des contraintes de GPU.\nEn clair : nos résumés sont faits sur la base uniquement (à peu près) du premier tiers de chaque article scientifique !\nCe, tant lors de l'entrainement, que lors du test.\nAinsi, les deux autres tiers de l'article ne sont en fait pas résumés du tout : ils ne sont tout simplement pas pris en compte, alors qu'ils contiennent généralement des éléments tout aussi importants que le début de l'article, à inclure dans le résumé.\nEt cette situation est finalement identique avec BART seul (qui est naturellement plafonné à 1024 token), et avec notre modèle longformer-bart (que nous sommes obligés de \"brider\" à 1024 token du fait des contraintes de mémoire du GPU sur kaggle).\n\nCeci explique les scores très médiocres obtenus.\n\nPour faire mieux, tout en tenant compte des limites de notre plateforme (kaggle), il nous faut donc nous diriger vers d'autres stratagèmes, applicables dans le temps qu'il nous reste pour cette compétition et le rendu de ce projet.\n\nNous choisissons donc de tester le \"chunking\" des articles scientifiques de l'ensemble de test, qui est le plus simple à mettre en oeuvre. \n\n\n\n        ","metadata":{}},{"cell_type":"markdown","source":"# Test de la méthode du résumé par morceaux des articles de l'ensemble de test\n\nConcrètement, nous allons donc subdiviser chaque article à résumer en \"morceaux\" de 1024 token chacun, puis faire résumer par notre modèle chaque morceau, et, simplement, concaténer à la suite tous ces résumés entre eux pour former un résumé de l'article entier.\n\nLa méthode est frustre, car on n'aura pas les liens et la fluidité entre les différents \"morceaux\".\nEn outre, couper à 1024 token peut couper au milieu d'une phrase, par exemple - mais c'est toujours mieux que de prendre le résumé d'un seul tiers de l'article pour le résumé de l'article tout entier ! Et, surtout, c'est faisable dans le délai et avec les moyens matériels et humains dont nous disposons.\n","metadata":{}},{"cell_type":"markdown","source":"## articles \"OBS\"","metadata":{}},{"cell_type":"code","source":"from transformers import LongformerTokenizer, BartTokenizer, BartForConditionalGeneration\nimport torch\nimport pandas as pd\n\n\n# Initialiser les tokenizers Longformer et BART\nlongformer_tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\nbart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n\n# Charger un modèle BART pour la génération de résumé\n#model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\nmodel = model\n\n\ndef generer_resume(article, model, tokenizer, max_length=150,chunk_size=1024):\n    \"\"\"\n    Fonction pour générer un résumé d'un article en utilisant notre modèle Longformer-BART.\n    \"\"\"\n         \n    # Diviser le texte en morceaux\n    #morceaux = [article[i:i + chunk_size] for i in range(0, len(article), chunk_size)]\n    morceaux = [\n    (article[i:i + chunk_size] + ' ' * (chunk_size - len(article[i:i + chunk_size]))) \n    for i in range(0, len(article), chunk_size)\n]\n    resumes = []\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n        \n    for morceau in morceaux:\n        # Générer un résumé pour chaque morceau\n        inputs = tokenizer(morceau, padding='max_length', return_tensors=\"pt\", truncation=True, max_length=chunk_size)\n        # Si le modèle est en mode CPU, déplacer les entrées et le modèle sur le bon périphérique\n                \n        inputs = {key: (value.to(device) if isinstance(value, torch.Tensor) else value) for key, value in inputs.items()}\n        #inputs = {key: value.to(device) for key, value in inputs.items()}\n       \n        \n         # Obtenir les sorties du modèle\n        with torch.no_grad():\n            # Appeler la méthode forward du modèle en utilisant les bons paramètres\n            outputs = model(input_ids=inputs['input_ids'], \n                            attention_mask=inputs['attention_mask'],  # Remarque: utiliser 'attention_mask' ici\n                            decoder_input_ids=inputs['input_ids'],  # Utiliser les input_ids comme entrée pour le décodeur\n                            labels=None)\n        # Décoder le résumé depuis la sortie du modèle\n        # Utilisation de la méthode de génération du modèle BART sur les sorties de Longformer\n        summary_ids = model.bart.generate(\n            inputs['input_ids'], \n            max_length=max_length, \n            num_beams=4, \n            early_stopping=True\n            )\n         \n         \n        resumes.append(tokenizer.decode(summary_ids[0], skip_special_tokens=True))\n\n    # Combiner les résumés partiels en un seul texte\n    resume = \" \".join(resumes)\n   \n    return resume\n\n\n\n\ndef traiter_ensemble_test(dataset_RCT, model, tokenizer):\n    \"\"\"\n    Fonction qui traite l'ensemble du dataset_RCT et génère des résumés pour chaque article.\n    \"\"\"\n    # Convertir le dataset en DataFrame \n    df = pd.DataFrame(dataset_RCT)\n\n    # Créer une nouvelle colonne pour les résumés générés\n    df['résumé'] = df['article'].apply(lambda x: generer_resume(x, model, tokenizer))\n\n    return df[['identifiant', 'article', 'résumé']]\n\n\n\n# Traiter l'ensemble de test et générer les résumés\nresultats_obs = traiter_ensemble_test(df_articles_test, model, bart_tokenizer)\n\n# Afficher les résultats\nprint(resultats_obs)\n\n# Supprimer la colonne 'article' et renommer les colonnes dans 'resultats' (pour mise au bon format pour la compétition)\nresultats_obs = resultats_obs.drop(columns=['article'])  # Supprimer la colonne 'article'\nresultats_obs = resultats_obs.rename(columns={'identifiant': 'id', 'résumé': 'abstract'})  # Renommer les colonnes\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Articles \"RCT\"","metadata":{}},{"cell_type":"code","source":"from transformers import LongformerTokenizer, BartTokenizer, BartForConditionalGeneration\nimport torch\nimport pandas as pd\n\n\n# Initialiser les tokenizers Longformer et BART\nlongformer_tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\nbart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n\n# Charger un modèle BART pour la génération de résumé\n#model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\nmodel = model\n\n\ndef generer_resume(article, model, tokenizer, max_length=150,chunk_size=1024):\n    \"\"\"\n    Fonction pour générer un résumé d'un article en utilisant notre modèle Longformer-BART.\n    \"\"\"\n         \n    # Diviser le texte en morceaux\n    #morceaux = [article[i:i + chunk_size] for i in range(0, len(article), chunk_size)]\n    morceaux = [\n    (article[i:i + chunk_size] + ' ' * (chunk_size - len(article[i:i + chunk_size]))) \n    for i in range(0, len(article), chunk_size)\n]\n    resumes = []\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n        \n    for morceau in morceaux:\n        # Générer un résumé pour chaque morceau\n        inputs = tokenizer(morceau, padding='max_length', return_tensors=\"pt\", truncation=True, max_length=chunk_size)\n        # Si le modèle est en mode CPU, déplacer les entrées et le modèle sur le bon périphérique\n                \n        inputs = {key: (value.to(device) if isinstance(value, torch.Tensor) else value) for key, value in inputs.items()}\n        #inputs = {key: value.to(device) for key, value in inputs.items()}\n       \n        \n         # Obtenir les sorties du modèle\n        with torch.no_grad():\n            # Appeler la méthode forward du modèle en utilisant les bons paramètres\n            outputs = model(input_ids=inputs['input_ids'], \n                            attention_mask=inputs['attention_mask'],  # Remarque: utiliser 'attention_mask' ici\n                            decoder_input_ids=inputs['input_ids'],  # Utiliser les input_ids comme entrée pour le décodeur\n                            labels=None)\n        # Décoder le résumé depuis la sortie du modèle\n        # Utilisation de la méthode de génération du modèle BART sur les sorties de Longformer\n        summary_ids = model.bart.generate(\n            inputs['input_ids'], \n            max_length=max_length, \n            num_beams=4, \n            early_stopping=True\n            )\n         \n         \n        resumes.append(tokenizer.decode(summary_ids[0], skip_special_tokens=True))\n\n    # Combiner les résumés partiels en un seul texte\n    resume = \" \".join(resumes)\n   \n    return resume\n\n\n\n\ndef traiter_ensemble_test(dataset_RCT, model, tokenizer):\n    \"\"\"\n    Fonction qui traite l'ensemble du dataset_RCT et génère des résumés pour chaque article.\n    \"\"\"\n    # Convertir le dataset en DataFrame \n    df = pd.DataFrame(dataset_RCT)\n\n    # Créer une nouvelle colonne pour les résumés générés\n    df['résumé'] = df['article'].apply(lambda x: generer_resume(x, model, tokenizer))\n\n    return df[['identifiant', 'article', 'résumé']]\n\n\n\n# Traiter l'ensemble de test et générer les résumés\nresultats_rct = traiter_ensemble_test(df_articles_test, model, bart_tokenizer)\n\n# Afficher les résultats\nprint(resultats_rct)\n\n# Supprimer la colonne 'article' et renommer les colonnes dans 'resultats' (pour mise au bon format pour la compétition)\nresultats_rct = resultats_rct.drop(columns=['article'])  # Supprimer la colonne 'article'\nresultats_rct = resultats_rct.rename(columns={'identifiant': 'id', 'résumé': 'abstract'})  # Renommer les colonnes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T17:02:56.202754Z","iopub.execute_input":"2025-01-11T17:02:56.203067Z","iopub.status.idle":"2025-01-11T17:14:58.522648Z","shell.execute_reply.started":"2025-01-11T17:02:56.203041Z","shell.execute_reply":"2025-01-11T17:14:58.521807Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"   identifiant                                            article  \\\n0     26811968  Randomized Controlled Trial of Hospital-Based ...   \n1     34737472  Behavioural intervention for adolescent uptake...   \n2     34844893  Prediction of clinical response to corticoster...   \n3     34969647  Feasibility and efficacy of a multidisciplinar...   \n4     35532871  Iron homeostasis in heart transplant recipient...   \n5     35638317  The effect of duloxetine on mechanistic pain p...   \n6     36479935  Clinical Predictors of Adherence to Exercise T...   \n7     36621009  Supporting return to work after psychiatric ho...   \n8     37972955  LIGHTSITE III: 13-Month Efficacy and Safety Ev...   \n9     37991188  Evolution in Laparoscopic Gastrectomy From a R...   \n10    38052803  Long-term outcome of children with acute promy...   \n11    38060092  Efficacy and Safety of the Travoprost Intraocu...   \n12    38117526  Efficacy and Safety of PF-07038124 in Patients...   \n13    38184526  Prognostic significance of surgery and radioth...   \n14    38189649  Determinants of Subjective Mental and Function...   \n15    38197254  Daily skin-to-skin contact alters microbiota d...   \n16    38218840  The effectiveness of an m-Health intervention ...   \n17    38315470  Alirocumab in Pediatric Patients With Heterozy...   \n18    38417090  Effectiveness of Seizure Dogs for People With ...   \n19    38437855  Active vitamin D treatment in the prevention o...   \n20    38446126  Intravenous iron for heart failure, iron defic...   \n21    38531621  Guselkumab provides durable improvement across...   \n22    38591920  Multi-strain probiotics during pregnancy in wo...   \n23    38609994  The effects of telehealth-delivered mindfulnes...   \n24    38668732  A randomized controlled trial of a postdischar...   \n\n                                               résumé  \n0   Randomized Controlled Trial of Hospital-Based ...  \n1   Unplanned pregnancy can have a major impact on...  \n2   Plantar fasciitis (PF) is a common condition a...  \n3   Feasibility and efficacy of a multidisciplinar...  \n4   Iron homeostasis in heart transplant recipient...  \n5   The effect of duloxetine on mechanistic pain p...  \n6   Cardiac rehabilitation (CR) improves exercise ...  \n7   Supporting return to work after psychiatric ho...  \n8   Age-related macular degeneration (AMD) is a re...  \n9   Evolution in Laparoscopic Gastrectomy From a R...  \n10  Arsenic-based therapy substantially improves t...  \n11  Glaucoma therapies have improved and diversifi...  \n12  Atopic dermatitis and plaque psoriasis are chr...  \n13  Prostate cancer (PC) is a significant disease ...  \n14  ICU survivors frequently suffer from impairmen...  \n15  Daily skin-to-skin contact alters microbiota d...  \n16  The effectiveness of an m-Health intervention ...  \n17  Alirocumab in Pediatric Patients With Heterozy...  \n18  Epilepsy imposes a significant clinical and ec...  \n19  Active vitamin D treatment in the prevention o...  \n20  Intravenous iron increases haemoglobin, improv...  \n21  Guselkumab provides durable improvement across...  \n22  Multi-strain probiotics during pregnancy in wo...  \n23  Chronic low back pain (CLBP) is a costly probl...  \n24  A randomized controlled trial of a postdischar...  \n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"## Fusion de nos deux fichiers de résumés générés (resultats_obs et resultats_rct) en un seul fichier submission, puis conversion en csv pour soumission à la compétition (permet d'avoir le score rouge).","metadata":{}},{"cell_type":"code","source":"# Concaténer les deux DataFrames\nresultats = pd.concat([resultats_obs, resultats_rct], ignore_index=True)\n\n# Sauvegarder le DataFrame concaténé dans un fichier CSV, si nécessaire\nresultats.to_csv('submission.csv', index=False)\n\n# Afficher le DataFrame concaténé\nprint(resultats)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Résultats du \"chunking\" des articles à résumer\n\nLe score rouge est doublé (nous passons de 0.065 à 0.135 sur l'échantillon testé lors de la soumission à la compétition - 0.151 sur la totalité de la soumission, résultat visible une fois la compétition close).\nLa méthode, pour \"frustre\" qu'elle soit, est donc efficace.\n\nElle est \"frustre\", parce que comme dit en introduction, on n'implémente rien qui permette au modèle de faire du lien entre les différents \"chunk\" de chaque article, et parce que côté résumé, on se contente de mettre bout à bout les sous-résumés obtenus, pour construire le résumé total de l'article entier.\n\nElle est \"frustre\" aussi pour une autre raison : nous n'avons effectué cette subdivision des articles que sur l'ensemble de test. Mais en revanche, nous fine-tunons toujours notre modèle sur les articles du train set sans les avoir, quant à eux, subdivisés en morceaux de 1024 token chacun !\nEn d'autres termes, nous fine-tunons toujours notre modèle à une tâche de résumé du premier tiers de chaque article scienfique...en lui fournissant comme étiquette le résumé total de ce même article.\nMais on a estimé, et le score nous donne raison, que le modèle pré-entrainé était déjà pré-entraîné, et que le fine-tuner avec cette approximation était en fait déjà \"pas trop mal\". Surtout, le coût de l'implémentation du chunking sur l'ensemble d'entrainement, plus lourd en durée de travail humain et en temps de travail de la machine nécessaires, que sur l'ensemble de test (*), était peu compatible avec le délai nous 0restant alors pour participer à la compétition.\n\n\n(*) Il nous faut préciser ici le caractère, malheureusement chronophage, de la prise en mains de la plateforme kaggle, que nous n'avions jamais utilisée, ainsi que le caractère malcommode, pour les novices que nous étions, de son dossier \"working\", à la capacité réduite par rapport à nos besoins, et dont la touche \"download\" buguait, nous forçant à reprendre de zéro, et ce à plusieurs reprises, le fine-tuning de nos modèles durant de longues heures... Ce temps n'a pu être utilisé à des explorations plus en rapport direct avec les LLM, ce qui est source de regrets pour nous.","metadata":{"jp-MarkdownHeadingCollapsed":true}},{"cell_type":"markdown","source":"# Limites et perspectives\n\n\nComme dit à l'instant, nous pourrions améliorer encore le score en approfondissant et affinant la stratégie de chunking de nos articles.\nPar exemple, nous apprenons que nous pourrions implémenter une couche d'attention hiérarchique qui résume les chunks avant de les passer à la partie BART de notre modèle, afin que ce dernier puisse avoir une compréhension des relations entre ces différents chunk.\nBien évidemment, nous pourrions mettre en place le même chunking pour améliorer le fine-tuning sur les articles de l'ensemble d'entrainement du modèle, car celui-ci consisterait alors réellement à mettre en lien chaque article en entier, avec son résumé complet.\n\nNous avons construit un modèle utilisant les transformer \"longformer\" et \"bart\", ce qui peut poser plusieurs questions :\n\n- la première est que, au vu du faible écart entre le score rouge (exécrable) sur bart non fine-tuné, et le score de notre modèle \"hybride\" fine-tuné, le choix du modèle et son fine-tuning ont semblé finalement, dans l'environnement informatique à notre disposition, avec ses limites (1024 token...), bien moins importants que la bonne préparation (chunking...) des textes à résumer.\nSi nous avions eu le temps, nous aurions pu effectuer le fine-tuning et le test de bart seul sur les articles de test découpés en chunk, pour comparer !\n\n- ce faible écart entre bart non fine-tuné, et notre modèle \"hybride\" (dument fine-tuné), aurait sans doute été bien plus important dans un environnement de travail (capacité GPU, etc) nous permettant d'utiliser réellement les capacités de longformer (4096 token). Dit autrement, la construction du modèle \"hybride\", pour une utilisation dans l'environnement kaggle, a été du temps de travail inutilement utilisé (sauf pour notre apprentissage bien sûr, mais si nous étions en entreprise ce temps aurait constitué une perte inutile - et nous y avons consacré un temps assez long).\n\nEn revanche, dans un autre environnement informatique, plus capacitaire, ce temps de construction aurait été très utile, puisqu'évitant de devoir effectuer le moindre chunking ensuite.\n\nUn apprentissage pratique issu de la réalisation de ce projet est donc, pour nous, qu'il faut en premier bien connaître l'environnement informatique dans lequel nous travaillons, et ses capacités, puis construire nos solutions dans ce cadre, en les y adaptant dès le départ. Et non construire des solutions puis se rendre compte que l'environnement nous contraint à les brider.\n\n- la seconde est que nous avons estimé qu'un résumé \"de qualité\" était un résumé effectué avec l'aide d'un modèle pouvant générer du texte.\nC'est à dire qu'un résumé \"de qualité\" était ce qu'on appelle un résumé abstractif, qui a en effet pour qualité d'être plus fluide que la simple juxtaposition des phrases jugées importantes par le modèle, et par suite (simplement) copiées du texte par ses soins (cas des résumés extractifs).\n\nCependant, dans un article récent, Adrien Guille et Saïd Raoufdine remarquent que : \"Toutefois, les résumés abstractifs générés par ces outils peuvent être\nbiaisés, non informatifs voire même mensongers. Ainsi, dans certains contextes sensibles (e.g. résumé d’articles scientifiques ou d’articles de presse), le résumé\nextractif reste une approche plus fiable.\" (Raoufdine Said, Adrien Guille. Résumé interactif de documents. 24ème Conférence sur l’Extraction\net la Gestion des Connaissances (EGC), Jan 2024, Dijon, France. hal-04448464 - https://hal.science/hal-04448464v1/file/1002963.pdf)\n\nEn outre, notre chunking \"frustre\" fait probablement perdre pas mal de ses qualités de fluidité au résumé abstractif tel que généré en pratique sur nos articles de test...\n\nDeux raisons pour lesquelles finalement, le modèle longformer seul (qui n'est pas un modèle générateur de texte), aurait pu être pertinent, plutôt qu'un modèle générant du texte (comme bart) ou comportant un sous-modèle générant du texte (comme notre hybride \"longformer-bart\"). En effet, il avait toutes les qualités requises pour être un bon modèle de résumé extractif sur nos articles scientifiques longs.","metadata":{}}]}